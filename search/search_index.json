{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the United States Department of War's #AI4SDLC Site","text":""},{"location":"#the-future-of-software-development-in-the-dow-starts-here","title":"The Future of Software Development in the DoW Starts Here","text":"<p>Artificial Intelligence (AI) is reshaping how software is designed, built, tested, and deployed. In the DoW, where security, compliance, and mission impact are paramount, AI must be harnessed responsibly, strategically, and effectively.</p> <p>#AI4SDLC is your playbook for integrating AI into the Software Development Lifecycle (SDLC)\u2014providing clear, actionable guidance tailored to DoW software value streams. Whether you\u2019re a developer, security engineer, acquisition professional, or program manager, this is where you\u2019ll find the insights and strategies to make AI work for mission success.  </p> <p>Our goal is to provide practical guidance while also linking to relevant and helpful content from across the DoW and industry, helping teams can access the latest policies, frameworks, and best practices.  </p>"},{"location":"#_1","title":"Overview","text":""},{"location":"#ai4sdlc","title":"#AI4SDLC?","text":"<p>AI is already transforming software engineering, but the DoW can\u2019t afford to adopt AI blindly. To be effective, AI integration must align with:</p> <ul> <li>Mission Readiness \u2013 AI must enhance, not disrupt, DoW software pipelines.  </li> <li>Security &amp; Compliance \u2013 AI tools must meet strict DoW security standards and Zero Trust principles.  </li> <li>Human-AI Teaming \u2013 AI should augment and empower software teams, not replace them. </li> <li>Operational Resilience \u2013 AI models must be governable, explainable, and adaptable to DoW requirements.  </li> </ul> <p>That\u2019s where #AI4SDLC comes in. We deliver practical, tested plays to guide AI adoption at every stage of the SDLC. What's more important is that we are collaborating with programs across the DoW making sure our hands on guidance aligns with their on-the-ground experiences.</p>"},{"location":"#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Generative AI (GenAI) refers to artificial intelligence systems\u2014typically large language models (LLMs)\u2014that can generate new content in response to natural language prompts. Unlike traditional AI that classifies, detects, or predicts within defined rules, GenAI can produce human-readable code, text, infrastructure scripts, documentation, diagrams, and even test cases.</p> <p>In the context of the SDLC, GenAI acts as an amplifier for human intent. Engineers and analysts describe what they want, and the GenAI tool attempts to generate a working solution. When properly integrated and governed, this can reduce toil, accelerate scaffolding, and improve documentation quality.</p> <p>However, GenAI is not magic\u2014it introduces new forms of error, creates verification challenges, and shifts the role of developers and engineers from creators to reviewers. It requires intentional design and oversight to ensure alignment with mission needs, security standards, and operational constraints.</p>"},{"location":"#genais-impact-across-the-software-value-stream","title":"GenAI's Impact Across the Software Value Stream","text":"<p>Generative AI isn't just a tool for writing code\u2014it is reshaping the entire software development value stream, from initial concept to deployed operations. Its potential and pitfalls extend well beyond individual developer productivity.</p> <p>When thoughtfully integrated, GenAI supports:</p> <ul> <li> <p>Requirements Engineering - Accelerates the transformation of stakeholder intent into structured, testable user stories or models. Prompts can be used to generate acceptance criteria, identify edge cases, or translate between technical and non-technical language.</p> </li> <li> <p>Architecture and Design  - Assists with documenting system context, suggesting interface definitions, creating UML sketches, and even proposing architectural tradeoffs based on past designs or known patterns.</p> </li> <li> <p>Development and Testing - Generates boilerplate code, test scaffolding, and documentation. Code suggestions can speed up delivery, but they also introduce new risks\u2014requiring human review, security scanning, and traceability.</p> </li> <li> <p>Deployment and Sustainment - Produces configuration files (e.g., Dockerfiles, CI/CD YAML), infrastructure-as-code templates, and observability scripts. GenAI can also assist in identifying performance tuning issues or log pattern anomalies.</p> </li> <li> <p>Validation and Compliance - Helps draft documentation, security policies, SBOM entries, and regulatory artifacts. Can be used to cross-check outputs against known standards\u2014though human validation remains essential.</p> </li> </ul> <p>The result is not just faster delivery\u2014but new interaction models, trust boundaries, and workforce behaviors across the SDLC. This site helps teams navigate these shifts securely and responsibly.</p>"},{"location":"#playbook-orientation","title":"Playbook Orientation","text":""},{"location":"#what-youll-find-here","title":"What You\u2019ll Find Here","text":"<p>Each Play in #AI4SDLC provides structured, no-nonsense guidance on adopting AI in secure, scalable, and mission-driven ways. Our plays cover:</p> <ul> <li>Hosting AI \u2013 Should you run AI on-prem, in the cloud, or hybrid? We break it down.  </li> <li>AI for Development \u2013 AI copilots, IDE plugins, and agentic workflows\u2014what works for DoW teams? </li> <li>Security &amp; Trust \u2013 How to govern AI tools, mitigate risk, and enforce compliance without slowing innovation.  </li> <li>AI for DevSecOps \u2013 Automating testing, security, and compliance while maintaining human oversight.  </li> <li>Operational Impact \u2013 AI isn\u2019t just a tool\u2014it\u2019s part of your mission execution strategy.</li> </ul> <p>No fluff. No hype. Just clear, actionable guidance.</p> <p>We also provide foundational knowledge to support teams across roles\u2014whether you\u2019re AI-curious or building for mission-critical environments. Topics include prompting, human-machine teaming, DevSecOps integration, and trust calibration.</p>"},{"location":"#available-and-upcoming-plays","title":"Available and Upcoming Plays","text":"<p>This guidance is structured as a series of modular \u201cplays,\u201d each focused on a specific aspect of AI-augmented software engineering:</p>"},{"location":"#available-plays","title":"Available Plays","text":"<ul> <li>Home / Basics \u2013 Terminology, responsible AI use, and the case for action  </li> <li>Fundamentals for Designing an AI-Augmented Tool Chain \u2013 Foundational interaction and integration patterns  </li> <li>Code Generation &amp; Completion \u2013 Leading practices, risks, and DevSecOps alignment for GenAI-driven authoring  </li> </ul>"},{"location":"#upcoming-plays","title":"Upcoming Plays","text":"<ul> <li>Governance for Responsible GenAI Adoption </li> <li>Building an AI-Augmented Workforce </li> <li>Risk Reference Companion </li> <li>Futures Watch: Agentics and Emerging Practices</li> </ul> <p>Each play is designed to stand alone while reinforcing shared principles\u2014so you can start where it matters most to your team or mission.</p>"},{"location":"#human-machine-teaming-themes","title":"Human-Machine Teaming Themes","text":"<p>AI augmentation isn\u2019t just a tooling upgrade\u2014it\u2019s a shift in how humans and machines collaborate across the software lifecycle. As organizations adopt GenAI, they must address more than integration and governance\u2014they must rethink roles, decision-making, and trust.</p> <p>The following themes appear throughout the AI4SDLC plays and reflect critical human-centric issues teams must face when responsibly using GenAI in secure, mission-aligned environments:</p>"},{"location":"#shifting-roles-from-creators-to-reviewers","title":"Shifting Roles: From Creators to Reviewers","text":"<p>GenAI shifts human work from hand-coding to reviewing, prompting, and validating machine-suggested outputs. This changes team dynamics, responsibilities, and required skills. [Read more \u2192 TBD]</p>"},{"location":"#cognitive-overload-and-prompting-fatigue","title":"Cognitive Overload and Prompting Fatigue","text":"<p>As teams adopt GenAI, developers and engineers are experiencing decision fatigue from unclear outputs, inconsistent prompting results, and lack of context. Effective prompting strategies and human-in-the-loop workflows are essential. [Read more \u2192 TBD]</p>"},{"location":"#calibrated-trust-and-human-oversight","title":"Calibrated Trust and Human Oversight","text":"<p>Trusting AI-generated outputs is not binary. Teams must design workflows that ensure the right level of human oversight based on risk, criticality, and system phase. [Read more \u2192 TBD]</p>"},{"location":"#human-machine-interaction-patterns","title":"Human-Machine Interaction Patterns","text":"<p>As teams integrate GenAI into their workflows, they aren't just choosing tools\u2014they're defining how humans and machines collaborate. From standalone web interfaces to agentic platforms, each interaction pattern affects traceability, trust, and governance. Whether you're prompting in an IDE, using an AI-first workspace, or delegating tasks to autonomous agents, it's critical to architect the right level of human oversight. [Read more \u2192 TBD]</p>"},{"location":"#who-should-use-this-playbook","title":"Who Should Use This Playbook?","text":"<p>This isn\u2019t just for software developers. If you\u2019re part of any software value stream in the DoW, AI is already impacting your world.</p> <ul> <li>Developers &amp; Engineers \u2013 AI-powered tools to accelerate coding &amp; debugging.  </li> <li>Security &amp; Compliance Teams \u2013 AI governance &amp; risk mitigation strategies.  </li> <li>Program &amp; Acquisition Managers \u2013 Links to Guidance on AI adoption and procurement within DoW policies.  </li> <li>AI Researchers &amp; Policy Leaders \u2013 Insights into AI\u2019s evolving role in DoW software.  </li> </ul> <p>If AI touches your software mission, #AI4SDLC is for you.</p>"},{"location":"#get-started-with-ai4sdlc","title":"Get Started with #AI4SDLC","text":"<ol> <li>Explore Plays \u2013 Browse AI guidance tailored for DoW SDLC.  </li> <li>New to AI in DoW? Start Here \u2013 Foundational insights to build your knowledge.  </li> <li>Latest AI Trends \u2013 Emerging AI use cases shaping DoW software development.  </li> </ol> <p>AI is not the future. It\u2019s now. Equip yourself with the right strategies to make it work for DoW missions.</p> <p>Mission-driven. Security-first. AI-powered. Welcome to #AI4SDLC.</p>"},{"location":"glossary/","title":"Glossary of Terms and Acronyms**","text":"Term / Acronym Definition ADR Architectural Decision Record AI Artificial Intelligence API Application Programming Interface ATO Authority to Operate AWS Amazon Web Services BOM Bill of Materials CD Continuous Delivery CFR Change Failure Rate CI Continuous Integration CUI Controlled Unclassified Information DB Database FLAN Fine-tuned Language Net (e.g., FLAN-T5) GPT Generative Pre-trained Transformer IDE Integrated Development Environment IL4 Impact Level 4 (DoD classification tier) ISR Intelligence, Surveillance, and Reconnaissance JWCC Joint Warfighting Cloud Capability LLM Large Language Model ML Machine Learning MLBOM Machine Learning Bill of Materials NIST National Institute of Standards and Technology RAG Retrieval-Augmented Generation RMF Risk Management Framework SBOM Software Bill of Materials SCIF Sensitive Compartmented Information Facility SCRM Supply Chain Risk Management SDLC Software Development Lifecycle ZTA Zero Trust Architecture"},{"location":"lifecycle/","title":"Guidance Lifecycle","text":"<p>This site uses a four-stage lifecycle system to indicate content maturity and help you identify the most appropriate guidance for your team's needs:</p> <p>\ud83d\udd3a ALPHA - Early draft under development. Incomplete content may contain placeholders or open questions. Intended for work group members, reviewers, and early contributors who can provide feedback on emerging concepts.</p> <p>\ud83d\udfe7 BETA - Core guidance is present and usable. Content is stable enough for practical application, but feedback is encouraged as refinements may still occur. Suitable for early adopters and pilot programs.</p> <p>\ud83d\udd37 RC - Release Candidate content is functionally complete and undergoing final validation. Few changes expected. Appropriate for broader stakeholders preparing for full adoption across their programs.</p> <p>\ud83d\udfe2 GA - General Availability content is fully reviewed and approved. This represents stable, recommended guidance for broad use across all DoW teams and mission partner environments.</p> <p>Each page displays its current lifecycle stage and last updated timestamp. Content may advance through stages based on validation, feedback, and operational experience\u2014some guidance may skip stages when appropriate for mission needs.</p> <p>This approach means you can quickly assess whether content aligns with your team's risk tolerance and implementation timeline while supporting the DoW's need for both innovation and mission assurance.</p>"},{"location":"plays/AI_supply_chain_transparency_guide/","title":"Play: AI Development Tool Supply Chain Transparency Guide","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#coming-soon","title":"\ud83d\udea7 COMING SOON \ud83d\udea7","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#executive-summary-the-play-in-brief","title":"Executive Summary (The Play in Brief)","text":"<p>As AI-powered development tools become embedded in DoD software factories\u2014from code completion plugins to automated testing frameworks\u2014traditional Software Bills of Materials (SBOMs) miss critical dependencies and risks. Organizations need visibility into the AI models powering their development tools, the data these tools access, and the dependencies they introduce into the software development lifecycle.</p> <p>This play provides practical guidance for extending SBOM practices to cover AI-augmented development tools, ensuring teams can track, audit, and secure the AI components that help them build software\u2014not the AI systems they deliver to users.</p> <ul> <li> <p>TL;DR: AI development tools introduce new supply chain risks that require enhanced SBOM practices to track models, training data, and tool dependencies within the SDLC.</p> </li> <li> <p>Intended audience: DevSecOps Engineers, Software Factory Architects, Tool Chain Managers, and Development Team Leads.</p> </li> <li> <p>Key takeaway: Treat AI development tools like any other SDLC dependency\u2014document them, version them, and manage their supply chain risks.</p> </li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#1-ai-development-tools-as-supply-chain-components","title":"1. AI Development Tools as Supply Chain Components","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#the-new-dependency-layer","title":"The New Dependency Layer","text":"<ul> <li>Code completion tools (GitHub Copilot, Amazon CodeWhisperer)</li> <li>AI-powered testing frameworks and test generators</li> <li>Documentation generators and code review assistants</li> <li>CI/CD optimization and deployment automation tools</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#why-traditional-sboms-miss-these-risks","title":"Why Traditional SBOMs Miss These Risks","text":"<ul> <li>AI models embedded in development tools aren't captured</li> <li>Training data influences on generated code aren't tracked</li> <li>Cloud-hosted AI services create external dependencies</li> <li>Tool updates can change behavior without version visibility</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#2-enhanced-sbom-practices-for-ai-development-tools","title":"2. Enhanced SBOM Practices for AI Development Tools","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#extending-current-sbom-frameworks","title":"Extending Current SBOM Frameworks","text":"<ul> <li>Adding AI tool metadata to existing SPDX/CycloneDx formats</li> <li>Tracking AI service endpoints and API dependencies</li> <li>Documenting model versions and update mechanisms</li> <li>Capturing tool configuration and prompt templates</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#tool-specific-documentation-requirements","title":"Tool-Specific Documentation Requirements","text":"<ul> <li>IDE plugins and extensions with AI capabilities</li> <li>CI/CD integrations with AI-powered optimization</li> <li>Testing frameworks using AI for test generation</li> <li>Documentation tools with AI-assisted content creation</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#3-model-transparency-for-development-tools","title":"3. Model Transparency for Development Tools","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#what-to-track-for-ai-powered-dev-tools","title":"What to Track for AI-Powered Dev Tools","text":"<ul> <li>Model architecture and capabilities used by the tool</li> <li>Training data sources and potential code exposure</li> <li>Version control and update frequency</li> <li>Performance characteristics and limitations</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#vendor-vs-self-hosted-tool-considerations","title":"Vendor vs. Self-Hosted Tool Considerations","text":"<ul> <li>SaaS-based tools: API dependencies and data flow</li> <li>On-premises tools: Model hosting and update management</li> <li>Hybrid approaches: Local processing with cloud augmentation</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#4-data-flow-and-privacy-in-ai-development-tools","title":"4. Data Flow and Privacy in AI Development Tools","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#code-and-context-exposure","title":"Code and Context Exposure","text":"<ul> <li>What code gets sent to AI models during development</li> <li>Retention policies for code snippets and context</li> <li>Data classification handling (CUI, export-controlled code)</li> <li>Anonymization and sanitization practices</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#audit-trail-requirements","title":"Audit Trail Requirements","text":"<ul> <li>Logging AI tool interactions and outputs</li> <li>Tracking generated code and its integration</li> <li>Maintaining provenance for AI-assisted development</li> <li>Supporting compliance and security reviews</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#5-integration-with-devsecops-pipelines","title":"5. Integration with DevSecOps Pipelines","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#pipeline-touch-points-for-ai-tool-transparency","title":"Pipeline Touch Points for AI Tool Transparency","text":"<ul> <li>IDE and development environment documentation</li> <li>Build system AI tool dependency tracking</li> <li>Testing pipeline AI component inventory</li> <li>Deployment automation AI service dependencies</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#automated-discovery-and-documentation","title":"Automated Discovery and Documentation","text":"<ul> <li>Tool scanning for AI-powered components</li> <li>API endpoint discovery and dependency mapping</li> <li>Configuration drift detection for AI tools</li> <li>Integration with existing SBOM generation workflows</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#6-risk-assessment-for-ai-development-tools","title":"6. Risk Assessment for AI Development Tools","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#security-considerations","title":"Security Considerations","text":"<ul> <li>Code injection risks from AI-generated content</li> <li>Supply chain attacks through AI tool compromises</li> <li>Data exfiltration through AI service interactions</li> <li>Model poisoning affecting code generation quality</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#operational-risks","title":"Operational Risks","text":"<ul> <li>Tool availability and service dependencies</li> <li>Model drift affecting development consistency</li> <li>Vendor lock-in and migration challenges</li> <li>Performance impact on development workflows</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#7-policy-and-governance-framework","title":"7. Policy and Governance Framework","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#ai-development-tool-approval-process","title":"AI Development Tool Approval Process","text":"<ul> <li>Evaluation criteria for new AI-powered tools</li> <li>Risk assessment and mitigation requirements</li> <li>Documentation standards for approved tools</li> <li>Regular review and reauthorization procedures</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#usage-guidelines-and-controls","title":"Usage Guidelines and Controls","text":"<ul> <li>Approved use cases for different tool types</li> <li>Data classification restrictions and handling</li> <li>Output validation and review requirements</li> <li>Training and awareness for development teams</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#8-implementation-roadmap","title":"8. Implementation Roadmap","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#phase-1-current-state-assessment-30-days","title":"Phase 1: Current State Assessment (30 days)","text":"<ul> <li>Inventory existing AI-powered development tools</li> <li>Map tool usage across development teams</li> <li>Identify gaps in current SBOM coverage</li> <li>Establish baseline risk assessment</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#phase-2-enhanced-documentation-60-days","title":"Phase 2: Enhanced Documentation (60 days)","text":"<ul> <li>Extend SBOM practices to include AI tools</li> <li>Implement automated discovery mechanisms</li> <li>Create tool-specific documentation templates</li> <li>Establish audit trail and logging requirements</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#phase-3-governance-and-control-90-days","title":"Phase 3: Governance and Control (90 days)","text":"<ul> <li>Deploy approval processes for new AI tools</li> <li>Implement monitoring and compliance checking</li> <li>Train teams on new documentation requirements</li> <li>Establish continuous improvement processes</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#9-practical-tools-and-templates","title":"9. Practical Tools and Templates","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#sbom-enhancement-examples","title":"SBOM Enhancement Examples","text":"<ul> <li>Sample entries for AI development tools</li> <li>Template extensions for common tool types</li> <li>Integration scripts for popular development environments</li> <li>Validation checklists for AI tool documentation</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<ul> <li>Automated detection of new AI tool usage</li> <li>Configuration change notifications</li> <li>Compliance violation alerts</li> <li>Performance impact monitoring</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#10-measuring-success","title":"10. Measuring Success","text":""},{"location":"plays/AI_supply_chain_transparency_guide/#key-metrics","title":"Key Metrics","text":"<ul> <li>Percentage of AI development tools documented in SBOMs</li> <li>Time to detect and document new AI tool adoption</li> <li>Compliance audit success rates</li> <li>Security incident response effectiveness</li> </ul>"},{"location":"plays/AI_supply_chain_transparency_guide/#continuous-improvement","title":"Continuous Improvement","text":"<ul> <li>Regular assessment of documentation coverage</li> <li>Tool effectiveness evaluation and optimization</li> <li>Process refinement based on lessons learned</li> <li>Integration with broader supply chain risk management</li> </ul> <p>Next Play: TBD*</p>"},{"location":"plays/ai-autonomy-implementation-guide/","title":"AI Autonomy Implementation Guide","text":""},{"location":"plays/ai-autonomy-implementation-guide/#companion-to-navigating-the-ai-autonomy-continuum","title":"Companion to: Navigating the AI Autonomy Continuum","text":"<p>This implementation guide provides the tactical details needed to successfully deploy AI autonomy patterns across your software development lifecycle. While the main play focuses on strategic decision-making and pattern selection, this guide contains the operational checklists, monitoring frameworks, and implementation procedures required to execute those decisions safely and effectively.</p>"},{"location":"plays/ai-autonomy-implementation-guide/#who-should-use-this-guide","title":"Who Should Use This Guide","text":"<p>This guide is designed for implementation teams who need to translate strategic AI autonomy decisions into operational reality:</p> <ul> <li>DevSecOps Leads implementing pattern-specific governance and monitoring</li> <li>Software Factory Architects designing infrastructure to support AI autonomy patterns</li> <li>AI/ML Engineers deploying and maintaining AI agents and orchestration systems</li> <li>Security Engineers establishing audit trails and compliance controls</li> <li>Platform Engineers building the foundational capabilities for multi-pattern environments</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#how-to-use-this-guide","title":"How to Use This Guide","text":"<p>Start with the Pattern You're Implementing: Each pattern section is self-contained, so you can focus on the specific autonomy approach your team has selected.</p> <p>Use Readiness Assessment First: Before deploying any pattern, complete the readiness indicators to ensure your organization has the necessary capabilities.</p> <p>Implement Monitoring from Day One: The health metrics and warning signs are designed to be deployed alongside your AI systems, not added later.</p> <p>Iterate and Improve: The continuous improvement actions help you refine your implementation based on real-world experience.</p>"},{"location":"plays/ai-autonomy-implementation-guide/#cross-references-to-main-play","title":"Cross-References to Main Play","text":"<p>For strategic guidance on when and why to use each pattern, see Navigating the AI Autonomy Continuum:</p> <ul> <li>Pattern Selection Framework: Section 3 of the main play</li> <li>Risk Considerations: Section 4 (Cross-Cutting Considerations)</li> <li>Multi-Pattern Architecture: Section 5 (How to Use This Play)</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#implementation-guide-contents","title":"Implementation Guide Contents","text":""},{"location":"plays/ai-autonomy-implementation-guide/#part-i-pattern-specific-implementation","title":"Part I: Pattern-Specific Implementation","text":"<ul> <li>Pattern 1 - Assistive Tools Implementation</li> <li>Pattern 2 - Delegated Agents Implementation  </li> <li>Pattern 3 - Orchestrated Systems Implementation</li> <li>Pattern 4 - Adaptive Ecosystems Implementation</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#part-ii-multi-pattern-operations-coming-soon","title":"Part II: Multi-Pattern Operations (Coming Soon)","text":"<ul> <li>Cross-Pattern Governance</li> <li>Integrated Monitoring and Alerting</li> <li>Pattern Interaction Management</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#part-iii-troubleshooting-and-optimization-coming-soon","title":"Part III: Troubleshooting and Optimization (Coming Soon)","text":"<ul> <li>Common Implementation Challenges</li> <li>Shadow AI Detection and Response</li> <li>Trust Calibration Techniques</li> <li>Continuous Improvement Processes</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#part-i-pattern-specific-implementation_1","title":"Part I: Pattern-Specific Implementation","text":""},{"location":"plays/ai-autonomy-implementation-guide/#pattern-1-assistive-tools-implementation","title":"Pattern 1 - Assistive Tools Implementation","text":""},{"location":"plays/ai-autonomy-implementation-guide/#pattern-1-readiness-indicators","title":"Pattern 1 Readiness Indicators","text":"<p>Before implementing assistive AI tools, ensure your organization has the foundational capabilities to use them safely and effectively:</p> <p>Infrastructure &amp; Integration:</p> <ul> <li>[ ] IDE environments support approved AI plug-ins or extensions</li> <li>[ ] Network policies allow controlled access to AI services (on-prem or approved SaaS)</li> <li>[ ] Logging infrastructure can capture AI tool interactions and outputs</li> </ul> <p>Governance &amp; Policy:</p> <ul> <li>[ ] Approved AI tools list established and communicated</li> <li>[ ] Data classification policies cover AI tool usage restrictions</li> <li>[ ] Incident response procedures include AI-related scenarios</li> <li>[ ] Procurement processes evaluate AI tools for security and compliance</li> </ul> <p>Workforce &amp; Culture:</p> <ul> <li>[ ] Teams trained on appropriate AI tool usage and limitations</li> <li>[ ] Human review practices established for AI-generated outputs</li> <li>[ ] Clear escalation paths when AI suggestions are unclear or concerning</li> <li>[ ] Trust calibration understanding: treating AI as assistant, not authority</li> </ul> <p>Security &amp; Compliance:</p> <ul> <li>[ ] AI tool usage aligns with Zero Trust architecture principles</li> <li>[ ] Audit trails capture sufficient detail for compliance requirements</li> <li>[ ] Shadow AI detection and prevention mechanisms in place</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-1-health-metrics","title":"Pattern 1 Health Metrics","text":"<p>Monitor these indicators to ensure Pattern 1 implementation remains effective and secure:</p> <p>Usage &amp; Adoption Metrics:</p> <ul> <li>Human Review Rate: % of AI-generated outputs reviewed before implementation (target: \u226595% for substantive outputs)<sup>1</sup></li> <li>Tool Utilization: % of approved teams actively using sanctioned AI tools</li> <li>Shadow AI Incidents: Number of unauthorized AI tool usage events detected</li> <li>Prompt Reuse: % of prompts following approved templates or patterns</li> </ul> <p>Quality &amp; Effectiveness Metrics:</p> <ul> <li>Output Acceptance Rate: % of AI suggestions accepted without modification</li> <li>Time to Review: Average time spent reviewing AI-generated content</li> <li>Defect Escape Rate: % of AI-assisted code changes that introduce bugs</li> <li>Developer Satisfaction: Team sentiment scores regarding AI tool usefulness</li> </ul> <p>Security &amp; Governance Metrics:</p> <ul> <li>Audit Completeness: % of AI interactions properly logged and attributable</li> <li>Policy Compliance: % of AI tool usage following approved guidelines</li> <li>Escalation Response: Time to address AI-related security or quality concerns</li> <li>Trust Calibration: Ratio of appropriate acceptance vs. rejection of AI suggestions</li> </ul> <p>Warning Signs to Monitor:</p> <ul> <li>\ud83d\udd34 Over-reliance: Teams accepting AI outputs without adequate review</li> <li>\ud83d\udd34 Under-utilization: Low adoption rates suggesting tool-mission mismatch</li> <li>\ud83d\udd34 Shadow AI Growth: Increasing use of unauthorized tools or services</li> <li>\ud83d\udd34 Review Fatigue: Declining quality of human review over time</li> <li>\ud83d\udd34 Trust Drift: Teams becoming overly skeptical or overly trusting of AI outputs</li> </ul> <p>Continuous Improvement Actions: - Quarterly review of approved tool effectiveness and team feedback - Regular updates to prompt libraries based on lessons learned - Adjustment of review requirements based on trust calibration data - Enhancement of training programs based on observed usage patterns</p> <p>This aligns with NIST AI RMF's principle of maintaining human control and oversight in AI workflows.<sup>2</sup></p>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-2-delegated-agents-implementation","title":"Pattern 2 - Delegated Agents Implementation","text":""},{"location":"plays/ai-autonomy-implementation-guide/#pattern-2-readiness-indicators","title":"Pattern 2 Readiness Indicators","text":"<p>Before implementing delegated AI agents, ensure your organization can support scoped autonomy safely:</p> <p>Infrastructure &amp; Integration:</p> <ul> <li>[ ] Agent execution platforms (IDEs, CI/CD systems) support controlled agent deployment</li> <li>[ ] Permission management systems can enforce least-privilege agent access</li> <li>[ ] Comprehensive logging captures all agent actions and their human triggers</li> <li>[ ] Rollback mechanisms can quickly revert agent-generated changes</li> </ul> <p>Governance &amp; Policy:</p> <ul> <li>[ ] Agent registration and approval processes established</li> <li>[ ] Clear scope definitions for what agents can and cannot do</li> <li>[ ] Human review requirements defined for different agent outputs</li> <li>[ ] Escalation procedures when agents exceed intended scope</li> </ul> <p>Workforce &amp; Culture:</p> <ul> <li>[ ] Teams trained on supervising and reviewing agent outputs</li> <li>[ ] Clear accountability models for agent-generated artifacts</li> <li>[ ] Trust calibration practices for evaluating agent reliability</li> <li>[ ] Skills for prompt engineering and agent task definition</li> </ul> <p>Security &amp; Compliance:</p> <ul> <li>[ ] Agent access controls integrated with existing security frameworks</li> <li>[ ] Supply chain scanning covers agent-generated dependencies and code</li> <li>[ ] Audit trails meet compliance requirements for traceability</li> <li>[ ] Incident response procedures cover agent-related security events</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-2-health-metrics","title":"Pattern 2 Health Metrics","text":"<p>Monitor these indicators to ensure Pattern 2 agents operate safely and effectively:</p> <p>Agent Performance Metrics:</p> <ul> <li>Task Completion Rate: % of agent-initiated tasks completed successfully</li> <li>Human Intervention Rate: % of agent tasks requiring human correction or completion</li> <li>Output Acceptance Rate: % of agent outputs accepted without modification</li> <li>Time to Task Completion: Agent efficiency compared to human baseline</li> </ul> <p>Governance &amp; Security Metrics:</p> <ul> <li>Agent Registration Compliance: % of active agents properly registered and approved</li> <li>Scope Adherence: % of agent actions staying within defined boundaries</li> <li>Audit Trail Completeness: % of agent actions properly logged and attributable</li> <li>Permission Violation Incidents: Number of agents attempting unauthorized actions</li> </ul> <p>Quality &amp; Trust Metrics: - Defect Injection Rate: % of agent outputs introducing bugs or issues - Review Effectiveness: % of problematic agent outputs caught during human review - Trust Calibration Score: Alignment between team confidence and actual agent reliability - Escalation Response Time: Speed of human intervention when agents encounter problems</p> <p>Warning Signs to Monitor: - \ud83d\udd34 Scope Creep: Agents attempting tasks beyond their defined boundaries - \ud83d\udd34 Review Bypass: Agent outputs being accepted without proper human oversight - \ud83d\udd34 Error Amplification: Agent mistakes propagating through multiple workflows - \ud83d\udd34 Shadow Agent Activity: Use of unauthorized agents or SaaS services - \ud83d\udd34 Trust Miscalibration: Teams over- or under-trusting agent capabilities</p> <p>Continuous Improvement Actions: - Regular review of agent scope definitions and success criteria - Refinement of prompt libraries and agent task templates - Enhancement of human review processes based on error patterns - Updates to agent permissions and controls based on observed behavior - Training adjustments based on trust calibration and supervision effectiveness</p>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-3-orchestrated-systems-implementation","title":"Pattern 3 - Orchestrated Systems Implementation","text":""},{"location":"plays/ai-autonomy-implementation-guide/#pattern-3-readiness-indicators","title":"Pattern 3 Readiness Indicators","text":"<p>Before implementing orchestrated multi-agent systems, ensure your organization can manage coordinated AI autonomy:</p> <p>Infrastructure &amp; Architecture:</p> <ul> <li>[ ] Orchestration platform capable of managing multi-agent workflows</li> <li>[ ] Service mesh or API gateway supporting secure agent-to-agent communication</li> <li>[ ] Comprehensive observability stack tracking cross-agent interactions</li> <li>[ ] Circuit breakers and fault isolation to prevent cascading failures</li> <li>[ ] Rollback capabilities for multi-agent workflow states</li> </ul> <p>Governance &amp; Policy:</p> <ul> <li>[ ] Multi-agent coordination policies and approval processes</li> <li>[ ] Inter-agent trust and verification protocols established</li> <li>[ ] Escalation triggers and human intervention procedures defined</li> <li>[ ] Agent lifecycle management (deployment, updates, retirement)</li> <li>[ ] Cross-functional governance covering all participating agent roles</li> </ul> <p>Workforce &amp; Culture:</p> <ul> <li>[ ] Teams skilled in multi-agent system supervision and debugging</li> <li>[ ] Orchestration platform administration and monitoring capabilities</li> <li>[ ] Incident response procedures for multi-agent system failures</li> <li>[ ] Understanding of emergent behaviors in agent interactions</li> </ul> <p>Security &amp; Compliance:</p> <ul> <li>[ ] Zero Trust principles applied to agent-to-agent communications</li> <li>[ ] End-to-end audit trails across all agent interactions</li> <li>[ ] Security controls preventing unauthorized agents from joining workflows</li> <li>[ ] Compliance validation that spans multi-agent processes</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-3-health-metrics","title":"Pattern 3 Health Metrics","text":"<p>Monitor these indicators to ensure orchestrated multi-agent systems operate safely and effectively:</p> <p>Orchestration Performance Metrics:</p> <ul> <li>Workflow Completion Rate: % of multi-agent workflows completed successfully end-to-end</li> <li>Agent Coordination Efficiency: Time and resource overhead of agent coordination vs. single-agent alternatives</li> <li>Escalation Rate: % of workflows requiring human intervention</li> <li>Cross-Agent Verification Success: % of agent outputs properly validated by peer agents</li> </ul> <p>System Resilience Metrics:</p> <ul> <li>Fault Isolation Effectiveness: % of agent failures contained without cascading effects</li> <li>Circuit Breaker Activation Rate: Frequency of automatic failsafes preventing system-wide issues</li> <li>Recovery Time: Speed of system recovery from multi-agent failures</li> <li>Rollback Success Rate: % of successful rollbacks when multi-agent workflows fail</li> </ul> <p>Governance &amp; Security Metrics:</p> <ul> <li>Agent Registry Compliance: % of active agents properly registered in orchestration system</li> <li>Inter-Agent Trust Validation: % of agent-to-agent communications properly authenticated and authorized</li> <li>Audit Trail Completeness: % of cross-agent interactions properly logged and traceable</li> <li>Unauthorized Agent Detection: Number of rogue or unauthorized agents detected and blocked</li> </ul> <p>Warning Signs to Monitor:</p> <ul> <li>\ud83d\udd34 Cascading Failures: Agent errors propagating across multiple workflow stages</li> <li>\ud83d\udd34 Orchestration Bottlenecks: Single points of failure in agent coordination</li> <li>\ud83d\udd34 Trust Chain Breaks: Agent outputs being consumed without proper validation</li> <li>\ud83d\udd34 Emergent Behaviors: Unexpected interactions between agents leading to unintended outcomes</li> <li>\ud83d\udd34 Supervision Gaps: Human operators losing visibility into multi-agent system state</li> </ul> <p>Continuous Improvement Actions:</p> <ul> <li>Regular testing of multi-agent failure scenarios and recovery procedures</li> <li>Refinement of agent coordination protocols based on observed interaction patterns</li> <li>Enhancement of observability tools to better track cross-agent dependencies</li> <li>Updates to orchestration logic based on workflow efficiency analysis</li> <li>Training programs for human supervisors on multi-agent system management</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-4-adaptive-ecosystems-implementation","title":"Pattern 4 - Adaptive Ecosystems Implementation","text":""},{"location":"plays/ai-autonomy-implementation-guide/#pattern-4-readiness-indicators","title":"Pattern 4 Readiness Indicators","text":"<p>Before considering self-optimizing AI systems, organizations must demonstrate mastery of adaptive governance and autonomous system stewardship:</p> <p>Infrastructure &amp; Architecture:</p> <ul> <li>[ ] Fully autonomous CI/CD pipelines with comprehensive safety controls</li> <li>[ ] Real-time telemetry and feedback loops integrated across entire SDLC</li> <li>[ ] Advanced AI/ML infrastructure capable of continuous model training and deployment</li> <li>[ ] Comprehensive rollback and circuit breaker systems tested under failure conditions</li> <li>[ ] Observability stack providing full visibility into autonomous system behavior</li> </ul> <p>Governance &amp; Policy:</p> <ul> <li>[ ] Adaptive governance frameworks that can evolve with system behavior</li> <li>[ ] Continuous compliance monitoring and enforcement mechanisms</li> <li>[ ] Emergent behavior detection and response protocols</li> <li>[ ] Human override capabilities with tested escalation procedures</li> <li>[ ] Mission alignment monitoring and course correction systems</li> </ul> <p>Workforce &amp; Culture:</p> <ul> <li>[ ] Teams skilled in autonomous system stewardship and governance</li> <li>[ ] Platform engineering capabilities for managing self-optimizing systems</li> <li>[ ] Incident response expertise for autonomous system failures</li> <li>[ ] Deep understanding of AI safety and alignment principles</li> </ul> <p>Security &amp; Compliance:</p> <ul> <li>[ ] Autonomous security controls that adapt to emerging threats</li> <li>[ ] Continuous compliance validation across all optimization loops</li> <li>[ ] Advanced threat detection for AI-specific attack vectors</li> <li>[ ] Secure autonomous decision-making with full audit capabilities</li> </ul>"},{"location":"plays/ai-autonomy-implementation-guide/#pattern-4-health-metrics","title":"Pattern 4 Health Metrics","text":"<p>Monitor these indicators to ensure self-optimizing systems remain aligned and safe:</p> <p>Autonomous Operation Metrics:</p> <ul> <li>Optimization Effectiveness: Measurable improvements in target metrics (performance, security, user satisfaction)</li> <li>Human Intervention Rate: % of optimization cycles requiring human override or correction</li> <li>Goal Alignment: Consistency between autonomous optimizations and defined mission objectives</li> <li>Feedback Loop Integrity: Quality and reliability of telemetry driving optimization decisions</li> </ul> <p>Safety &amp; Governance Metrics:</p> <ul> <li>Emergent Behavior Detection: Number of unexpected system behaviors identified and addressed</li> <li>Compliance Maintenance: % of autonomous changes maintaining regulatory and policy compliance</li> <li>Rollback Effectiveness: Success rate and speed of reverting problematic autonomous changes</li> <li>Human Override Response: Time and effectiveness of human intervention when triggered</li> </ul> <p>System Resilience Metrics:</p> <ul> <li>Circuit Breaker Activation: Frequency and effectiveness of automated safety controls</li> <li>Drift Detection: Ability to identify when system behavior diverges from expected patterns</li> <li>Explainability Maintenance: Continued ability to interpret and explain autonomous decisions</li> <li>Trust Calibration: Alignment between system confidence and actual performance</li> </ul> <p>Warning Signs to Monitor:</p> <ul> <li>\ud83d\udd34 Goal Drift: Autonomous optimizations moving away from mission objectives</li> <li>\ud83d\udd34 Compliance Violations: System changes bypassing required regulatory or security controls</li> <li>\ud83d\udd34 Emergent Failures: Unexpected system behaviors leading to degraded performance or failures</li> <li>\ud83d\udd34 Explanation Gaps: Inability to understand or explain autonomous system decisions</li> <li>\ud83d\udd34 Override Failures: Human intervention systems failing to work when needed</li> </ul> <p>Continuous Improvement Actions:</p> <ul> <li>Regular evaluation of optimization effectiveness and mission alignment</li> <li>Enhancement of emergent behavior detection and response capabilities</li> <li>Refinement of governance frameworks based on autonomous system evolution</li> <li>Updates to safety controls and human override mechanisms</li> <li>Ongoing training for human stewards managing autonomous systems</li> </ul> <p>End of Implementation Guide</p> <ol> <li> <p>SmartBear Software, \"State of Software Quality | Code Review,\" 2022. [Online]. Available: https://smartbear.com/state-of-software-quality/code-review/. [Accessed: Aug. 18, 2025]\u00a0\u21a9</p> </li> <li> <p>National Institute of Standards and Technology, Artificial Intelligence Risk Management Framework (AI RMF) 1.0, Gaithersburg, MD, USA, 2023. [Online]. Available: https://nvlpubs.nistpubs/ai/nist.ai.100-1.pdf. [Accessed: Aug. 18, 2025]\u00a0\u21a9</p> </li> </ol>"},{"location":"plays/ai-autonomy_continuum_play/","title":"Play: Navigating the AI Autonomy Continuum","text":""},{"location":"plays/ai-autonomy_continuum_play/#executive-summary-the-play-in-brief","title":"Executive Summary (The Play in Brief)","text":"<p>AI in the SDLC isn't a binary leap from \"manual\" to \"fully automated.\" It's a continuum of AI autonomy patterns \u2014 from assistive tools embedded in workflows to self-optimizing ecosystems capable of continuous innovation.  </p> <p>This play provides a DoD-aligned framework for understanding four AI autonomy patterns that exist along this continuum, helping leaders architect secure, mission-aligned systems while maintaining calibrated trust. Unlike traditional maturity models that suggest linear progression, these patterns can be implemented simultaneously across different SDLC workflows based on mission requirements, risk tolerance, and operational context.</p> <p>Each pattern along the autonomy continuum has distinct architectural, governance, and workforce implications. Organizations can operate multiple patterns in parallel \u2014 using assistive tools for sensitive code while deploying orchestrated agents for CI/CD pipelines. Selecting the wrong pattern for a given workflow, or implementing patterns without proper readiness, can create mission risk, security gaps, and compliance challenges.</p> <ul> <li> <p>Intended audience: CIOs, Chief Engineers, Program Managers, Software Factory Architects, DevSecOps Leads, AI/ML Engineers, Policy Makers</p> </li> <li> <p>Key takeaway: The AI autonomy continuum offers multiple patterns that can coexist. Success depends on matching the right pattern to the right workflow \u2014 autonomy is earned through readiness, not granted through technology adoption.</p> </li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#1-why-this-play-matters","title":"1. Why This Play Matters","text":"<ul> <li>DoD and federal agencies face pressure to leverage AI for speed, adaptability, and scale.</li> <li>Selecting inappropriate AI autonomy patterns for specific workflows risks security, compliance, and operational stability.</li> <li>A structured continuum of patterns helps teams make mission-aligned decisions and implement AI capabilities with confidence.</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#2-what-is-autonomy-and-what-is-an-ai-agent","title":"2. What is Autonomy and What is an AI Agent?","text":""},{"location":"plays/ai-autonomy_continuum_play/#21-what-is-ai-autonomy","title":"2.1 What is AI Autonomy?","text":"<p>AI Autonomy is the ability of an artificial intelligence system to operate and make decisions with minimal or no human intervention. Autonomy describes the degree to which a system can sense its environment, plan actions, and execute them without continuous human input.</p> <p>Over the past fifty years, multiple taxonomies have emerged to describe levels of autonomy across domains such as avionics, manufacturing, and power systems. These taxonomies generally range from no automation (full reliance on humans) to full automation (a system acting entirely on its own within expected contexts).</p> <ul> <li>NIST ALFUS Framework: NIST\u2019s Autonomy Levels for Unmanned Systems (ALFUS) describes autonomy in terms of context complexity, human independence, and mission capability <sup>1</sup>.</li> <li>DoD Levels of Autonomy (LOA): The Department of Defense defines autonomy in operational systems with an emphasis on governance, human\u2013machine teaming, and mission risk.</li> </ul> <p>\ud83d\udccc Key insight: Most existing taxonomies emphasize \u201cautomation\u201d rather than \u201cteaming.\u201d For the SDLC, our focus is on autonomy as part of human\u2013AI collaboration, not replacement.</p>"},{"location":"plays/ai-autonomy_continuum_play/#22-what-is-an-ai-agent","title":"2.2 What is an AI Agent?","text":"<p>For the purpose of this play, we define an AI Agent as:</p> <p>A software system that can interact with its environment, gather information, and use that information to plan and execute actions in pursuit of goals, often with some degree of autonomy.</p> <p>Key properties:</p> <ul> <li>Observation: Agents interface with APIs, data, metrics, or user inputs, often with memory of past interactions.</li> <li>Planning: Agents reason about goals, constraints, and roles, using models such as LLMs for reasoning or domain-specific logic.</li> <li>Action: Agents act by calling tools, updating code or artifacts, delegating to other agents, or prompting humans for clarification.<sup>2</sup></li> </ul> <p></p>"},{"location":"plays/ai-autonomy_continuum_play/#expert-perspectives","title":"Expert Perspectives","text":"<p>Different communities frame AI agents through complementary lenses:</p> <p>Researchers like Masterman et al. define agents as \u201centities able to plan and take actions to execute goals.\u201d<sup>3</sup> MITRE experts build on this, with Dr. Peter Schwartz emphasizing that agents should be judged by what they do, not how they do it. Dr. Keith Miller extends this view, describing agentic systems as ranging from simple copilots to orchestrated multi-agent collectives with shared awareness of state, beliefs, and goals.</p> <p>For this play, we focus on AI Agents that leverage LLMs, while recognizing that smaller or domain-specific agents may not require LLMs.</p>"},{"location":"plays/ai-autonomy_continuum_play/#evolution-of-intelligent-agents","title":"Evolution of Intelligent Agents","text":"<p>The progression of intelligent agents parallels the SDLC\u2019s increasing automation \u2014 from static helpers to adaptive collaborators.</p> <ul> <li>1950s\u20131970s: Rule-Based Systems -  Early expert systems like MYCIN followed hard-coded if\u2013then rules. They were predictable but brittle, with no learning.<sup>4</sup></li> </ul> <p>SDLC Parallel: Early linters and static checkers that flagged rule violations without adapting to context.</p> <ul> <li>1980s\u20131990s: Adaptive Agents -    Machine learning and reinforcement learning introduced adaptability, enabling agents to refine behavior from data and feedback.<sup>5</sup></li> </ul> <p>SDLC Parallel: Bug prediction models that learned from historical commit data to anticipate likely error-prone code.</p> <ul> <li>2000s\u20132010s: Deep Learning and Context Awareness -    Deep neural networks allowed agents to process unstructured data (text, images, speech) and recognize patterns across large contexts.<sup>6</sup></li> </ul> <p>SDLC Parallel: Automated anomaly detection in CI/CD pipelines, identifying unusual build failures or performance regressions.</p> <ul> <li>2018\u2013Present: LLMs and Multi-Agent Systems -    Large Language Models unlocked reasoning, planning, and natural language interaction. Agents now collaborate in multi-agent systems, coordinating tasks across the SDLC.<sup>7</sup></li> </ul> <p>SDLC Parallel: Code copilots and orchestrators that generate, review, and integrate code, or coordinate multiple agents in DevSecOps pipelines.</p> <p>\ud83d\udccc Key Insight: The trajectory is clear: from deterministic rule-followers \u2192 adaptive learners \u2192 context-aware analyzers \u2192 reasoning collaborators. This same arc defines the autonomy continuum and the patterns we now apply to the SDLC.</p>"},{"location":"plays/ai-autonomy_continuum_play/#how-are-ai-agents-used-today","title":"How are AI Agents Used Today?","text":"<p>Modern AI agents often mimic the cognitive loop humans follow: observe, plan, act.</p> <ul> <li>Observe: Gather data from APIs, sensors, logs, or prior context.</li> <li>Plan: Define goals, sequence steps, allocate tasks, or query reasoning models.</li> <li>Act: Execute via digital systems, delegate to other agents, or request clarification from humans.</li> </ul> <p>This cycle is continuous, enabling agents to adapt to new information in near real time.</p>"},{"location":"plays/ai-autonomy_continuum_play/#when-are-ai-agents-beneficial","title":"When Are AI Agents Beneficial?","text":"<ul> <li>Automating repetitive tasks to free humans for higher-value work.</li> <li>Operating in complex or dynamic environments where context must be tracked across interactions.</li> <li>Enhancing customer or user support with consistent, context-aware responses.</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#when-to-avoid-ai-agents","title":"When to Avoid AI Agents","text":"<ul> <li>Tasks requiring human judgment, creativity, or ethics (e.g., strategic decision-making, empathy-driven contexts).</li> <li>High-consequence domains where errors may cause material harm and are difficult to detect.</li> <li>Scenarios demanding emotional intelligence, which AI cannot authentically replicate.</li> </ul> <p>\ud83d\udccc Key Insight \"The concept of software agents has been around for a long time, but of course now they\u2019re supercharged by LLMs. LLMs suddenly give them the ability to interact with humans through natural language and pictures, provide reasonable zero-shot responses, perform commonsense reasoning, construct their own plans, perform in-context learning, generate/proofread/summarize documents, generate/debug/execute code, etc. This opens the door to a much higher degree of task complexity than they were capable of just a few years ago.\"</p> <p>- Dr. Peter Schwartz, MITRE</p> <p>For the purpose of this play, AI Agents will be limited to those that utilize LLMs bearing in mind, emerging technology and processes may change the landscape.</p> <p>Understanding these agent capabilities helps teams select appropriate autonomy patterns for different SDLC workflows - matching agent sophistication to mission requirements and risk tolerance.</p>"},{"location":"plays/ai-autonomy_continuum_play/#3-ai-autonomy-patterns-for-the-software-development-lifecycle","title":"3. AI Autonomy Patterns for the Software Development Lifecycle","text":""},{"location":"plays/ai-autonomy_continuum_play/#understanding-the-ai-autonomy-continuum","title":"Understanding the AI Autonomy Continuum","text":"<p>AI autonomy in the SDLC isn't a single implementation choice\u2014it's a continuum of capabilities that spans from simple assistive tools to self-optimizing ecosystems. Rather than treating this as a linear progression where teams must \"advance\" from one level to the next, real-world adoption follows a pattern-based approach where multiple autonomy levels operate simultaneously across different parts of the software delivery pipeline.</p> <p>This section introduces four foundational AI autonomy patterns that can be applied independently or in combination, based on mission requirements, risk tolerance, and the specific characteristics of each SDLC workflow.</p>"},{"location":"plays/ai-autonomy_continuum_play/#why-patterns-not-stages","title":"Why Patterns, Not Stages?","text":"<p>Traditional maturity models suggest organizations should progress through sequential stages, eventually \"graduating\" to the highest level. But AI autonomy in mission-critical environments works differently:</p> <p>Real-world complexity demands flexibility:</p> <ul> <li>Authentication code may require Pattern 1 (human-controlled tools) for security assurance</li> <li>Test generation can safely use Pattern 2 (delegated agents) for efficiency  </li> <li>CI/CD pipelines might benefit from Pattern 3 (orchestrated systems) for coordination</li> <li>Performance optimization could experiment with Pattern 4 (adaptive ecosystems) in controlled environments</li> </ul> <p>Risk varies by context, not organizational maturity: A highly mature DevSecOps team might deliberately choose Pattern 1 for cryptographic code while simultaneously running Pattern 3 for deployment orchestration. The \"best\" pattern depends on the specific workflow, data sensitivity, and consequences of failure\u2014not the team's overall AI sophistication.</p> <p>Patterns enable parallel adoption: Teams can introduce new patterns incrementally without abandoning proven approaches. A successful Pattern 1 implementation doesn't become obsolete when Pattern 2 is introduced\u2014it continues operating in the contexts where human oversight provides the most value.</p>"},{"location":"plays/ai-autonomy_continuum_play/#the-four-ai-autonomy-patterns","title":"The Four AI Autonomy Patterns","text":"<p>Each pattern represents a distinct approach to human-AI collaboration in the SDLC, with different architectural requirements, governance needs, and risk profiles:</p> <ul> <li>Pattern 1 - Assistive Tools: AI augments human capabilities with bounded, human-triggered assistance</li> <li>Pattern 2 - Delegated Agents: AI executes complete workflows within defined scopes and human oversight  </li> <li>Pattern 3 - Orchestrated Systems: Multiple AI agents coordinate across SDLC phases under policy-driven governance</li> <li>Pattern 4 - Adaptive Ecosystems: Self-optimizing AI systems that continuously improve based on real-world feedback</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#selecting-the-right-pattern","title":"Selecting the Right Pattern","text":"<p>The most important architectural decision isn't which pattern to implement\u2014it's which pattern fits each specific use case. This requires teams to evaluate:</p> <ul> <li>Mission criticality: How much risk can this workflow tolerate?</li> <li>Human expertise: What level of AI oversight can the team provide?</li> <li>Governance requirements: What audit, compliance, and traceability standards apply?</li> <li>Technical boundaries: What are the integration points and failure modes?</li> </ul> <p>The following sections detail each pattern's characteristics, architectural implications, and guidance for responsible implementation. Remember: the goal isn't to achieve the \"highest\" pattern\u2014it's to match the right pattern to the right problem while maintaining mission assurance and operational control.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-1-ai-as-a-tool-ai-augmented-tools","title":"Pattern 1 \u2013 AI as a Tool (AI-Augmented Tools)","text":"<p>Definition: AI assists humans by automating bounded, repetitive tasks under direct human control. Tools in this pattern operate within predefined rules or narrow functions. They require a human to initiate, supervise, and validate their use. They improve efficiency but do not alter team dynamics or overall workflows.</p> <p>SDLC Lens: At this level, AI appears as assistive add-ons inside existing tools and environments. Examples include autocomplete in an IDE, inline documentation, or static analysis hints. Importantly, all usage is human-triggered and human-reviewed.</p> <p>Examples:</p> <ul> <li>Code completion (e.g., Copilot-style suggestions).</li> <li>Inline documentation and comment generation.</li> <li>Backlog grooming aids (AI-generated user stories).</li> <li>Static analysis augmentation with AI explanations.</li> </ul> <p>Architectural &amp; Risk Implications:</p> <ul> <li>Integration: Minimal complexity \u2014 typically IDE plug-ins, SaaS integrations, or extensions to existing tools.</li> <li>Validation: Humans must validate all outputs; unchecked adoption risks code quality and security.</li> <li>Auditability: Logging of prompts, outputs, and tool usage is essential to ensure traceability.</li> <li>Governance: The biggest risk at this pattern is \"Shadow AI\" \u2014 when developers use unapproved AI tools outside organizational visibility and control. <sup>8</sup></li> </ul> <p>This aligns with the DoD DevSecOps Reference Design, which requires human-in-the-loop review for all code and configuration changes as part of CI/CD pipelines.</p> <p>Shadow AI Risk at Pattern 1: Shadow AI refers to the use of AI tools, plug-ins, or services outside approved governance, visibility, and security controls.</p> <ul> <li>Example: A developer pastes source code into ChatGPT, Copilot, or an unvetted SaaS tool, unintentionally leaking proprietary, export-controlled, or classified data.</li> <li>Analogy: This is the AI version of \"shadow IT,\" where teams spin up unsanctioned cloud services without organizational approval.</li> <li>Implication: Without governance, organizations lose visibility into how AI is being used, what data it touches, and what risks are being introduced.</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-1-selection-guidance","title":"Pattern 1 Selection Guidance","text":"<p>When Pattern 1 is the Right Choice:</p> <p>Pattern 1 is ideal for workflows that benefit from AI assistance but require human judgment, involve sensitive data, or operate in high-assurance environments where human accountability cannot be delegated.</p> <p>Optimal Use Cases:</p> <ul> <li>Authentication and authorization code development</li> <li>Cryptographic implementations (review assistance only)</li> <li>Requirements analysis and documentation</li> <li>Code review and static analysis augmentation</li> <li>Learning and skill development for developers</li> <li>Prototyping and experimentation in sandboxed environments</li> </ul> <p>Risk-Benefit Analysis:</p> <ul> <li>Low to moderate risk tolerance contexts</li> <li>High human expertise available for review and validation</li> <li>Sensitive or classified data handling requirements</li> <li>Regulatory compliance environments requiring human accountability</li> <li>Mission-critical systems where AI errors could have significant consequences</li> </ul> <p>Integration Considerations:</p> <ul> <li>Fits well into existing DevSecOps pipelines without major architectural changes</li> <li>Minimal training overhead for development teams</li> <li>Can be implemented incrementally across different SDLC phases</li> <li>Maintains clear human accountability for all outputs</li> </ul> <p>When Pattern 1 May Not Be Sufficient:</p> <ul> <li>High-volume, repetitive tasks where human review becomes a bottleneck</li> <li>Workflows requiring complex coordination across multiple tools or systems</li> <li>Teams seeking to automate entire workflows rather than augment individual tasks</li> <li>Environments where the overhead of constant human review outweighs the benefits</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-1-implementation","title":"Pattern 1 Implementation","text":"<p>For detailed readiness assessment, implementation checklists, monitoring guidance, and health metrics, see the companion AI Autonomy Implementation Guide.</p> <p>Pattern 1 Success Indicator: Teams demonstrate calibrated trust in AI assistive tools\u2014leveraging them effectively for appropriate tasks while maintaining rigorous human oversight and accountability. AI enhances productivity without compromising security, quality, or mission assurance.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-2-ai-as-a-team-member-human-triggered-finely-scoped-agent","title":"Pattern 2 \u2013 AI as a Team Member (Human-Triggered, Finely Scoped Agent)","text":"<p>Definition: An AI agent assigned to a specific role or bounded task, executing an entire workflow only after a human trigger. Unlike Pattern 1 tools, these agents operate across a slightly broader context (multiple steps instead of one), but remain narrow in scope. They are still subordinate to human decision-making and validation.</p> <p>SDLC Lens: Pattern 2 AI feels like a \"copilot teammate\" in the workflow. It doesn't just suggest a line of code \u2014 it can carry out a full slice of work inside the SDLC once instructed.  </p> <p>Examples include: </p> <ul> <li>In an IDE: generating unit tests for a new module.  </li> <li>In requirements management: drafting a traceability matrix or generating test cases from requirements.  </li> <li>In a CI/CD pipeline: executing a dependency update, opening a pull request, and running regression tests.  </li> <li>In system design: creating stubbed APIs from user stories.  </li> </ul> <p>Crucially, humans initiate the task and retain review/approval responsibility.  </p> <p>Architectural &amp; Risk Implications:</p> <ul> <li>Governance: Agents must be registered, sanctioned, and monitored within approved platforms (IDEs, CI/CD). Policies should explicitly prohibit unsanctioned SaaS agents. Governance must also enforce human-in-the-loop review and traceability of all agent outputs.</li> <li>Scoped Permissions: Agents should operate with least-privilege access (e.g., can open PRs but not merge).</li> <li>Audit Logging: Every action must be logged and attributed to the triggering human.</li> <li>Explainability &amp; Feedback Loops: Outputs must be observable and reviewable, with structured acceptance/rejection cycles to build calibrated trust.</li> </ul> <p>Shadow AI Risk at Pattern 2: As in Pattern 1, Shadow AI remains a concern \u2014 but the stakes are higher. At Pattern 1, unauthorized use mostly risked exposing sensitive data. At Pattern 2, unsanctioned agents can directly modify SDLC artifacts (code, test scripts, configuration files).</p> <p>This introduces new risks:</p> <ul> <li>Supply Chain Vulnerability \u2014 a rogue or unsanctioned agent could insert unvetted dependencies or insecure code that propagates downstream.</li> <li>Compliance Gaps \u2014 activity outside sanctioned channels may bypass audit trails, reviews, and security checks.</li> </ul> <p>Example: A developer connects an unapproved SaaS test-generation agent to a repo. The agent commits code that passes functional tests but introduces a hidden security flaw, bypassing governance because its activity wasn't logged or reviewed.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-2-selection-guidance","title":"Pattern 2 Selection Guidance","text":"<p>When Pattern 2 is the Right Choice:</p> <p>Pattern 2 works well for bounded, repeatable workflows that benefit from end-to-end automation but still require human oversight and accountability.</p> <p>Optimal Use Cases:</p> <ul> <li>Test suite generation and maintenance</li> <li>Documentation creation and updates</li> <li>Dependency management and updates</li> <li>API scaffolding and boilerplate code generation</li> <li>Code refactoring within defined parameters</li> <li>Requirements traceability matrix generation</li> </ul> <p>Risk-Benefit Analysis:</p> <ul> <li>Moderate risk tolerance with structured oversight</li> <li>Well-defined workflows with clear success criteria</li> <li>Repetitive tasks where human review overhead is justified by efficiency gains</li> <li>Non-critical systems where agent errors can be caught and corrected</li> <li>Teams with agent supervision experience or strong review processes</li> </ul> <p>Integration Considerations:</p> <ul> <li>Requires more sophisticated tooling than Pattern 1</li> <li>Benefits from standardized prompt libraries and agent templates</li> <li>Needs robust testing and validation pipelines</li> <li>Should integrate with existing code review and approval processes</li> </ul> <p>When Pattern 2 May Not Be Appropriate:</p> <ul> <li>High-stakes, mission-critical code where errors have severe consequences</li> <li>Workflows requiring creative problem-solving or strategic decision-making</li> <li>Environments with limited capacity for comprehensive agent output review</li> <li>Systems handling classified or highly sensitive data without proper controls</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-2-implementation","title":"Pattern 2 Implementation","text":"<p>For detailed readiness assessment, implementation checklists, monitoring guidance, and health metrics, see the companion AI Autonomy Implementation Guide.</p> <p>Pattern 2 Success Indicator: Teams effectively delegate bounded workflows to AI agents while maintaining appropriate human oversight, accountability, and trust calibration. Agents enhance productivity for repetitive tasks without compromising quality or introducing unacceptable risk.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-3-ai-as-a-fully-autonomous-team-orchestrated-multi-agent-systems","title":"Pattern 3 \u2013 AI as a Fully Autonomous Team (Orchestrated Multi-Agent Systems)","text":"<p>Definition: Pattern 3 introduces multiple autonomous agents working together across the SDLC. These agents coordinate to complete interconnected workflows end-to-end, under the supervision of human operators. While humans no longer trigger every task, they still provide oversight, policy boundaries, and final accountability.</p> <p>SDLC Lens (with Examples): Instead of a single teammate agent, Pattern 3 resembles an AI team within the team. Different agents specialize in roles (testing, compliance, deployment, monitoring) and are coordinated by an orchestration layer.</p> <p>Examples:</p> <ul> <li>A requirement-change event triggers coordinated updates across test plans, security scans, and documentation \u2014 each handled by different agents.</li> <li>A CI/CD pipeline has one agent managing builds, another running security scans, and another validating compliance before release.</li> <li>Agents cross-check each other's outputs to detect inconsistencies before delivery.</li> </ul> <p>Humans no longer initiate every action but instead monitor orchestration dashboards, approve exceptions, and intervene if the system escalates issues.</p> <p>Architectural &amp; Risk Implications:</p> <ul> <li>Orchestration Layer: Needed to sequence workflows, resolve conflicts, and provide observability into agent interactions.</li> <li>Inter-Agent Trust: Outputs from one agent must be verified before another consumes them.</li> <li>Error Containment: Safeguards must prevent cascading errors across the SDLC.</li> <li>Governance: Clear escalation rules define when humans are reintroduced into the loop.</li> <li>Audit Logging: Must capture not just agent actions, but cross-agent interactions for accountability.</li> </ul> <p>Shadow AI Risks at Pattern 3: At this level, Shadow AI introduces systemic risks. If unsanctioned agents are introduced into the orchestration layer, they may:</p> <ul> <li>Bypass trust models and inject unvalidated outputs.</li> <li>Break observability by operating outside governance and logging.</li> <li>Amplify propagation risk where rogue outputs cascade through workflows.</li> </ul> <p>Example: An unapproved SaaS deployment agent bypasses compliance checks and pushes builds into production, leaving no trace in the sanctioned audit system.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-3-selection-guidance","title":"Pattern 3 Selection Guidance","text":"<p>When Pattern 3 is the Right Choice:</p> <p>Pattern 3 is suitable for complex, interconnected workflows that benefit from specialized agent coordination but can operate under human supervision rather than direct control.</p> <p>Optimal Use Cases:</p> <ul> <li>End-to-end CI/CD pipeline automation with multiple validation stages</li> <li>Coordinated testing across multiple environments and test types</li> <li>Multi-stage compliance and security scanning workflows</li> <li>Integrated documentation, code generation, and testing pipelines</li> <li>Cross-system integration testing and validation</li> <li>Automated incident response with multiple containment and notification steps</li> </ul> <p>Risk-Benefit Analysis:</p> <ul> <li>Moderate to high risk tolerance with robust oversight mechanisms</li> <li>Complex workflows where agent coordination provides clear efficiency gains</li> <li>Mature DevSecOps environments with strong observability and governance</li> <li>Repeatable processes with well-defined success criteria and error handling</li> <li>Teams experienced with Pattern 1 and Pattern 2 implementations</li> </ul> <p>Integration Considerations:</p> <ul> <li>Requires sophisticated orchestration and monitoring infrastructure</li> <li>Benefits from standardized agent interfaces and communication protocols</li> <li>Needs comprehensive testing of agent interaction scenarios</li> <li>Should implement gradual rollout with fallback to human-controlled processes</li> </ul> <p>When Pattern 3 May Not Be Appropriate:</p> <ul> <li>Organizations without proven success with Pattern 1 and Pattern 2</li> <li>High-stakes environments where multi-agent failures could have severe consequences</li> <li>Workflows requiring frequent human judgment or creative problem-solving</li> <li>Systems with limited observability or debugging capabilities</li> <li>Teams lacking experience with complex distributed system management</li> </ul> <p>Pattern 3 Success Indicator: Organizations successfully coordinate multiple specialized AI agents to complete complex workflows while maintaining human oversight, system resilience, and full accountability. Multi-agent orchestration delivers efficiency gains without sacrificing governance or introducing unacceptable systemic risks.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-4-ai-as-a-self-optimizing-engine-software-flywheel","title":"Pattern 4 \u2013 AI as a Self-Optimizing Engine (Software Flywheel)","text":"<p>Definition: Pattern 4 represents the culmination of AI autonomy in the SDLC: a self-sustaining AI ecosystem that continuously designs, builds, tests, deploys, monitors, and improves software with minimal human intervention. At this level, humans set strategic direction and guardrails, but the system adapts and optimizes itself in near real time.</p> <p>SDLC Lens (with Examples): Pattern 4 envisions a closed-loop flywheel: telemetry, user feedback, and mission context feed directly into the design and deployment cycle. Unlike Patterns 1\u20133, humans do not trigger or oversee every action. Instead, the system continuously evolves based on real-world signals.</p> <p>Examples (aspirational):</p> <ul> <li>An AI system releases a security patch minutes after detecting a new exploit in production traffic.</li> <li>Performance monitoring drives automatic refactoring: inefficient code paths are rewritten and redeployed autonomously.</li> <li>User feedback is ingested at scale, reprioritizing backlog items and generating features without human tasking.</li> </ul> <p>At this pattern, humans transition from operators to stewards and governors, ensuring the flywheel remains aligned with mission objectives, compliance standards, and ethical principles.</p> <p>Architectural &amp; Risk Implications:</p> <ul> <li>Adaptive Governance: Policies and oversight must operate continuously, not periodically. Governance becomes a dynamic system that can adapt as agents learn and optimize.</li> <li>Compliance Enforcement: Automated checks for regulatory, security, and export control compliance must be embedded into every loop.</li> <li>Emergent Behavior Monitoring: Systems must actively detect and interpret behaviors not explicitly trained for \u2014 from anomalous optimizations to goal drift.</li> <li>Rollback &amp; Resilience: Circuit breakers, rollback pipelines, and human override switches must be first-class citizens, tested as rigorously as production code.</li> <li>Explainability at Scale: Decisions must remain interpretable to humans for mission assurance, auditing, and trust.</li> </ul> <p>Shadow AI Risks at Pattern 4: In a self-optimizing loop, unauthorized tools or agents can destabilize the entire system:</p> <ul> <li>Skewing optimization by injecting false telemetry or manipulated test results.</li> <li>Breaking compliance by introducing unapproved features or updates into the loop.</li> <li>Undermining trust by evolving the system in ways misaligned with mission priorities.</li> </ul> <p>Example: An unsanctioned monitoring agent feeds corrupted metrics into the flywheel, causing the system to prioritize the wrong performance goals while bypassing compliance checks.</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-4-selection-guidance","title":"Pattern 4 Selection Guidance","text":"<p>When Pattern 4 Might Be Appropriate:</p> <p>Pattern 4 is suitable only for organizations with proven mastery of Patterns 1-3 and specific use cases where continuous optimization provides mission-critical value.</p> <p>Potential Use Cases (Experimental):</p> <ul> <li>High-frequency, low-risk optimization tasks (performance tuning, resource allocation)</li> <li>Continuous security monitoring and response in well-defined threat environments</li> <li>Automated experimentation and A/B testing in non-critical systems</li> <li>Self-healing infrastructure in controlled environments</li> <li>Adaptive user experience optimization with safety guardrails</li> </ul> <p>Risk-Benefit Analysis:</p> <ul> <li>High risk tolerance with extensive safeguards and human oversight</li> <li>Mission-critical optimization needs that justify the complexity and risk</li> <li>Mature AI/ML capabilities across the entire organization</li> <li>Proven success with sophisticated Pattern 3 implementations</li> <li>Regulatory environments that can accommodate adaptive autonomous systems</li> </ul> <p>Critical Prerequisites:</p> <ul> <li>Demonstrated success with Pattern 3 multi-agent orchestration</li> <li>Robust governance frameworks proven effective across complex AI systems</li> <li>Advanced monitoring and explainability capabilities</li> <li>Comprehensive risk management and incident response procedures</li> </ul> <p>When Pattern 4 Should Be Avoided:</p> <ul> <li>Organizations without proven success with Patterns 1-3</li> <li>High-stakes environments where autonomous optimization errors could have severe consequences</li> <li>Regulatory environments requiring explicit human approval for all changes</li> <li>Systems with limited observability or explainability capabilities</li> <li>Teams lacking deep expertise in AI safety and autonomous system governance</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-4-implementation","title":"Pattern 4 Implementation","text":"<p>For detailed readiness assessment, implementation checklists, monitoring guidance, and health metrics, see the companion AI Autonomy Implementation Guide.</p> <p>Current Reality Check (Aspirational):</p> <p>Pattern 4 is not yet real at operational scale \u2014 it remains a vision for the future:</p> <ul> <li>Research Prototypes: Universities and labs are experimenting with autonomous repair loops, adaptive MLOps, and self-healing pipelines. These systems remain brittle, limited in scope, and require close human oversight.</li> <li>Commercial Building Blocks: Tools like GitHub Copilot, OpenAI's o1 reasoning models, LangGraph, and MLOps orchestration platforms provide fragments of the flywheel, but not a fully self-sustaining ecosystem.</li> <li>Governance Gaps: Current DoD and NIST frameworks assume explicit human accountability at critical decision points, a direct contradiction to Pattern 4 minimal-intervention autonomy.</li> <li>Trust and Safety: Multi-agent orchestration creates emergent risks that no governance model has yet fully contained.</li> </ul> <p>\ud83d\udccc Key Insight: Pattern 4 is best treated as an aspirational North Star \u2014 valuable for anticipating risks, guiding governance design, and preparing workforce evolution. It is not a current maturity requirement, nor should it be assumed to be inevitable.</p> <p>Pattern 4 Success Indicator: Organizations successfully operate self-optimizing AI ecosystems that continuously improve software quality, performance, and user value while maintaining mission alignment, regulatory compliance, and human stewardship. Autonomous optimization delivers transformational capabilities without compromising safety, governance, or accountability.</p>"},{"location":"plays/ai-autonomy_continuum_play/#4-cross-cutting-considerations","title":"4. Cross-Cutting Considerations","text":"<p>While the autonomy patterns describe how AI evolves in the SDLC, certain considerations cut across every stage. These ensure that autonomy does not compromise mission assurance, compliance, or security.</p>"},{"location":"plays/ai-autonomy_continuum_play/#cybersecurity-by-design","title":"Cybersecurity by Design","text":"<p>AI systems must be designed with security-first principles. This includes threat modeling against AI-specific attack vectors (prompt injection, data poisoning), enforcing Zero Trust architectures for agent communication, and ensuring that AI supply chains are secure. At all stages, AI must be subject to the same secure coding, hardening, and accreditation processes as other mission software.</p>"},{"location":"plays/ai-autonomy_continuum_play/#calibrated-trust","title":"Calibrated Trust","text":"<p>Trust in AI outputs should never be absolute. Teams must set confidence thresholds, establish escalation triggers, and ensure explainability of decisions. Early stages may require near-100% human review, while later stages can adopt risk-based review strategies. Trust must be measurable and iteratively calibrated, not assumed.</p>"},{"location":"plays/ai-autonomy_continuum_play/#data-stewardship","title":"Data Stewardship","text":"<p>AI systems rely on high-quality, well-governed data. Provenance tracking, classification, and secure handling are essential to prevent misuse or leakage. In DoD contexts, this also means aligning with data sovereignty, export control, and classification handling rules. Data integrity is foundational: corrupted or untrusted data undermines autonomy at every level.</p>"},{"location":"plays/ai-autonomy_continuum_play/#workforce-evolution","title":"Workforce Evolution","text":"<p>As autonomy increases, human roles shift from being direct executors to acting as supervisors, auditors, and curators. This demands new skills in governance, oversight, and AI literacy. Workforce design must anticipate changes in responsibility \u2014 for example, engineers becoming reviewers of AI outputs, or acquisition officers becoming stewards of AI-enabled contracts.</p>"},{"location":"plays/ai-autonomy_continuum_play/#governance-alignment","title":"Governance Alignment","text":"<p>Every stage of autonomy must remain aligned with evolving governance frameworks. This includes NIST AI RMF, DoD AI Ethical Principles, and acquisition policies that mandate human accountability. Governance is not static: it must adapt in parallel with technology, ensuring compliance and safety nets as systems become more autonomous.</p> <p>\ud83d\udccc Key Insight: These cross-cutting considerations are not optional add-ons \u2014 they are the safety rails that keep autonomy from becoming fragility. Whether operating with pattern 1 or experimenting with pattern 4 proto-flywheel systems , these dimensions must be addressed continuously.</p>"},{"location":"plays/ai-autonomy_continuum_play/#5-how-to-use-this-play","title":"5. How to Use This Play","text":"<p>This play is not a maturity roadmap. It is a pattern selection guide: a way for organizations to match the right AI autonomy approach to each workflow while maintaining mission assurance and operational control. Teams will likely implement multiple patterns simultaneously across different parts of their SDLC.</p>"},{"location":"plays/ai-autonomy_continuum_play/#1-map-your-current-ai-usage","title":"1. Map Your Current AI Usage","text":"<p>Before selecting new patterns, understand what's already in use:</p> <ul> <li>Inventory existing AI tools across all SDLC phases (development, testing, deployment, operations)</li> <li>Identify which patterns are already active (even informally)</li> <li>Assess governance coverage - are all AI uses visible and approved?</li> <li>Map Shadow AI risks - what unauthorized tools might teams be using?</li> </ul> <p>This creates a baseline reality of your current AI autonomy footprint.</p>"},{"location":"plays/ai-autonomy_continuum_play/#2-match-patterns-to-workflows","title":"2. Match Patterns to Workflows","text":"<p>Different SDLC workflows have different risk profiles, complexity needs, and human oversight requirements:</p> <p>Workflow-Based Selection:</p> <ul> <li>High-stakes code (auth, crypto, safety-critical): Pattern 1 (human-controlled assistance)</li> <li>Repetitive tasks (testing, documentation): Pattern 2 (delegated agents) </li> <li>Complex orchestration (CI/CD pipelines): Pattern 3 (multi-agent coordination)</li> <li>Continuous optimization (performance tuning): Pattern 4 (experimental, if at all)</li> </ul> <p>Risk-Based Selection:</p> <ul> <li>Sensitive data handling: Patterns 1-2 maximum</li> <li>Mission-critical systems: Pattern 1 preferred, Pattern 2 with extensive review</li> <li>Development/testing environments: Higher pattern experimentation acceptable</li> <li>Production systems: Conservative pattern selection with proven governance</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#3-plan-multi-pattern-architecture","title":"3. Plan Multi-Pattern Architecture","text":"<p>Most organizations will operate multiple patterns simultaneously:</p> <p>Architectural Considerations:</p> <ul> <li>Different trust boundaries for each pattern</li> <li>Varied governance controls based on pattern risk levels  </li> <li>Integrated audit trails across all patterns</li> <li>Consistent human oversight models that scale across patterns</li> </ul> <p>Implementation Strategy:</p> <ul> <li>Start with Pattern 1 broadly across low-risk workflows</li> <li>Introduce Pattern 2 selectively for bounded, repeatable tasks</li> <li>Consider Pattern 3 only after demonstrating success with Patterns 1-2</li> <li>Treat Pattern 4 as experimental research, not operational deployment</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#4-build-pattern-specific-readiness","title":"4. Build Pattern-Specific Readiness","text":"<p>Each pattern requires different organizational capabilities:</p> <ul> <li>Pattern 1: Human review processes, Shadow AI detection, prompt governance</li> <li>Pattern 2: Agent supervision skills, scoped permission management, escalation procedures  </li> <li>Pattern 3: Multi-agent orchestration, system resilience, complex monitoring</li> <li>Pattern 4: Autonomous system stewardship, emergent behavior detection, advanced AI safety</li> </ul> <p>Focus readiness efforts on the patterns you plan to implement, not on abstract \"AI maturity.\"</p>"},{"location":"plays/ai-autonomy_continuum_play/#5-monitor-pattern-health","title":"5. Monitor Pattern Health","text":"<p>Success is measured by how well each pattern serves its intended workflows:</p> <ul> <li>Pattern effectiveness: Is the pattern solving the problem it was designed for?</li> <li>Governance coverage: Are all pattern implementations visible and controlled?</li> <li>Risk management: Are pattern-specific risks being adequately managed?</li> <li>Team confidence: Do operators trust and effectively supervise each pattern?</li> </ul> <p>\ud83d\udccc Key insight: The goal is not to \"advance\" through patterns, but to optimize pattern selection for mission outcomes while maintaining appropriate control and assurance.</p>"},{"location":"plays/ai-autonomy_continuum_play/#visual-checklist-for-using-this-play","title":"\u2705 Visual Checklist for Using This Play","text":"<p>Before implementing any AI autonomy pattern:</p> <ol> <li>Map Current State</li> <li>[ ] Have we inventoried all existing AI tool usage across the SDLC?</li> <li>[ ] Do we understand which patterns are already in use (formally or informally)?</li> <li> <p>[ ] Have we identified and addressed Shadow AI risks?</p> </li> <li> <p>Match Patterns to Workflows</p> </li> <li>[ ] Have we assessed the risk profile and requirements of each target workflow?</li> <li>[ ] Is the selected pattern appropriate for the data sensitivity and mission criticality?</li> <li> <p>[ ] Do we understand why this pattern is better than alternatives for this use case?</p> </li> <li> <p>Build Readiness</p> </li> <li>[ ] Do we have the infrastructure, governance, and skills needed for this specific pattern?</li> <li>[ ] Are human oversight and escalation procedures designed and tested?</li> <li> <p>[ ] Have we established pattern-specific success metrics and monitoring?</p> </li> <li> <p>Deploy with Safeguards</p> </li> <li>[ ] Are we starting with controlled, low-risk implementations?</li> <li>[ ] Do we have rollback plans and circuit breakers for pattern failures?</li> <li>[ ] Are we measuring pattern effectiveness and adjusting as needed?</li> </ol>"},{"location":"plays/ai-autonomy_continuum_play/#6-measures-and-success-indicators","title":"6. Measures and Success Indicators","text":"<p>Implementing AI autonomy patterns requires evidence of effectiveness, not just adoption. This section provides the strategic measurement framework that leaders need to evaluate whether each pattern is serving its intended purpose, operating safely, and delivering value to mission outcomes.</p> <p>\ud83d\udccb Strategic vs. Tactical Measurement: This section focuses on what to measure and why - the executive-level indicators needed for pattern selection, organizational readiness assessment, and cross-pattern governance. For how to measure and when - including specific thresholds, operational dashboards, monitoring procedures, and continuous improvement actions - see the AI Autonomy Implementation Guide.</p> <p>Key Distinction: * This section (Strategic): Quarterly reviews, pattern effectiveness, organizational health indicators * Implementation Guide (Tactical): Daily monitoring, operational thresholds, alerting procedures, process adjustments</p>"},{"location":"plays/ai-autonomy_continuum_play/#pattern-success-philosophy","title":"Pattern Success Philosophy","text":"<p>Each pattern has distinct success criteria reflecting its intended use and risk profile:</p> <p>Pattern 1 (Assistive Tools) Success: Teams demonstrate calibrated trust in AI assistive tools\u2014leveraging them effectively for appropriate tasks while maintaining rigorous human oversight and accountability. AI enhances productivity without compromising security, quality, or mission assurance.</p> <p>Pattern 2 (Delegated Agents) Success: Teams effectively delegate bounded workflows to AI agents while maintaining appropriate human oversight, accountability, and trust calibration. Agents enhance productivity for repetitive tasks without compromising quality or introducing unacceptable risk.</p> <p>Pattern 3 (Orchestrated Systems) Success: Organizations successfully coordinate multiple specialized AI agents to complete complex workflows while maintaining human oversight, system resilience, and full accountability. Multi-agent orchestration delivers efficiency gains without sacrificing governance or introducing unacceptable systemic risks.</p> <p>Pattern 4 (Adaptive Ecosystems) Success: Organizations successfully operate self-optimizing AI ecosystems that continuously improve software quality, performance, and user value while maintaining mission alignment, regulatory compliance, and human stewardship. Autonomous optimization delivers transformational capabilities without compromising safety, governance, or accountability.</p>"},{"location":"plays/ai-autonomy_continuum_play/#cross-pattern-health-indicators","title":"Cross-Pattern Health Indicators","text":"<p>These organizational-level metrics apply across all patterns and provide baseline health indicators for multi-pattern environments:</p> <p>Governance Effectiveness: * Pattern Selection Appropriateness: % of workflows using the most suitable pattern based on risk, complexity, and oversight requirements * Cross-Pattern Governance Consistency: Uniform application of security, audit, and compliance controls across different autonomy patterns * AI Implementation Coverage: % of AI implementations operating within approved boundaries and oversight mechanisms</p> <p>Organizational Readiness: * Multi-Pattern Coordination: Successful isolation and coordination between different pattern implementations to prevent conflicts or security issues * Resource Allocation Efficiency: Appropriate distribution of human oversight and technical resources based on pattern risk profiles * Workforce Adaptation: Effectiveness of role transitions from direct executors to supervisors, auditors, and curators</p> <p>Trust and Safety: * Trust Calibration Maturity: Alignment between team confidence in AI outputs and actual reliability across all implemented patterns * Shadow AI Prevention: Declining trend in unauthorized AI tool use as governance matures and approved alternatives meet user needs * Incident Response Effectiveness: Speed and quality of response to AI-related security, quality, or compliance events</p>"},{"location":"plays/ai-autonomy_continuum_play/#leading-vs-lagging-indicators","title":"Leading vs. Lagging Indicators","text":"<p>Leading Indicators (predict future performance): * Training completion rates for pattern-specific skills * Governance policy coverage for new AI implementations * Readiness assessment scores before pattern deployment * Pilot program success rates in controlled environments</p> <p>Lagging Indicators (measure actual outcomes): * Production incident rates attributed to AI implementations * Compliance audit findings related to AI governance * Mission outcome improvements attributable to AI patterns * Long-term trust calibration trends across teams</p>"},{"location":"plays/ai-autonomy_continuum_play/#warning-signs-across-all-patterns","title":"\ud83d\udd34Warning Signs Across All Patterns","text":"<p>Monitor these risk indicators regardless of which patterns are in use:</p> <ul> <li>Trust Miscalibration: Teams over- or under-trusting AI capabilities relative to actual performance</li> <li>Governance Gaps: AI implementations operating outside approved oversight mechanisms</li> <li>Review Fatigue: Declining quality of human oversight due to volume or repetition</li> <li>Pattern Drift: Gradual expansion of AI autonomy beyond intended boundaries</li> <li>Shadow AI Growth: Increasing use of unauthorized tools despite approved alternatives</li> <li>Cross-Pattern Conflicts: Different autonomy patterns interfering with each other or creating security vulnerabilities</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#measurement-strategy-guidelines","title":"Measurement Strategy Guidelines","text":"<p>Start with Baseline Measurement:</p> <ul> <li>Capture pre-implementation metrics for workflows targeted for AI augmentation</li> <li>Establish current trust calibration and governance coverage levels</li> <li>Document existing human effort and error rates for comparison</li> </ul> <p>Focus on Pattern Effectiveness:</p> <ul> <li>Measure whether each pattern is solving the problems it was designed to address</li> <li>Evaluate pattern selection decisions based on actual outcomes vs. intended benefits</li> <li>Adjust pattern implementation based on evidence of effectiveness</li> </ul> <p>Scale Monitoring to Risk:</p> <ul> <li>Apply more intensive monitoring to higher autonomy patterns</li> <li>Tailor governance controls to pattern-specific risk profiles</li> <li>Adjust measurement frequency based on pattern maturity and observed stability</li> </ul> <p>Evolve Metrics with Maturity:</p> <ul> <li>Regularly reassess whether patterns are delivering intended value</li> <li>Update success criteria as patterns mature and organizational needs change</li> <li>Refine measurement approaches based on lessons learned across patterns</li> </ul>"},{"location":"plays/ai-autonomy_continuum_play/#ai-autonomy-health-index","title":"AI Autonomy Health Index","text":"<p>This index provides a snapshot of organizational readiness for operating AI-enabled workflows with any combination of autonomy patterns. Unlike a static maturity model, it measures operational health in key domains that can go up or down over time.</p> Category \u2705 Green (Healthy) \u26a0\ufe0f Yellow (At Risk) \u274c Red (Unhealthy) Governance &amp; Compliance All AI tools/agents are sanctioned, registered, and monitored. NIST RMF/DoD AI guidance explicitly applied. Most AI tools governed, but occasional Shadow AI reported. Logging/monitoring incomplete. Significant Shadow AI use; no consistent policy for agent approval or monitoring. Trust Calibration AI outputs consistently reviewed &amp; validated; structured feedback loops; trust scores improving. Outputs inconsistently reviewed; feedback loops ad hoc; uneven team trust. AI outputs bypass review; blind acceptance or outright rejection without calibration. Workforce Readiness Roles evolving into supervisors, auditors, curators. Training on AI oversight &amp; ethics complete. Workforce understands tools but lacks formal oversight/ethics training. Role shifts unclear. Workforce either over-relies on AI without oversight or resists adoption entirely. Risk Management Playbooks for rollback, error containment, human overrides tested regularly. AI incident response defined. Some safeguards exist but untested or inconsistent. AI risks not fully in playbooks. No structured risk management. AI errors cause disruption without mitigation. <p>How to use this index: Leaders can score themselves (\u2705 / \u26a0\ufe0f / \u274c) across categories quarterly to assess pattern implementation health. This avoids the trap of chasing \"advanced\" patterns and instead ensures effectiveness at whatever patterns are currently in use.</p> <p>\ud83d\udccc Key Insight: Success is not measured by implementing \"higher\" patterns, but by optimizing pattern selection and implementation for mission outcomes. The best measurement strategy focuses on whether each pattern is solving the problems it was designed to address while maintaining appropriate risk controls.</p> <p>For detailed metrics, monitoring procedures, and operational thresholds for each pattern, see the AI Autonomy Implementation Guide.</p>"},{"location":"plays/ai-autonomy_continuum_play/#7-recommendations-next-best-play","title":"7. Recommendations &amp; Next Best Play","text":"<ul> <li>Treat AI autonomy patterns not as maturity levels, but as operational choices \u2014 reflecting how well different approaches serve specific workflows while maintaining mission assurance and appropriate control.</li> <li>A \"higher\" pattern is not \"better\" by default. The right pattern depends on workflow characteristics, risk tolerance, and operational context \u2014 not organizational sophistication.</li> <li>Use Pattern 1 and Pattern 2 as proving grounds for governance and trust calibration: they provide the safest environments to build AI oversight capabilities, measure effectiveness, and refine human-AI collaboration models before considering more complex implementations.</li> <li>Continuously reassess \"AI health\" with a pattern effectiveness index \u2014 evaluating whether each pattern implementation is solving its intended problems while maintaining security, governance, and mission alignment.</li> <li>Recognize that Pattern 4 is aspirational. Treat it as a design horizon for governance planning and workforce development, but focus operational efforts on optimizing Patterns 1-3 for current mission needs.</li> <li>Plan for multi-pattern architectures from the start \u2014 most mature organizations will operate multiple patterns simultaneously, requiring integrated governance, consistent oversight models, and coordinated risk management.</li> </ul> <p>\ud83d\udccc Key Insight: By shifting from \"maturity ladder\" to health index, autonomy becomes about operating safely, responsibly, and mission-aligned with the patterns you've chosen \u2014 not about reaching the \"highest\" level of automation.</p>"},{"location":"plays/ai-autonomy_continuum_play/#8-closing-thoughts","title":"8. Closing Thoughts","text":"<p>The AI Autonomy Continuum is not a prescriptive path or an inevitable end state \u2014 it is a navigation aid. Each pattern provides a lens for understanding how autonomy shapes the SDLC, the risks that emerge, and the safeguards required.</p> <p>Leaders should treat autonomy as a dynamic health index, not a maturity ladder. The real measure of success is whether teams can operate safely, securely, and effectively with their chosen patterns, while making thoughtful decisions about when and how to evolve their approach.</p> <p>Different patterns serve different purposes \u2014 Pattern 1 may be the optimal long-term choice for sensitive workflows, while Pattern 3 might be ideal for complex orchestration needs. There is no \"graduation\" requirement, only the imperative to match the right pattern to the right problem while maintaining mission assurance.</p> <p>This play is intentionally open-ended. Future plays may expand on topics such as governance models for multi-agent systems, workforce transformation, or testing methods for emergent behavior. Until then, the charge is clear: advance only as fast as your guardrails allow \u2014 and never confuse speed for readiness.</p> <p>End of Play</p> <ol> <li> <p>National Institute of Standards and Technology, Autonomy Levels for Unmanned Systems (ALFUS) Framework, Volume I: Terminology, NIST Special Publication 1011, Version 1.1, Sep. 2004. [Online]. Available: https://www.nist.gov/system/files/documents/el/isd/ks/NISTSP_1011_ver_1-1.pdf. [Accessed: Aug. 18, 2025].\u00a0\u21a9</p> </li> <li> <p>Boston Consulting Group, \"AI Agents: What They Are and Their Business Impact,\" BCG, Apr. 2025. [Online]. Available: https://www.bcg.com/capabilities/artificial-intelligence/ai-agents. [Accessed: Aug. 18, 2025].\u00a0\u21a9</p> </li> <li> <p>Masterman, T., Besen, S., Sawtell, M., &amp; Chao, A. (2024). The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey (No. arXiv:2404.11584). arXiv. https://doi.org/10.48550/arXiv.2404.11584\u00a0\u21a9</p> </li> <li> <p>E. H. Shortliffe, Computer-Based Medical Consultations: MYCIN. New York, NY, USA: Elsevier, 1976.\u00a0\u21a9</p> </li> <li> <p>R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: MIT Press, 1998.\u00a0\u21a9</p> </li> <li> <p>A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImageNet classification with deep convolutional neural networks,\u201d in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 25, 2012, pp. 1097\u20131105.\u00a0\u21a9</p> </li> <li> <p>T. B. Brown et al., \u201cLanguage models are few-shot learners,\u201d in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), vol. 33, 2020, pp. 1877\u20131901.\u00a0\u21a9</p> </li> <li> <p>Department of Defense Chief Information Officer (DoD CIO), DoD Enterprise DevSecOps Reference Design, Version 1.0, Aug. 12, 2019. [Online]. Available: https://dodcio.defense.gov/Portals/0/Documents/DoD%20Enterprise%20DevSecOps%20Reference%20Design%20v1.0_Public%20Release.pdf. [Accessed: Aug. 18, 2025]\u00a0\u21a9</p> </li> </ol>"},{"location":"plays/code-gen-play/","title":"Play: Leading Practices for Code Completion and Generation","text":""},{"location":"plays/code-gen-play/#executive-summary","title":"Executive Summary","text":"<p>The use of Generative AI (GenAI) for code completion and generation is accelerating across public and private sectors. These tools promise faster delivery, improved maintainability, and even enhanced security\u2014but they also introduce new risks. In fact, some leading technology firms that develop GenAI capabilities restrict their own engineers from using these tools for production code, underscoring the need for measured, policy-aligned adoption.</p> <p>This play provides DoD-aligned guidance on current leading practices, risk mitigation strategies, and governance approaches\u2014with an eye toward emerging agentic platforms and the evolving role of human-machine teaming in software creation.</p> <p>GenAI-powered code completion and generation tools are transforming software engineering in the DoD. However, ensuring reliability, traceability, compliance, and mission assurance is essential when integrating any AI-generated code into DoD software pipelines. This play outlines secure usage patterns, appropriate boundaries, and validation mechanisms to support intentional, auditable integration.</p>"},{"location":"plays/code-gen-play/#1-introduction","title":"1. Introduction","text":""},{"location":"plays/code-gen-play/#why-this-play-matters","title":"Why this play matters.","text":"<p>Generative AI (GenAI) tools for code completion and code generation are transforming how DoD software teams design, write, and test code. These tools offer measurable gains in productivity, maintainability, and even security\u2014if used correctly.  Recent studies show gains ranging from 4% in sustained engineering productivity<sup>1</sup> to 55% in task completion speed for specific use cases.<sup>2</sup> But without structured oversight, they also introduce risks: incorrect or hallucinated code, trust boundary violations, and amplification of bad practices at scale.  Additional risks include security vulnerabilities, data privacy exposures, ethical and legal liabilities, and reduced human accountability for generated outputs\u2014especially when operating at speed and scale within classified or operational environments.</p> <p>\ud83d\udccc Clarifying Scope: Human Augmentation, Not Full Automation</p> <p>This play focuses on augmenting human developers\u2014not replacing them. While fully autonomous agents may emerge in future workflows, this guidance centers on GenAI tools that assist and accelerate human-led coding practices. Future plays will address autonomous agents explicitly.</p> <p>This play begins to clarify how to securely integrate GenAI capabilities\u2014from autocomplete-style suggestions to full-function code generation\u2014into the SDLC. These tools are not intended to replace human developers, but to augment them\u2014accelerating development and improving maintainability while reinforcing accountability and security. These tools are not intended to replace human developers, but to augment them\u2014accelerating development and improving maintainability while reinforcing accountability and security. .</p>"},{"location":"plays/code-gen-play/#purpose-of-the-play","title":"Purpose of the Play","text":"<p>Clarify  safe and intentional practices for integrating GenAI-powered code completion and generation into the SDLC:</p> <ul> <li>Promote responsible use of GenAI-augmented coding tools in DoD environments.</li> <li>Establish boundaries between assistive automation and autonomous decision-making.</li> <li>Build Establish traceability as a foundation for building trust in AI-augmented software practices.</li> </ul>"},{"location":"plays/code-gen-play/#scope-and-applicability","title":"Scope and Applicability","text":"<p>This play focuses on GenAI-driven capabilities used to assist with authoring and modifying code artifacts, including both inline completions and prompt-driven generation. The term \u201ccode\u201d includes application logic, infrastructure scripts, tests, and pipeline definitions\u2014spanning the work of developers, testers, SREs, and platform engineers.</p> <p>To guide safe and auditable use, this play references two common usage patterns: CHop (Chat-Oriented Programming), a structured approach emphasizing prompt governance, and VIBE programming, a less formal and higher-risk pattern often used during prototyping. See [Section 4 \u2013 Key Definitions] for full descriptions and associated risk guidance.</p> <p>In-Scope:</p> <ul> <li>IDE-based code completion tools (e.g., GitHub Copilot, Continue.dev)</li> <li>Prompt-driven code generation workflows using LLM-based tools such as TabNine and Codestral</li> <li>Test generation and static analysis using GenAI in CI/CD pipelines</li> <li>AI-augmented pair programming and assistant models</li> </ul> <p>Out-of-Scope (for now): Fully autonomous agents will be addressed in future plays</p> <p>\ud83d\udccc Scope and Role Diversity:</p> <p>While this play focuses on code generation, the term \u201ccode\u201d encompasses a wide variety of artifact types beyond traditional software functions. In modern delivery pipelines, infrastructure code, test automation, pipeline definitions, and security scripts are all authored\u2014and increasingly generated\u2014by a variety of roles. This play applies wherever GenAI tools are used to generate structured logic or configuration artifacts, regardless of whether the author identifies as a developer, tester, SRE, or platform engineer. Role-specific guidance appears in future plays.</p>"},{"location":"plays/code-gen-play/#where-we-are-today","title":"Where We Are Today","text":"<ul> <li>Experimentation with prompt-based GenAI tools is rapidly advancing across industry and academia, with selected adoption pilots underway in U.S. government and defense organizations.</li> <li>Current DoD guidance focuses on augmentation\u2014keeping a human in the loop\u2014while establishing safeguards for responsible use.</li> <li>Fully autonomous code agents are an emerging area but are not yet ready for widespread adoption or mission-critical use.</li> </ul> <p>Despite trailing other sectors by 10% in current GenAI adoption (44% in government vs. 54% across industries), public sector interest is rapidly growing. 84% of government organizations plan to invest in GenAI within the next year, even as strategic readiness, policy clarity, and workforce skills lag.<sup>3</sup>   For the Department of Defense, this gap reinforces the importance of intentional adoption. This play emphasizes safe, augmented use of GenAI for code completion and generation\u2014prioritizing mission assurance, trust, and traceability over speed or full automation.</p>"},{"location":"plays/code-gen-play/#what-is-generative-ai-in-software-engineering","title":"What is Generative AI in Software Engineering?","text":"<p>Generative AI models\u2014such as OpenAI Codex, GitHub Copilot, and Meta\u2019s Code Llama\u2014assist software teams by:</p> <ul> <li>Suggesting code snippets, functions, or entire scripts.  This is for both core software code as well as for the necessary automated unit testing requisite for strong CI/CD pipeline practices.</li> <li>Automating boilerplate coding tasks.</li> <li>Enhancing code documentation and suggestions for focused refactoring.</li> </ul>"},{"location":"plays/code-gen-play/#2-prerequisites-and-foundations","title":"2. Prerequisites and Foundations","text":"<p>Before introducing GenAI-based code completion and generation tools into a DoD software environment, teams must establish foundational practices that enable secure, auditable, and mission-aligned integration. This play assumes the presence of key DevSecOps capabilities and an understanding of evolving human-machine interaction patterns, as outlined in Play: Fundamentals.</p>"},{"location":"plays/code-gen-play/#devsecops-baseline","title":"DevSecOps Baseline","text":"<p>This play builds on the assumption that your software team or organization has achieved baseline DevSecOps capabilities. GenAI tooling must not be treated as standalone or informal\u2014it must be integrated into trusted delivery pipelines with visibility and traceability to enable auditability, reduce risk, and uphold mission assurance.</p> <p>Why? Because ungoverned GenAI use can bypass traditional security gates, propagate unvetted code, and obscure accountability. Without visibility and traceability, teams cannot verify provenance, assess performance, or respond to emerging threats\u2014placing both the mission and the organization at risk.</p> <p>Key prerequisites include:</p> <ul> <li> <p>CI/CD Automation - Automated pipelines that support linting, testing, artifact signing, and environment promotion\u2014allowing GenAI outputs to flow securely into trusted paths.  Online materials and guidance such as MinimumCD.org provide guidance on the base essentials or the minimums necessary to implement continuous delivery.</p> </li> <li> <p>Secure Software Supply Chain - Adoption of frameworks like the Secure Software Development Framework (SSDF), as defined in NIST Special Publication 800-218, or the Supply-chain Levels for Software Artifacts (SLSA), maintained by the Open Source Security Foundation at slsa.dev, enables provenance, dependency control, and vulnerability scanning\u2014even for AI-generated code.</p> </li> <li> <p>Infrastructure-as-Code (IaC) - GenAI may suggest or write IaC templates (e.g., Terraform, CloudFormation). While these may be created by an infrastructure engineer less familiar with software engineering principles, these generated scripts must be treated these as critical assets, subject to the same secure design and validation workflows.</p> </li> <li> <p>SBOM / MLBOM / AIBOM Practices - Generated code must be tracked and attributed in the software bill of materials. Prompt provenance and model source should be considered part of an emerging Machine Learning Bill of Materials (MLBOM) and Artificial Intelligence Bill of materials (AIBOM).</p> </li> </ul> <p>Note: This mirrors traceability expectations in model-based engineering workflows, where executable artifacts generated from tools like CAMEO (SysML) or MATLAB/Simulink are documented. GenAI artifacts must meet the same bar for lineage, compliance, and mission assurance.</p>"},{"location":"plays/code-gen-play/#takeaway","title":"\ud83d\udccc Takeaway","text":"<p>GenAI tools for code generation and completion must be treated as components of the DevSecOps ecosystem\u2014not as shortcuts or sidecar helpers. Integrating these tools requires automated validation, traceability, and aligned governance. Their use must reinforce\u2014not bypass\u2014the secure, auditable, and mission-assured delivery principles already established in modern software pipelines.</p>"},{"location":"plays/code-gen-play/#human-machine-interaction-patterns","title":"Human-Machine Interaction Patterns","text":"<p>How teams interact with GenAI tools shapes traceability, validation, and alignment with mission assurance goals. Interaction patterns determine whether generated outputs can be audited, governed, and integrated into secure DevSecOps workflows.</p> <p>This play references patterns introduced in Fundamentals for Designing an AI-Augmented Tool Chain:</p> <ul> <li>Standalone Interfaces (e.g., ChatGPT browser use): Not recommended\u2014no traceability or policy enforcement.</li> <li>IDE Plugins (e.g., Copilot, Continue.dev): Inline suggestions with limited visibility. Use only with strong local controls and peer review.</li> <li>API-Based Integrations: Enable prompt logging, CI/CD control, and SBOM/MLBOM support. Ideal for structured GenAI use. These are well-suited for structured GenAI use, especially in environments requiring auditability, observability, and policy enforcement.</li> <li>Agentic Platforms: High capability and autonomy, but require guardrails, execution limits, and policy-as-code enforcement.</li> </ul> <p>Select interaction patterns that align with your trust boundaries, observability needs, and pipeline maturity. See Fundamentals for Designing an AI-Augmented Tool Chain  for detailed architectural recommendations.</p>"},{"location":"plays/code-gen-play/#3-the-basics","title":"3. The Basics","text":"<p>Effective use of GenAI for code completion and generation starts with practical discipline. This section introduces two foundational techniques that shape GenAI output: providing relevant technical context and crafting precise, structured prompts. These skills help teams produce consistent, trustworthy results while reinforcing traceability and intent.</p>"},{"location":"plays/code-gen-play/#context-awareness","title":"Context Awareness","text":"<p>GenAI models operate within constrained \"context windows\"\u2014meaning they can only \u201csee\u201d a limited amount of input (code, comments, architecture notes) at a time. Without the right context, outputs may be incomplete, incorrect, or misaligned with system goals.</p> <p>To improve relevance and reduce hallucination in GenAI-generated code:</p> <ul> <li>Provide scoped architectural context Specify the layer, framework, and purpose of the code</li> </ul> <p>Example: Instead of saying \u201cgenerate a login component,\u201d say \u201cgenerate an Angular frontend login component that calls a Flask-based API endpoint for JWT token authentication.\u201d Tip: Paste relevant architectural stubs (e.g., existing interface definitions or service contracts) to guide completions.</p> <ul> <li>Inject domain-specific terms Reinforce military or mission-relevant semantics the model might not know by default</li> </ul> <p>Example: \u201cGenerate a parser for a tactical data link message using the Link-16 protocol\u201d will produce better results than \u201cgenerate a network message parser.\u201d Tip: Maintain a glossary or prompt library of key mission-specific terms and phrases for prompt reuse.</p> <ul> <li>Include security constraints Explicitly call out rules for data handling, encryption, and access control</li> </ul> <p>Example: \u201cWrite a Python function that logs access attempts and enforces RBAC using DoD CAC authentication.\u201d Tip: Pair prompts with brief in-line comments or notes on required compliance standards (e.g., FIPS 140-3, NIST 800-53).</p> <ul> <li>Avoid prompts that assume global awareness or full-system understanding Keep prompts focused and bounded to specific tasks with provided context</li> </ul> <p>Pitfall: \u201cGenerate the backend for our logistics app\u201d is too vague. Better: \u201cCreate a Flask route for submitting transport requests using the existing <code>RequestForm</code> class and storing data in PostgreSQL.\u201d</p>"},{"location":"plays/code-gen-play/#takeaway_1","title":"\ud83d\udccc Takeaway","text":"<p>Treat GenAI more like an unseasoned contributor than a trusted teammate\u2014it can be remarkably helpful, but only when guided with precision. Think of prompting as mentoring: you need to set clear expectations, define boundaries, and provide architectural and domain context. Just as junior staff need coaching, these tools benefit from examples, constraints, and continuous review.</p> <p>Effective use means articulating the \"why\" and \"how\" behind tasks, not just the \"what.\" </p>"},{"location":"plays/code-gen-play/#prompting","title":"Prompting","text":""},{"location":"plays/code-gen-play/#what-is-a-prompt","title":"What is a Prompt?","text":"<p>A prompt is the input provided to a Generative AI model that directs it to produce a specific output.</p> <p>In software engineering contexts, a prompt typically includes:</p> <ul> <li>Instructions or tasks (e.g., \u201cWrite a Python function to hash passwords using SHA-256.\u201d)</li> <li>Code context or examples (e.g., function headers or config templates)</li> <li>Constraints (e.g., required language, security policies)</li> <li>Expected outcome (e.g., unit-testable code, secure configuration)</li> </ul>"},{"location":"plays/code-gen-play/#takeaway_2","title":"\ud83d\udccc Takeaway:","text":"<p>Think of prompting as giving direction to a capable but inexperienced teammate. The more clearly you describe the task\u2014including relevant context, boundaries, and constraints\u2014the more useful and trustworthy the output will be. Precision isn't optional\u2014it's the difference between noise and value.</p> <p>You wouldn\u2019t ask a new hire to \u201cjust write some code.\u201d Don\u2019t do that with GenAI either.</p>"},{"location":"plays/code-gen-play/#leading-practices-for-writing-ai-prompts","title":"Leading Practices for Writing AI Prompts","text":"<p>Writing high-quality prompts is critical to ensuring that AI-generated code is useful, secure, and aligned with system intent. Prompts should be treated as governed artifacts, subject to peer review, reuse, and revision over time.</p> <p>Even though GenAI outputs can vary between runs, storing and governing prompts still enables: -   Traceability of design decisions -   Reviewability for alignment with policy or coding standards -   Feedback loops to improve future prompt effectiveness</p> <p>Emerging tools such as Helicone, PromptLayer, and LangSmith enable observability, version control, and feedback tracking for prompt engineering. These capabilities are especially valuable in mission environments where reproducibility, traceability, and auditability are required.</p> <p>Here are key leading practices for operational use:</p> Principle Practice Example Be Specific Instead of \u201cgenerate a login function,\u201d say \u201cwrite a Python login function using JWT and RBAC.\u201d Set Constraints Define what libraries, standards, or DoD policies must be followed. State Expected Output Describe the format (e.g., JSON, Bash script, Python function with docstring). Avoid Ambiguity Avoid vague verbs (\u201coptimize,\u201d \u201cimprove\u201d) without clear criteria. Reference Context Include domain, service boundaries, file structure, or architectural decisions if relevant. Use Declarative Language Ask for outcomes (\u201cvalidate all user inputs\u201d) rather than low-level steps. <p>\u2705 Tip: Prompts that are too open-ended lead to hallucination or overly verbose outputs. Precision and clarity are key to reliability and reuse.</p>"},{"location":"plays/code-gen-play/#takeaway_3","title":"\ud83d\udccc Takeaway:","text":"<p>Think of prompts less like natural conversation and more like test cases for generative systems: they should be scoped, structured, and explainable. </p>"},{"location":"plays/code-gen-play/#prompt-patterns-table","title":"Prompt Patterns Table","text":"<p>Below is a set of example prompts for common use cases relevant to DoD software engineering:</p> Use Case Effective Prompt Algorithm Generation \u201cWrite a Python function that implements merge sort for a list of integers and returns a new sorted list.\u201d Secure Coding \u201cGenerate a Java login method using OAuth 2.0 and input validation to prevent SQL injection.\u201d Test Generation \u201cCreate a set of JUnit tests for a function that calculates flight duration given departure and arrival times.\u201d CI/CD Integration \u201cWrite a GitHub Actions workflow that runs unit tests and performs a SAST scan on each pull request.\u201d IaC Template \u201cCreate a Terraform script that provisions a public/private subnet pair in an AWS VPC.\u201d Logging &amp; Monitoring \u201cAdd structured JSON logging to a Python API handler for DoD telemetry collection.\u201d <p>\ud83d\udeab Avoid: \u201cMake a script for user auth.\u201d</p> <p>\u2705 Prefer: \u201cGenerate a Python script for user authentication using JWT, with comments and input validation.\u201d</p>"},{"location":"plays/code-gen-play/#upcoming-dod-prompting-guide","title":"Upcoming DoD Prompting Guide","text":"<p>Check back soon for the upcoming prompting guide tailored to software value streams for DoD software. This guide will include a description of what prompting is, how to construct a prompt, and examples based on the many roles\u2014from product owners to software engineers, testers, and operations personnel.</p>"},{"location":"plays/code-gen-play/#industry-prompting-guides","title":"Industry Prompting Guides","text":"<p>The following resources offer practical and evolving guidance for writing effective AI prompts. While tailored for different use cases, many principles (e.g., specificity, context-setting, declarative language) are applicable across DoD environments.</p> Source Guide Title Notes Microsoft Prompt engineering 101 Practical patterns, risk considerations, and use case structure for GitHub Copilot Google DeepMind What is Prompt Engineering? (Google Cloud) Emphasizes examples, constraints, and prompt testing in code generation LangChain / LangSmith Prompt Engineering Quick Start (LangSmith UI) Versioning, testing, and observability in structured pipelines <p>\u2705 These guides reinforce that prompting is not just an art\u2014it's an engineering discipline, especially in regulated or mission-critical environments.</p>"},{"location":"plays/code-gen-play/#4-important-definitions","title":"4. Important Definitions","text":"<p>To align understanding across diverse teams and stakeholders, this section defines foundational terms used throughout this play.</p> <ul> <li> <p>Code Completion   Token-level or line-level suggestions provided inline as a developer types. Completions typically leverage the immediate code context (e.g., variable names, function signatures) and are most effective in IDE environments. Tools like GitHub Copilot and Continue.dev fall into this category.</p> </li> <li> <p>Code Generation   The use of prompts to produce complete code blocks, files, or configurations. Generation often spans multiple lines and may include functions, classes, test scaffolds, IaC templates, or documentation. It requires clear intent and validation, especially when used for production workflows.</p> </li> <li> <p>CHop Programming (Chat-Oriented Programming)   A structured, intentional approach where prompts are engineered to reliably produce high-quality, mission-aligned code. CHop emphasizes prompt versioning, reuse, and governance. It aligns with secure-by-design practices by treating prompts as first-class artifacts.</p> </li> <li> <p>VIBE Programming   A casual, exploratory mode of working with GenAI, where developers iterate quickly by \"vibing\" with the model\u2014adjusting prompts on the fly without formal structure or traceability. </p> </li> </ul> <p>\u26a0\ufe0f Warning: While this pattern may accelerate prototyping, it poses serious risks in high-assurance environments, including unvetted logic, hallucinated code, policy violations, and the inability to trace how decisions were made. VIBE should be explicitly restricted or bounded by policy, with strict gating before any output is promoted to shared environments or production workflows.</p>"},{"location":"plays/code-gen-play/#shifting-roles-from-creators-to-reviewers-and-beyond","title":"Shifting Roles: From Creators to Reviewers and Beyond","text":"<p>As GenAI capabilities become integrated into daily software workflows, the role of the developer is shifting\u2014from a focus on hand-coding logic to one of curating, reviewing, and validating machine-generated code.This isn\u2019t just a tooling change\u2014it\u2019s a cognitive and cultural shift</p> <p>This is not just a tooling change\u2014it\u2019s a cognitive and cultural transformation.</p>"},{"location":"plays/code-gen-play/#whats-changing","title":"What\u2019s Changing?","text":"<p>Traditional software development emphasizes problem decomposition, algorithm design, and precise implementation. Now with GenAI:</p> <ul> <li>Developers often describe intent in structured natural language, not just code.</li> <li>GenAI fills in boilerplate, scaffolding, and sometimes entire methods or configurations. </li> <li> <p>The human becomes a reviewer, risk manager, and design validator, evaluating:</p> </li> <li> <p>Logical correctness</p> </li> <li>Security and performance implications</li> <li>Fit to architecture, coding standards, and domain norms</li> </ul> <p>Emerging dynamic: In addition to reviewing GenAI output, developers are also leveraging GenAI to critique their own code\u2014identifying vulnerabilities, anti-patterns, or performance issues. This bi-directional review loop is becoming part of modern DevSecOps workflows.</p>"},{"location":"plays/code-gen-play/#risks-without-role-reframing","title":"\u26a0\ufe0f Risks Without Role Reframing","text":"<p>Without acknowledging this shift, teams risk: - Mis-calibrated trust: over-trusting GenAI output when it \u201clooks right,\u201d or under-trusting it when it could be helpful.Over-trusting model output, especially when it \u201clooks right\u201d</p> <ul> <li>Skill atrophy, as deeper logic design and design thinking are is outsourced too early</li> <li>Undertraining reviewers, especially newer engineers unfamiliar with system-level tradeoffs</li> <li>Ambiguity in accountability when machine-generated code enters production pipelines</li> </ul>"},{"location":"plays/code-gen-play/#what-teams-can-do","title":"What Teams Can Do","text":"<ul> <li>Treat GenAI output like a pull request from a junior developer: always review, never assume.</li> <li>Introduce peer-pairing and AI-pairing for both generation and validation.</li> <li>Encourage structured prompt reuse (see: CHop Programming).</li> <li>Define ownership expectations\u2014who is accountable for what\u2019s committed?</li> <li>Train teams in critical review, not just prompting\u2014and begin exploring GenAI tools that review human-written code as part of CI/CD.</li> <li>Prioritize training for both early-career and experienced engineers\u2014covering prompt design, GenAI use, system-level validation, and architectural reasoning\u2014to prevent skill atrophy and ensure mission-aligned decision-making.</li> </ul> <p>The future of software engineering isn\u2019t just faster coding\u2014it\u2019s smarter reviewing. Let\u2019s make sure our teams are equipped for the role they now play.</p>"},{"location":"plays/code-gen-play/#5-use-cases-and-boundaries","title":"5. Use Cases and Boundaries","text":"<p>Not all code is equal when it comes to applying GenAI. This section outlines where AI-generated code offers clear value\u2014and where the risk outweighs the reward.</p>"},{"location":"plays/code-gen-play/#appropriate-use-cases","title":"Appropriate Use Cases","text":"<p>These use cases are well-suited for GenAI assistance because they typically involve low to moderate risk, are bounded in scope, and have well-understood intent\u2014making human oversight easier and more effective. When paired with review, validation, and pipeline-integrated governance, they deliver efficiency gains without compromising mission assurance.</p> <ul> <li> <p>Infrastructure as Code (IaC)   Generating Terraform, CloudFormation, or Ansible templates to scaffold cloud infrastructure or policy-as-code declarations. These follow predictable patterns and benefit from repeatable structure.</p> </li> <li> <p>Unit and Integration Test Scaffolding   Creating boilerplate tests based on function signatures or example inputs/outputs. While initial scaffolding is safe, human augmentation is required for completeness and correctness.</p> </li> <li> <p>Boilerplate and Repetitive Code Generation   Reducing time spent writing repetitive accessors, data transfer objects (DTOs), API wrappers, and data mapping layers. These tasks are pattern-based and present limited functional risk.</p> </li> <li> <p>Documentation Support   Generating inline comments, method docstrings, or markdown files to support code clarity and maintainability. Prompts should reinforce clarity, not just verbosity.</p> </li> <li> <p>Code Refactoring Suggestions \u26a0\ufe0f Use with caution. While GenAI can propose modularization or naming improvements, automated refactoring\u2014especially on production or system-critical code\u2014requires careful human review and regression testing</p> </li> </ul> <p>\u2705 These scenarios benefit from AI acceleration while preserving human accountability and mission alignment.</p>"},{"location":"plays/code-gen-play/#high-risk-use-cases","title":"High-Risk Use Cases","text":"<p>These categories pose elevated risk for hallucination, incomplete logic, or critical vulnerabilities. GenAI tools should generally not be used for generation or modification of source code in these areas\u2014though they may be appropriate for review, analysis, or documentation in a controlled setting:</p> <ul> <li> <p>Authentication and Authorization Logic (AuthN/AuthZ)   Mistakes in access control or identity management can compromise the entire system. These functions merit expert design, human review, and rigorous validation.</p> </li> <li> <p>Encryption or Cryptographic Implementations   GenAI must not generate or modify crypto routines. Always use vetted libraries and defer implementation to trained professionals.</p> </li> <li> <p>Safety- or Mission-Critical Software   Software tied to life, safety, kinetic effects, or weapons systems must follow formal methods and certified workflows. GenAI may assist in reviewing logs or documentation, but not generating executable logic.</p> </li> </ul> <p>NOTE: This is a draft and should not be disseminated yet. Work group members are asked to provide feedback and may share with select individuals who can provide additional expertise.</p> <ul> <li>Export-Controlled or Classified Domains   Prompts involving ITAR, EAR, CUI, or classified topics must not be entered into commercial GenAI tools. Doing so may violate DoD policy and emerging CIO guidance related to data sharing and model boundary protections.</li> </ul> <p>\u274c While GenAI should not be used to author code in these domains, it may play a role in reviewing or critiquing human-authored logic\u2014under the same validation and audit controls used in high-assurance environments.</p>"},{"location":"plays/code-gen-play/#6-trust-verification-and-devsecops-pipeline-integration","title":"6. Trust, Verification, and DevSecOps Pipeline Integration","text":"<p>The trustworthiness of AI-generated code is not a static property\u2014it must be earned, validated, and maintained through repeatable processes. Within secure software delivery environments like those in the DoD, trust is achieved not by assuming GenAI is correct, but by designing workflows that verify and trace every output.</p> <p>This section outlines how teams can integrate GenAI-assisted code generation into existing DevSecOps pipelines, applying rigorous verification practices that preserve mission assurance and accountability.</p>"},{"location":"plays/code-gen-play/#why-trust-must-be-engineered","title":"Why Trust Must Be Engineered","text":"<p>AI-generated code is often syntactically correct but semantically wrong. It may pass initial tests but:</p> <ul> <li>Contain subtle security vulnerabilities</li> <li>Misuse libraries or APIs</li> <li>Violate architectural conventions</li> <li>Drift from mission-specific requirements</li> </ul> <p>But let\u2019s be clear: these risks are not unique to GenAI. Human-generated code also requires validation at time of creation\u2014not deferred to a late-stage review. That\u2019s the essence of DevSecOps: embedding trust, security, and accountability throughout the pipeline, regardless of who (or what) wrote the code.</p>"},{"location":"plays/code-gen-play/#verification-practices","title":"Verification Practices","text":"<p>To establish trust in GenAI-generated code, apply multiple layers of validation:</p> <ul> <li> <p>Automated Tests   Ensure generated functions are covered by unit and integration tests. When test scaffolds are generated alongside code, review both before inclusion.</p> </li> <li> <p>Static and Dynamic Analysis (SAST/DAST)   Scan all generated code as if it were human-authored. Treat it with zero-trust assumptions until verified.</p> </li> <li> <p>Peer Review   AI-assisted code must undergo human review. Focus on logic correctness, boundary conditions, and consistency with mission goals.</p> </li> <li> <p>Prompt and Output Logging   Capture the prompt, model used, and output version. Associate each with a commit or CI/CD build ID for traceability.</p> </li> <li> <p>IP &amp; License Validation   Treat GenAI-generated code as potentially tainted until proven otherwise. Apply SBOM scanning tools and license validators (e.g., FOSSA, ScanCode) to detect unapproved open-source license patterns or reuse of protected content.</p> </li> </ul>"},{"location":"plays/code-gen-play/#pipeline-integration","title":"Pipeline Integration","text":"<p>Trust must be enforced through the pipeline, not just during local development.</p> <ul> <li> <p>IDE + Pre-Commit Hooks   Warn or block commits with unreviewed or untagged AI-generated code.</p> </li> <li> <p>CI/CD Controls   Validate prompts and responses against policy (e.g., permitted models, classification labels). Run tests and scans in automated jobs.</p> </li> <li> <p>Prompt Artifact Management   Store reusable prompts in Git, tag by function (e.g., <code>generate-test-prompt-v3</code>), and associate with model version.</p> </li> <li> <p>Generated Code Labeling   Include inline comments to mark generated code (e.g., <code>// Generated by GitHub Copilot</code>). Support downstream analysis and SBOM inclusion.</p> </li> <li> <p>SBOM/MLBOM Integration   Track model usage and prompt lineage as part of SBOM or emerging MLBOM practices. Link to secure model registries where possible.</p> </li> </ul>"},{"location":"plays/code-gen-play/#tool-behavior-and-model-variability","title":"Tool Behavior and Model Variability","text":"<p>Not all GenAI tools behave the same way\u2014even when using similar model families. Developers and reviewers must account for meaningful differences in:</p> <ul> <li>IDE plugin behavior (e.g., GitHub Copilot vs. Continue.dev)</li> <li>Backend models (e.g., GPT-4 vs. Claude vs. open-source LLMs)</li> <li>Deployment context (e.g., SaaS vs. on-prem vs. air-gapped instances)</li> </ul> <p>These variations can affect everything from code style and verbosity to security filters and prompt interpretation.</p> <p>A prompt that generates clean, testable output in one context may produce noisy or unsafe results in another.</p>"},{"location":"plays/code-gen-play/#practical-recommendations","title":"Practical Recommendations","text":"<ul> <li>Maintain a vetted tool registry with approved GenAI tools, known limitations, and recommended usage patterns or guardrails for each (e.g., prompt libraries, constrained output types, acceptable use cases).</li> <li>Log prompt/model metadata with every commit or pipeline run to ensure traceability, reproducibility, and model lifecycle control.</li> <li>Evaluate tools in representative mission environments before widespread adoption\u2014especially for offline use or deployment on classified or air-gapped systems.</li> <li>Flag and test for version drift in IDE extensions, plugins, or model interpreters to prevent silent changes in behavior or output.</li> <li>Provide role-specific tradecraft guidance for approved tools, including prompt examples, review checklists, and lessons learned from secure use in operational contexts.</li> </ul> <p>For systemic risk guidance, see the upcoming Risk Reference Companion where \u201cTool Drift\u201d, \u201cModel Fragmentation\u201d, and \"Prompt Misuse\" are addressed in more depth.</p>"},{"location":"plays/code-gen-play/#example-human-in-the-loop-workflow","title":"Example Human-in-the-Loop Workflow","text":"<p>A simple, secure-by-design workflow might look like:</p> <p><code>AI Suggests</code> \u2192 <code>Automated Test Runs</code> \u2192 <code>Human Reviews &amp; Approves</code> \u2192 <code>Secure Deploy</code></p> <p>This pattern reinforces accountability and allows teams to scale GenAI usage without weakening their software assurance posture.</p>"},{"location":"plays/code-gen-play/#7-measures-and-feedback","title":"7. Measures and Feedback","text":"<p>Before adopting GenAI tools for code generation, teams must clarify:  </p> <p>What are we trying to improve\u2014and why?</p> <p>This section introduces system-aligned metrics that support trustable, explainable, and secure GenAI integration, guided by the AI-SWEC (AI Software Evaluation Criteria Framework).  Check back for the upcoming guidance and deeper dive into the AI-SWEC.</p>"},{"location":"plays/code-gen-play/#ai-swec-grounding-your-why","title":"AI-SWEC: Grounding Your \u201cWhy\u201d","text":"<p>The AI-SWEC framework helps teams evaluate the fit and effectiveness of GenAI tools using four dimensions:</p> Dimension Sample Questions Value Delivered Did GenAI meaningfully reduce lead time or cognitive burden? Did it support reuse? Effort Required How much prompting, revision, or integration work was needed? Risk Introduced Did the tool create any insecure code, incorrect logic, or hallucinated output? Confidence Gained Can the output be reviewed, tested, and trusted? Was traceability preserved?"},{"location":"plays/code-gen-play/#devsecops-aligned-metrics","title":"DevSecOps-Aligned Metrics","text":"<p>To assess adoption and system health, consider:</p>"},{"location":"plays/code-gen-play/#pipeline-flow","title":"Pipeline &amp; Flow","text":"<ul> <li>Time-to-merge for GenAI-generated code</li> <li>Percentage of PRs with GenAI attribution</li> <li>Prompt reuse rate (from prompt library or versioned prompts)</li> </ul>"},{"location":"plays/code-gen-play/#security-quality","title":"Security &amp; Quality","text":"<ul> <li>Post-generation defect rate (e.g., bugs detected in review or testing)</li> <li>Security issue density in GenAI-authored code (e.g., via SAST/DAST)</li> <li>Review delta: manual changes after GenAI suggestions</li> </ul>"},{"location":"plays/code-gen-play/#compliance-traceability","title":"Compliance &amp; Traceability","text":"<ul> <li>SBOM/MLBOM inclusion of GenAI-generated artifacts</li> <li>Model source attribution logged (e.g., which model, version)</li> <li>Commit tags or inline comments indicating generated code</li> </ul>"},{"location":"plays/code-gen-play/#team-feedback-review-confidence","title":"Team Feedback &amp; Review Confidence","text":"<p>Encourage structured team reflection:</p> <ul> <li>Did using GenAI improve the experience of coding, or just shift the work?</li> <li>What percentage of GenAI-generated code was accepted without revision?</li> <li>Do reviewers trust the output? Why or why not?</li> </ul>"},{"location":"plays/code-gen-play/#pre-adoption-guidance","title":"Pre-Adoption Guidance","text":"<p>Before experimenting with GenAI tools, define your \u201cwhy\u201d:</p> <ul> <li>What phase of the SDLC are we targeting?</li> <li>Are we aiming to improve delivery speed, quality, or knowledge sharing?</li> <li>How will we know if it worked?</li> </ul> <p>Use the AI-SWEC to create a shared hypothesis and evaluation plan.</p> <p>\ud83d\udccc Measure what matters. Don\u2019t mistake code generation volume for mission progress.</p> <p>MITRE\u2019s AI Software Evaluation Criteria (AI-SWEC) framework is a public-good resource developed to support this type of intentional, mission-aligned experimentation. Created by MITRE as part of its work as a Federally Funded Research and Development Center (FFRDC), AI-SWEC is freely available to U.S. government teams, allied partners, and the broader software engineering community.</p> <p>AI-SWEC is not a productivity tracker or tool comparison matrix\u2014it is a lightweight, outcome-focused decision support framework. It helps teams evaluate the adoption of GenAI tools based on:</p> <ul> <li>Value Delivered</li> <li>Effort Required</li> <li>Risk Introduced</li> <li>Confidence Gained</li> </ul> <p>This framework is already in use by government and industry software teams to frame pilots, capture pre/post comparisons, and support trustable integration of GenAI tooling.</p> <p>For facilitation support or templates, contact ArchAITecture@mitre.org.</p>"},{"location":"plays/code-gen-play/#summary-what-to-track-across-the-lifecycle","title":"Summary: What to Track Across the Lifecycle","text":"<p>To evaluate GenAI-assisted code generation effectively, ensure your measures address:</p> <ul> <li>Time-to-value vs. time-to-rework \u2013 Are we gaining speed or just creating cleanup?</li> <li>Prompt reuse and effectiveness \u2013 Are teams learning and sharing effective interactions?</li> <li>Code quality metrics \u2013 Are GenAI outputs helping or harming code reliability and security?</li> <li>Feedback into training and governance \u2013 Are lessons learned shaping future tool use and policy?</li> </ul> <p>These metrics complement AI-SWEC and reinforce mission-focused, evidence-based adoption.</p>"},{"location":"plays/code-gen-play/#8-governance-and-policy","title":"8. Governance and Policy","text":"<p>Governance is essential, but beyond the scope of this play. Full guidance on usage policies, enforcement mechanisms, audit trails, and Zero Trust alignment for GenAI-assisted development will be covered in a dedicated play: Governance for Responsible GenAI Adoption (coming soon).</p> <p>In the meantime, refer to Home / Basics and Fundamentals for Designing an AI-Augmented Tool Chain for baseline traceability and security design principles.</p>"},{"location":"plays/code-gen-play/#9-training-and-workforce-enablement","title":"9. Training and Workforce Enablement","text":"<p>Effectively adopting GenAI tools means developing fluency across roles\u2014not just for software engineers.</p> <p>A full guidance play titled Building an AI-Augmented Workforce will expand on how to grow capability across the AI-Curious to AI-Native spectrum, pairing, mentoring, and preventing skill atrophy.</p> <p>For now, see Fundamentals for Designing an AI-Augmented Tool Chain for prompt engineering and human-in-the-loop patterns.</p>"},{"location":"plays/code-gen-play/#11-whats-emerging","title":"11. What\u2019s Emerging","text":"<p>Agentic platforms, secure software platforms with embedded GenAI, and the shift to low-code and no-code generation are reshaping how code is authored, tested, and deployed.</p> <p>A separate Futures Watch play will explore the implications of these patterns for architectural control, lifecycle governance, and workforce transformation.</p> <p>This play focuses on today\u2019s practices. Refer to Code Generation &amp; Completion for current implementation guidance.</p>"},{"location":"plays/code-gen-play/#12-risks-and-red-flags","title":"12. Risks and Red Flags","text":"<p>This play touches on common risks\u2014hallucinated code, overreliance on opaque models, and IP concerns\u2014but a more detailed exploration will be available in the Risk Reference Companion play.</p> <p>For architectural guidance on identifying and mitigating early-stage risks, refer to Fundamentals for Designing an AI-Augmented Tool Chain and the traceability patterns in Code Generation &amp; Completion.</p> <p>End of Play</p> <ol> <li> <p>BlueOptima. (2024). The Impact of Generative AI on Software Developer Performance (v8). BlueOptima Ltd. Internal Report.\u00a0\u21a9</p> </li> <li> <p>GitHub Copilot Research. (2022). Quantifying Productivity Gains from AI-Powered Code Completion. Retrieved from: https://github.blog/2022-09-14-research-how-github-copilot-helps-developers/\u00a0\u21a9</p> </li> <li> <p>SAS and Coleman Parkes, Your Journey to a GenAI Future: A Strategic Path to Success for Government, SAS Institute Inc., 2024. [Online]. Available: https://www.sas.com/en_us/whitepapers/genai-future-government-114056.html\u00a0\u21a9</p> </li> </ol>"},{"location":"plays/emerging-practices/","title":"Futures Watch: Emerging Practices in AI-Augmented Software Engineering","text":""},{"location":"plays/emerging-practices/#overview","title":"Overview","text":"<p>This play will explore future-facing trends and evolving practices that may reshape how software is designed, developed, and maintained in secure, mission-critical environments.</p>"},{"location":"plays/emerging-practices/#scope","title":"Scope","text":"<p>Futures Watch will analyze:</p> <ul> <li>Agentic software engineering (AI agents coordinating SDLC tasks)</li> <li>Secure digital platforms with embedded GenAI capabilities</li> <li>The low-code/no-code shift for prompt-driven logic authoring</li> <li>Architectural implications for governance, composability, and trust calibration</li> </ul>"},{"location":"plays/emerging-practices/#relationship-to-other-plays","title":"Relationship to Other Plays","text":"<p>This play builds upon:</p> <ul> <li>Code Generation &amp; Completion \u2013 current best practices for AI-assisted authoring</li> <li>Governance for Responsible GenAI Adoption \u2013 anticipating policy and oversight needs</li> </ul>"},{"location":"plays/emerging-practices/#coming-soon","title":"Coming Soon","text":"<p>This play will include architectural patterns, design considerations, and risk tradeoffs associated with future GenAI integrations.</p> <p>This content is exploratory in nature and intended to inform long-range planning, R&amp;D, and strategic readiness.</p>"},{"location":"plays/fundamentals-play/","title":"Play: Fundamentals for Designing an AI-Augmented Tool Chain","text":""},{"location":"plays/fundamentals-play/#executive-summary-the-play-in-brief","title":"Executive Summary (The Play in Brief)","text":"<p>Generative   AI (GenAI) is flooding into software teams\u2014often through developer tools, Integrated Development Environment (IDE) plugins, chatbots, and open Application Programming Interface (API) integrations. As a form of Machine Learning (ML), GenAI models are capable of generating new content\u2014like code, documentation, or test cases\u2014rather than simply classifying or predicting. Across the DoD, there's a clear mandate to accelerate adoption, with recent strategies urging agencies to scale AI capabilities in support of mission objectives. But without foundational knowledge of how these tools are architected\u2014and how they intersect with mission workflows, trust boundaries, and cyber risk\u2014organizations risk making decisions that are fast, but fragile. This play is designed to help DoD teams and software factories design their AI-augmented toolchains with intention. It starts with tools\u2014not because tools come first, but because tooling is where AI shows up first. By unpacking hosting models, usage patterns, and human interaction modalities, this play enables teams to make mission-aligned, architecture-led decisions that support lasting change.</p> <p>It\u2019s not about picking a model. It\u2019s about understanding how tools fit into the broader SDLC\u2014who uses them, why, and with what risk.</p> <ul> <li> <p>TL;DR: Start with your mission use case and workflow\u2014who needs the AI-augmented capability, why, and where it fits in the SDLC.  Then architect your hosting and integration strategy with an eye toward trust boundaries, usage models, interaction patterns, and cyber risk posture. This play unpacks the landscape of  options\u2014so teams can make deliberate, mission-aligned decisions about where GenAI belongs and how to adopt it responsibly.</p> </li> <li> <p>Intended audience: CIOs, Chief Engineers, DevSecOps Leads, Program Managers, Technical Leads, Software Engineers, and Software Factory Architects.</p> </li> <li> <p>\ud83d\udccc Key takeaway: The most important architectural decision isn\u2019t which model\u2014it\u2019s how and why you\u2019re using it. Define your use case first, then select a hosting and integration model that aligns with your mission, workforce, and risk posture.</p> </li> </ul>"},{"location":"plays/fundamentals-play/#1-why-this-play-matters","title":"1. Why This Play Matters","text":"<p>The moment a team or organization decides to experiment with or integrate GenAI into the software development lifecycle, they face a foundational architectural decision: Where will the model live? And just as importantly, who controls it, who can see the inputs, and what risks come with those choices?</p> <p>Choosing a hosting and usage model isn't a technical formality\u2014it\u2019s a strategic inflection point with operational, security, and trust implications. The wrong choice can introduce mission-impacting risk, stall authorizations, or limit scale. The right choice aligns with the mission's sensitivity, embraces the reality of cybersecurity-by-design, and supports long-term sustainability and trust.</p> <p>This is not just about hosting infrastructure. It\u2019s about:</p> <ul> <li>Defining your trust boundaries</li> <li>Controlling data ingress and egress</li> <li>Quantifying exposure to adversarial AI threats, model drift, and data leakage</li> <li>Ensuring compliance with DoD data classification, Zero Trust mandates, and supply chain integrity</li> </ul> <p>In short, before choosing a model or a vendor, teams must step back and ask: What is the mission, what is the risk tolerance, and what\u2019s the operational boundary that will keep us in control?</p>"},{"location":"plays/fundamentals-play/#2-architecting-the-ai-augmented-toolchain","title":"2. Architecting the AI-Augmented Toolchain","text":"<p>GenAI is not a bolt-on capability. As it becomes embedded across the software development lifecycle\u2014from planning and design to testing, deployment, and sustainment\u2014it demands deliberate architectural thinking. Choosing the right tooling isn\u2019t enough. We must design the AI-augmented toolchain to ensure it\u2019s observable, controllable, and secure by design.</p>"},{"location":"plays/fundamentals-play/#what-is-an-ai-augmented-toolchain","title":"What Is an AI-Augmented Toolchain?","text":"<p>An AI-augmented toolchain refers to a pipeline or suite of tools where one or more components (code generators, test writers, documentation agents, deployment optimizers, etc.) are infused with AI\u2014often via LLMs or agentic systems. These tools can be passive (suggesting code) or active (taking autonomous actions).</p> <p>In DoD and other high-assurance environments, augmenting the toolchain introduces new architectural surfaces:</p> <ul> <li>Trust boundaries between human, model, and action</li> <li>Prompt design and versioning as code artifacts</li> <li>Feedback loops that must be observable and auditable</li> <li>Emergent behavior that must be bounded or governed (e.g., AI agents bypassing security gates to optimize for speed)</li> </ul>"},{"location":"plays/fundamentals-play/#architecting-for-security-and-trustworthiness","title":"Architecting for Security and Trustworthiness","text":"<p>Just as we embrace *DevSecOps as a mindset, we must extend it to AI-Augmented DevSecOps. This includes:</p> Area Architectural Guidance Identity &amp; Access Enforce least-privilege access for AI tools. Ensure LLMs or agents cannot access sensitive scopes unless explicitly permitted. Prompt Provenance Treat prompts and context injections as code\u2014version them, audit them, and protect them. Model Boundaries Clarify where models live (local, cloud, hybrid) and enforce strict egress controls. Observability &amp; Logging Introduce telemetry at every interaction point: prompts, model outputs, agent actions. Testability Validate AI-generated outputs with both traditional and AI-aware test harnesses. Rollback &amp; Recovery Enable rollback of both model versions and AI-generated artifacts (e.g., code, configs). Policy Enforcement Integrate policies into DevSecOps to block unauthorized model use, drift, or dependency pulls."},{"location":"plays/fundamentals-play/#reference-architecture-concepts","title":"Reference Architecture Concepts","text":"<p>To build mission-ready, AI-augmented pipelines, organizations must rethink how data, models, and logic interact across the software development lifecycle. The following architectural patterns provide building blocks for trustworthy, scalable, and observable GenAI integrations:</p>"},{"location":"plays/fundamentals-play/#a-promptops-layer","title":"A. PromptOps Layer","text":"<p>Establish a dedicated layer in your pipeline to manage prompts as code\u2014including reusable templates, version control, parameter injection, and governance. This improves traceability and reproducibility of AI-generated outputs.  Think of it like CI/CD for prompts.</p>"},{"location":"plays/fundamentals-play/#b-retrieval-augmented-generation-rag-broker","title":"B. Retrieval-Augmented Generation (RAG) Broker","text":"<p>When external models (like LLMs) are used, a RAG broker retrieves internal, curated context (e.g., knowledge bases, architecture docs, ticket history) to send along with the prompt. This reduces hallucination risk and increases model relevance\u2014without retraining the model itself.</p>"},{"location":"plays/fundamentals-play/#c-policy-as-code-for-ai","title":"C. Policy-as-Code for AI","text":"<p>Use policy engines (e.g., Open Policy Agent, Conftest) to inspect and enforce rules around where and how AI is used. For example: block certain model types in production, restrict prompt content, or require logging before execution.   AI needs to be governed just like infrastructure.</p>"},{"location":"plays/fundamentals-play/#d-agent-execution-guardrails","title":"D. Agent Execution Guardrails","text":"<p>For agent-based systems (e.g., AutoGPT, Crew.AI, OpenHands), introduce runtime controls that constrain behavior. Examples include:</p> <ul> <li>Memory limits  </li> <li>Execution timeouts  </li> <li>Tool usage boundaries These guardrails reduce the risk of emergent or uncontrolled AI behaviors.</li> </ul>"},{"location":"plays/fundamentals-play/#e-ai-supply-chain-transparency-sboms-model-boms-and-data-cards","title":"E. AI Supply Chain Transparency: SBOMs, Model BOMs, and Data Cards","text":"<p>Track not only your traditional software dependencies (SBOM), but also AI-specific components:</p> <ul> <li>Model BOMs: The models you're using, their architecture, weights, and versioning</li> <li>Data Cards: Documentation for training and fine-tuning datasets  </li> <li>AI BOMs: Broader AI system dependencies and integration points</li> </ul> <p>This multi-layered transparency supports reproducibility, compliance, and secure AI supply chain practices.</p> <p>For detailed implementation guidance on Model BOMs, Data Cards, and AI supply chain documentation, see the companion AI Supply Chain Transparency Guide</p>"},{"location":"plays/fundamentals-play/#recommendation","title":"\ud83d\udccc Recommendation","text":"<p>These components are not one-size-fits-all. Start small\u2014map one toolchain, identify one use case\u2014and apply these patterns incrementally as trust and complexity grow.</p>"},{"location":"plays/fundamentals-play/#example-from-code-suggestion-to-secure-toolchain","title":"Example: From Code Suggestion to Secure Toolchain","text":"<p>Let\u2019s say your team adopts GitHub Copilot or integrates an internal LLM for code generation.</p> <p>Without architectural planning:</p> <ul> <li>Prompts aren\u2019t versioned.</li> <li>No record of generated output.</li> <li>Outputs go straight into pipeline with no review.</li> <li>Developers rely on outputs without understanding.</li> </ul> <p>With AI-Augmented Toolchain Architecture:</p> <ul> <li>Prompts and outputs are logged and versioned.</li> <li>Every AI suggestion is validated against policy.</li> <li>Code gen is gated by a test scaffold validator.</li> <li>Generated code can be traced back to a prompt and model version.</li> </ul>"},{"location":"plays/fundamentals-play/#takeaway","title":"\ud83d\udccc Takeaway","text":"<p>It's kind of like a driver assistance system. It doesn't prevent all accidents that can happen, but it makes traffic a little bit more secure.</p> <p>\u2014 Thomas Dohmke, CEO of GitHub, June 2023 </p>"},{"location":"plays/fundamentals-play/#3-define-the-hosting-and-usage-models","title":"3. Define the Hosting and Usage Models","text":"<p>Before selecting a tool, service, model, or vendor, it\u2019s essential to understand the distinct hosting and usage patterns available for GenAI across the SDLC. Each model comes with architectural, security, and operational implications\u2014especially in the context of federal classification levels, cATO pipelines, and Zero Trust mandates.</p> <p>The primary categories are:</p>"},{"location":"plays/fundamentals-play/#public-saas-model","title":"Public SaaS Model","text":"<p>Examples: ChatGPT via OpenAI.com, Claude, Bard, Gemini (unclassified public interfaces)</p> <ul> <li>\u2705 Pros: Immediate access, broad community knowledge, rapid iteration</li> <li>\u26a0\ufe0f Risks: Model weights are opaque, vendor-managed updates may introduce regression or drift; user inputs may be logged or retained; no guarantee of U.S. jurisdiction; limited ability to enforce data governance</li> <li>Security Context: High external trust boundary; not suitable for mission-critical, export-controlled, or CUI workloads</li> <li>Use Case Fit: Low-risk experimentation, internal education, code snippets for generic use\u2014not for production or sensitive use</li> </ul>"},{"location":"plays/fundamentals-play/#government-saas-controlled-cloud","title":"Government SaaS / Controlled Cloud","text":"<p>Examples: Azure OpenAI in IL4/5, AWS Bedrock in GovCloud, Google Gov AI</p> <ul> <li>\u2705 Pros: FedRAMP Moderate/High or IL4/5/6 compliance; access to proprietary model strengths with more boundary clarity; improved telemetry and logging</li> <li>\u26a0\ufe0f Risks: Reliant on third-party updates; difficult to guarantee reproducibility or model immutability; access control is only as strong as your cloud configuration</li> <li>Integration Fit: Good for DevSecOps teams using Platform One, Cloud One, or internal AI services aligned to JWCC constructs</li> <li>Security Context: Moderate to High trust boundary; acceptable for CUI and some mission workloads depending on use case</li> </ul>"},{"location":"plays/fundamentals-play/#self-hosted-air-gapped-open-source-model","title":"Self-Hosted / Air-Gapped / Open Source Model","text":"<p>Examples: LLaMA 2, Mistral, Falcon, Dolly, fine-tuned FLAN-T5, Mixtral, custom RAG solutions</p> <ul> <li>\u2705 Pros: Maximum control, full auditability, offline operation; essential for classified, air-gapped, or multi-national environments</li> <li>\u26a0\ufe0f Risks: Requires internal expertise to fine-tune, maintain, secure, and serve models; significant MLOps burden; potential risk of underperforming models without tuning</li> <li>Security Context: Highest trust boundary; architected for enclave deployment, SCIF integration, and red/black separation</li> <li>Use Case Fit: Mission-critical decision support, secure coding, embedded agents in ISR/command software, disconnected ops</li> </ul>"},{"location":"plays/fundamentals-play/#hybrid-models","title":"Hybrid Models","text":"<p>Examples: RAG architecture with local vector DB + external LLM callout, or mixed trust layering</p> <ul> <li>\u2705 Pros: Retain local context and control; reduce data exposure by decoupling model from sensitive knowledge; opportunity to build fine-tuned pipelines with prompt injection defense</li> <li>\u26a0\ufe0f Risks: Complex architecture introduces new attack surfaces; demands rigorous prompt sanitization, inference auditing, and failover strategies</li> <li>Trust Impact: Can increase calibrated trust if traceability and observability are engineered properly</li> </ul>"},{"location":"plays/fundamentals-play/#ai-hosting-and-usage-models-comparison-matrix","title":"AI Hosting and Usage Models: Comparison Matrix","text":"Criteria Public SaaS(e.g., OpenAI, Bard) Gov SaaS / Controlled Cloud(e.g., Azure OpenAI IL5) Self-Hosted / Air-Gapped(e.g., LLaMA2, Mistral) Hybrid (RAG / Mixed Trust) Security Posture \ud83d\udd34 Low (external trust boundary) \ud83d\udfe1 Moderate  (cloud-config dependent) \ud83d\udfe2 High (max control, enclave ready) \ud83d\udfe1 Variable (depends on architecture) Data Control \ud83d\udd34 None (inputs may be retained/logged) \ud83d\udfe1 Partial(depends on configuration) \ud83d\udfe2 Full (data stays within domain) \ud83d\udfe1 Conditional (requires strict design) Model Transparency \ud83d\udd34 Opaque (vendor-managed weights) \ud83d\udd34 Mostly opaque (versioning limited) \ud83d\udfe2 Transparent (open weights and config) \ud83d\udfe1 Partial (depends on external callouts) Performance \ud83d\udfe2 High (vendor-optimized infra) \ud83d\udfe2 High (cloud-accelerated) \ud83d\udfe1 Moderate (depends on in-house tuning) \ud83d\udfe1 Variable (depends on pipeline design) Operational Cost \ud83d\udfe1 Low up-front (can scale fast) \ud83d\udfe1 Subscription/ licensing (cost per token or instance) \ud83d\udd34 High setup\ud83d\udfe1 Lower long-term TCO \ud83d\udfe1 Moderate (RAG infra + model callout costs) Model Reproducibility \ud83d\udd34 None (model updates at vendor discretion) \ud83d\udd34 Limited(some vendor change control) \ud83d\udfe2 High(versions locked, reproducible) \ud83d\udfe1 Mixed (depends on callout dependencies) Mission Fit (Classified/CUI) \ud83d\udd34 Poor(not suitable) \ud83d\udfe1 Moderate (IL4/5 compliant workloads) \ud83d\udfe2 Excellent (supports SCIF, disconnected ops) \ud83d\udfe1 Moderate (requires strict partitioning) Integration Complexity \ud83d\udfe2 Minimal (API-based access) \ud83d\udfe2 Moderate  (P1/C1 aligned^) \ud83d\udd34 High (requires MLOps, infra buildout) \ud83d\udd34 High (requires orchestrated pipeline) <p>^ P1 = Platform One, C1 = Cloud One</p>"},{"location":"plays/fundamentals-play/#legend","title":"Legend:","text":"<ul> <li>\ud83d\udfe2 = Preferred / Strong Alignment</li> <li>\ud83d\udfe1 = Acceptable with Caution / Design Needed</li> <li>\ud83d\udd34 = High Risk / Higher Complexity </li> </ul>"},{"location":"plays/fundamentals-play/#4-decision-framework-choosing-the-right-ai-hosting-and-usage-model","title":"4. Decision Framework: Choosing the Right AI Hosting and Usage Model","text":"<p>When introducing GenAI into the software development lifecycle, it\u2019s tempting to reach for the most powerful model or the easiest plug-in. But success in a high-assurance, mission-driven environment like the DoD demands a deliberate decision-making framework\u2014one that accounts for classification level, trust posture, sustainment, and mission criticality.</p> <p>This framework helps guide teams\u2014software factories, PMOs, mission leads, and cyber operators\u2014through a set of architectural questions to select the right AI hosting and usage model, not just the most available one.</p>"},{"location":"plays/fundamentals-play/#step-1-determine-mission-sensitivity-and-context","title":"Step 1: Determine Mission Sensitivity and Context","text":"Question Why It Matters What is the classification level of the data or workload? Determines model placement (e.g., public SaaS is disallowed for CUI or classified) Is this mission-critical, safety-critical, or time-sensitive? High-stakes decisions demand higher trust, auditability, and model control Will AI outputs directly influence code, policy, deployment, or operations? Direct impact requires tighter control, testing, and provenance"},{"location":"plays/fundamentals-play/#step-2-assess-technical-and-operational-constraints","title":"Step 2: Assess Technical and Operational Constraints","text":"Question Why It Matters Are you able to sustain a self-hosted or hybrid model (e.g., infrastructure, MLOps)? Not every environment has the staff or resources to support open-source models securely Does your pipeline support prompt logging, model versioning, and traceability? Without these, you can\u2019t build calibrated trust or meet ATO expectations What latency, scale, or throughput requirements do you have? Some models work best locally; others need elastic cloud scale or acceleration"},{"location":"plays/fundamentals-play/#step-3-evaluate-cyber-and-compliance-risk","title":"Step 3: Evaluate Cyber and Compliance Risk","text":"Question Why It Matters Is this model FedRAMP authorized or operating inside IL4/5/6? Ensures alignment with DoD risk management frameworks Can you audit all AI interactions (input, model, output)? Required for cyber resilience and post-incident forensics Are there constraints around vendor ownership, model sourcing, or training data provenance? Critical for understanding and mitigating geopolitical risk or model bias exposure"},{"location":"plays/fundamentals-play/#step-4-match-to-a-hosting-model","title":"Step 4: Match to a Hosting Model","text":"Hosting Model Best Fit For\u2026 Public SaaS Low-risk prototyping and internal training only. Not mission workloads. Gov SaaS / Controlled Cloud Moderate-trust workloads in IL4/5/6 with vendor-supported models. Self-Hosted / Open Source High-assurance, enclave or air-gapped missions. Full model control and traceability. Hybrid / RAG Context-specific augmentation of existing SDLC with controlled external inference."},{"location":"plays/fundamentals-play/#output-architectural-decision-record-adr","title":"Output: Architectural Decision Record (ADR)","text":"<p>For each decision, teams should record:</p> <ul> <li>Mission description and data classification</li> <li>Selected hosting model and justification</li> <li>Expected model usage (e.g., generate tests, support code review)</li> <li>Controls in place (prompt logging, access control, rollback procedures)</li> <li>Risk exceptions or mitigations</li> </ul> <p>A sample architectural decision record template is available[here].(https://ArchitecturalDecisionRecord.md) </p>"},{"location":"plays/fundamentals-play/#5-how-humans-interact-with-ai-augmented-development-tools","title":"5. How Humans Interact with AI-Augmented Development Tools","text":"<p>This play focuses specifically on AI-augmented workflows where humans retain decision-making authority and AI tools provide suggestions, analysis, or assistance. This represents the current state of most GenAI tools in the SDLC\u2014from code completion to test generation\u2014where human oversight and validation remain essential. For guidance on autonomous AI systems and levels of autonomy, see the companion Navigating the AI Autonomy Continuum play.</p> <p>As organizations adopt GenAI, they're not just choosing models or hosting platforms\u2014they're defining how humans and machines will collaborate. These interaction patterns vary widely across environments, each bringing different levels of traceability, auditability, and risk.</p> <p>These patterns shape:</p> <ul> <li>Developer workflows and mental models  </li> <li>Security and compliance boundaries  </li> <li>Trust calibration and explainability  </li> <li>The software factory\u2019s ability to scale and govern</li> </ul>"},{"location":"plays/fundamentals-play/#ai-interaction-patterns-todays-landscape","title":"AI Interaction Patterns: Today\u2019s Landscape","text":"Pattern Description Benefits Challenges Standalone Web Interfaces (e.g., ChatGPT, Claude) Accessed via browser or mobile interface, disconnected from enterprise tools or pipelines. \u2705 Easy to access  \u2705 Fast iteration for prototyping and learning \ud83d\udd34 No integration with enterprise workflows  \ud83d\udd34 No traceability or auditability  \ud83d\udd34 Encourages \u201cout-of-band\u201d use IDE Plugins and Adapters (e.g., GitHub Copilot, Continue.Dev) Embedded directly into local development environments, offering in-line code suggestions. \u2705 Accelerates code scaffolding  \u2705 Familiar developer UX \u26a0\ufe0f No architectural context  \u26a0\ufe0f Little prompt/version control  \u26a0\ufe0f Difficult to share prompts across teams AI-First IDEs / Workspaces (e.g., WindSurf, OpenHands) Full-stack environments built around natural language workflows and agentic collaboration. \u2705 Abstracts complexity  \u2705 Integrated agents and tool orchestration \u26a0\ufe0f Redefines team roles  \u26a0\ufe0f Harder to trace decisions  \u26a0\ufe0f Challenges compliance and DevSecOps gates Custom API Integrations (e.g., OpenAI API, Bedrock, internal-hosted LLMs) Embedded into backend or infrastructure via programmatic callouts. \u2705 High control  \u2705 Supports observability and prompt templating \u26a0\ufe0f Requires prompt governance  \u26a0\ufe0f Must be securely integrated into pipelines Agentic Platforms (e.g., AutoGPT, DevAgent prototypes) Orchestrate multi-step tasks using AI agents with memory, planning, and autonomy. \u2705 Delegation of complex tasks  \u2705 Can span across SDLC phases (e.g., testing, deployment) \u26a0\ufe0f Changes developer role to \"AI supervisor\"  \u26a0\ufe0f Emergent behavior risk  \u26a0\ufe0f Needs new trust models and calibration layers"},{"location":"plays/fundamentals-play/#key-insight","title":"\ud83d\udccc Key Insight","text":"<p>\"Essentially, the human-in-the-loop approach reframes an automation problem as a Human-Computer Interaction (HCI) design problem. In turn, we've broadened the question of 'how do we build a smarter system?' to 'how do we incorporate useful, meaningful human interaction into the system?'\"</p> <p>\u2014 Ge Wang, Professor at Stanford University's Human-centered AI initiative</p>"},{"location":"plays/fundamentals-play/#architectural-implication","title":"Architectural Implication","text":"<p>Each interaction pattern affects:</p> <ul> <li>Data flow boundaries</li> <li>Prompt versioning and auditability</li> <li>Alignment with DevSecOps pipelines</li> <li>Calibrated trust for decision-making</li> </ul> <p>These choices must be made intentionally and architected accordingly\u2014especially in regulated or mission-critical environments.</p>"},{"location":"plays/fundamentals-play/#6-designing-for-trust-and-cybersecurity","title":"6. Designing for Trust and Cybersecurity","text":"<p>Incorporating GenAI into the SDLC isn\u2019t just about innovation\u2014it\u2019s a redefinition of trust boundaries. Every prompt, every model call, and every AI-generated output introduces a new surface area for cyber risk, architectural drift, and decision-making opacity.</p> <p>To build mission-ready AI systems, we must design trust in from the start, not inspect it in after the fact.</p>"},{"location":"plays/fundamentals-play/#trust-and-assurance-are-system-propertiesnot-features","title":"Trust and Assurance Are System Properties\u2014Not Features","text":"<p>In traditional systems, trust is built through validation, test coverage, logging, and code reviews. But AI changes the game:</p> <ul> <li>The same prompt may yield different results across time or models.</li> <li>Model updates may occur silently, breaking reproducibility.</li> <li>Human developers may unknowingly accept AI-generated errors or biased outputs.</li> </ul> <p>Trust and assurance in AI-augmented systems must be designed, measured, and recalibrated\u2014just like we do with human teammates. The non-deterministic nature of AI algorithms means we need both trust (confidence in the system's reliability) and assurance (evidence-based confidence in the system's behavior and controls).</p> <p>Trust in an AI-augmented system must be designed, measured,  and recalibrated\u2014just like we do with human teammates.</p>"},{"location":"plays/fundamentals-play/#use-the-calibrated-trust-lens","title":"Use the Calibrated Trust Lens","text":"<p>To support responsible and mission-aligned integration of AI, apply the Calibrated Trust framework (Belief, Understanding, Intent, and Reliance):</p> Dimension Design Questions Belief Is this AI tool appropriate for this task? Is it performing as advertised? Understanding Can users and reviewers explain how the model was used and what data it touched? Intent Are we confident the model\u2019s goals (training data, fine-tuning) align with mission objectives? Reliance Under what conditions should this output be trusted, used, or overridden? <p>This framework helps system owners place the right trust in the right AI at the right time.</p>"},{"location":"plays/fundamentals-play/#design-principles-for-secure-ai-augmented-systems","title":"Design Principles for Secure AI-Augmented Systems","text":"Principle Description Minimize Model Attack Surface Restrict model exposure to only those workflows where it adds verifiable value. Apply egress filtering. Enforce Prompt Governance Treat prompts like code\u2014require reviews, change control, and versioning. Audit All Interactions Log prompts, outputs, model version, and decision trails. Tie to existing DevSecOps audit logs. Enable Model Locking Freeze model versions in production systems. Defer updates until retested in staging. Bound Emergent Behavior Use runtime policies to define agent action scope, memory limits, and timeouts. Secure the Data Plane Encrypt context payloads, anonymize sensitive data, and validate all external callouts. Continuous Evaluation Monitor AI outputs against KPIs, ground truths, benchmarks, and known vulnerabilities. Watch for hallucination, bias, drift."},{"location":"plays/fundamentals-play/#enabling-controls-from-dod-strategy","title":"Enabling Controls from DoD Strategy","text":"<ul> <li>Align to Zero Trust Architecture (ZTA) principles\u2014identity, device, data, network, and workload segmentation</li> <li>Apply Supply Chain Risk Management (SCRM) to model sourcing, prompt datasets, and third-party APIs</li> <li>Adopt AI RMF (NIST SP 1270) and emerging DoD AI Governance practices for mission assurance</li> </ul>"},{"location":"plays/fundamentals-play/#key-insight_1","title":"\ud83d\udccc Key Insight","text":"<p>Calibrated trust is the 1:1 ratio between human trust and trustworthiness of the automation.</p> <p>\u2014 Patricia L. McDermott</p>"},{"location":"plays/fundamentals-play/#7-measures-and-success-indicators-measuring-what-matters","title":"7: Measures and Success Indicators \u2013 Measuring What Matters","text":"<p>Choosing the right AI hosting and usage model is only the first step. To ensure long-term success, teams must define and track key indicators that reflect not just adoption\u2014but effectiveness, governance, and alignment with mission outcomes.</p> <p>This section outlines how to design meaningful measurements for GenAI use across the SDLC, recognizing that metrics will vary by task, tool, and maturity level. As organizations experiment and scale, early metric volatility is expected and should be proactively communicated to leadership to prevent misinterpretation as failure.</p>"},{"location":"plays/fundamentals-play/#key-considerations","title":"Key Considerations","text":""},{"location":"plays/fundamentals-play/#start-with-the-mission-goal","title":"Start with the Mission Goal","text":"<p>Metrics should be anchored to your use case. Is the goal to improve code quality? Accelerate delivery? Enhance documentation? Metrics must fit the outcome, not the novelty of the tool.</p>"},{"location":"plays/fundamentals-play/#track-a-balanced-set-of-indicators","title":"Track a Balanced Set of Indicators","text":"<p>Avoid over-optimizing on a single metric. Instead, track multiple dimensions of performance\u2014DevSecOps health, developer experience, code quality, trust posture, and mission impact.</p>"},{"location":"plays/fundamentals-play/#expect-experimentation-and-learning","title":"Expect Experimentation and Learning","text":"<p>GenAI adoption introduces learning curves. New metrics will fluctuate as teams adjust, and even traditional ones may temporarily dip. This is normal\u2014and part of evaluating transformation.</p>"},{"location":"plays/fundamentals-play/#maintain-a-comprehensive-sdlc-view","title":"Maintain a Comprehensive SDLC View","text":"<p>Adoption effects ripple across the lifecycle\u2014from planning and testing to operations and compliance. Look beyond the IDE. Watch how workflows shift.</p>"},{"location":"plays/fundamentals-play/#on-metric-targets-and-minimums","title":"On Metric Targets and Minimums","text":"<p>While some metrics may have industry benchmarks or compliance thresholds, blindly aiming for generic targets can lead to shallow improvements\u2014or worse, gaming the system.</p> <p>Instead:</p> <ul> <li> <p>If the metric is new to your organization (e.g., % of AI-generated code accepted), the priority is to establish a baseline\u2014not chase a number.</p> </li> <li> <p>Use that baseline to understand behavior, not to assign judgment.</p> </li> <li> <p>Focus on trendlines, deltas, and alignment with mission goals\u2014especially for metrics related to DevEx, prompt governance, or ML model trust.</p> </li> <li> <p>Qualitative context matters\u2014metrics like Code Coverage or Churn require understanding of how and why those numbers move.</p> </li> </ul>"},{"location":"plays/fundamentals-play/#metric-categories-and-indicators","title":"Metric Categories and Indicators","text":"Measure Area Sample Metric / Indicator Why It Matters Security Posture % of AI interactions logged and reviewed  # of model version rollbacks initiated Ensures traceability, detects misuse, and supports post-incident forensics Hosting Alignment % of AI tools hosted in IL4/5+ environments  # of tools outside approved environments Highlights shadow IT, supports ATOs, and enforces classification policy Prompt Governance % of prompts versioned and stored  Time to detect/respond to unsafe prompt behavior Promotes secure-by-design usage and trust calibration Operational Effectiveness Time to integrate GenAI into CI/CD  % of outputs accepted without modification Tracks tooling maturity, usefulness, and risk of over-reliance Mission Impact Estimated hours saved by automation  % of teams actively using AI in at least one SDLC phase Supports ROI discussions and shows adoption maturity Developer Experience (DevEx) Sentiment survey scores  Time saved on routine coding tasks Signals morale, tool fit, and efficiency in daily work"},{"location":"plays/fundamentals-play/#traditional-metrics-still-matter","title":"Traditional Metrics Still Matter","text":"<p>Keep tracking foundational metrics\u2014they often reveal subtle shifts in quality or risk:</p> <ul> <li>Change Failure Rate (CFR): % of code changes causing production issues. GenAI should reduce CFR with better tests and cleaner code.</li> <li>Code Churn: Measures how often code is changed. GenAI might reduce churn or, conversely, increase it during prompt tuning.</li> <li>Code Coverage: GenAI-generated tests can improve this\u2014but quality, not just quantity, must be monitored.</li> </ul>"},{"location":"plays/fundamentals-play/#how-to-use-these-metrics","title":"How to Use These Metrics","text":"<ol> <li> <p>Pre-Adoption Baseline    Capture a snapshot of SDLC practices and outcomes before GenAI integration.</p> </li> <li> <p>Post-Adoption Comparison    Reassess metrics after each AI-Augmented tool rollout to evaluate measurable impact.</p> </li> <li> <p>Scorecard Reporting    Aggregate metrics into an AI Integration Score for quarterly leadership briefings, ATO artifacts, or cyber posture reviews.</p> </li> </ol>"},{"location":"plays/fundamentals-play/#chasing-the-right-metrics","title":"Chasing the Right Metrics","text":"<p>Don\u2019t chase metrics\u2014calibrate them. When introducing new metrics to track AI-augmented work, your first goal isn\u2019t to hit a target\u2014it\u2019s to understand your starting point.</p> <ul> <li>If it\u2019s a new metric: Establish a baseline. Don\u2019t assign judgment yet.</li> <li>If it\u2019s a legacy metric: Expect movement. Track trends and context, not just the number.</li> </ul> <p>Examples:</p> <ul> <li>A Code Coverage rate of 10% means developers are checking a box\u2014not delivering testable systems.</li> <li>A spike in Code Churn after AI adoption may indicate misaligned prompts or low-quality suggestions.</li> <li>A drop in CFR might reflect better testing\u2014or developers overriding useful feedback to \u201cimprove\u201d the number.</li> </ul> <p>\ud83d\udccc Metrics don\u2019t create value\u2014insight does. Use metrics to tell a story about your transformation, not to perform for one.</p>"},{"location":"plays/fundamentals-play/#expect-metric-volatilityand-plan-for-it","title":"Expect Metric Volatility\u2014And Plan for It","text":"<p>One of the most important truths about measuring AI-augmented SDLC performance is this: metrics will waver.</p> <ul> <li>If you\u2019re tracking existing metrics (like Change Failure Rate, deployment frequency, or code review coverage), they may dip or spike as teams adopt new tooling, rewire their workflows, and recalibrate what \u201cgood\u201d looks like.</li> <li>If you introduce net-new metrics (like prompt reuse rates or AI-generated output acceptance), expect early variability as teams build familiarity and establish baseline behaviors.</li> </ul> <p>This is normal\u2014and it does not mean the adoption is failing.</p> <p>Instead, use this \u201cmetrics turbulence\u201d as a signal:</p> <ul> <li>Is the team adjusting well to the AI-augmented workflow?</li> <li>Are quality or trust issues driving dips?</li> <li>Do we need to shift training, modify prompts, or adjust where the tool is used?</li> </ul> <p>Teams should anticipate a learning curve and pair metrics with context: surveys, interviews, human-in-the-loop feedback, and AI-SWEC evaluations. This helps distinguish between signal and noise, and supports a calibrated trust journey rather than a binary success/failure view.</p> <p>\ud83d\udccc A dip in metrics doesn\u2019t mean you made the wrong architectural choice. It means you\u2019re watching transformation in real time\u2014and transformation takes iteration.</p> <p>Early metric volatility is expected during AI adoption and should be proactively communicated to leadership to prevent misinterpretation as failure rather than part of the transformation curve.</p>"},{"location":"plays/fundamentals-play/#8-five-common-missteps","title":"8. Five Common Missteps","text":"<p>Even well-intentioned teams can run into trouble when integrating GenAI into the SDLC. Without an architecture-first, trust-aware approach, the AI can accelerate risk just as fast as it accelerates productivity.</p> <p>Here are the most common missteps observed in early adopters across government and industry\u2014and how to avoid them:</p>"},{"location":"plays/fundamentals-play/#1-using-public-saas-models-for-sensitive-workloads","title":"1. Using Public SaaS Models for Sensitive Workloads","text":"<p>The Mistake: Teams use OpenAI.com or other public endpoints for code review, test generation, or design suggestions on mission or export-controlled projects.</p> <p>Why It Happens: Accessibility and ease of use. It \u201cjust works.\u201d</p> <p>Consequence: Potential data exfiltration, policy violations, and untrackable model influence.</p> <p>Preventive Action: Enforce hosting tiers with strict boundary rules. Train teams on model provenance and data classification constraints.</p>"},{"location":"plays/fundamentals-play/#2-treating-prompts-like-ephemeral-artifacts","title":"2. Treating Prompts Like Ephemeral Artifacts","text":"<p>The Mistake: Prompts are created ad hoc, modified on the fly, and never versioned or reviewed.</p> <p>Why It Happens: Developers see prompts as \u201cinputs\u201d rather than \u201ccode.\u201d</p> <p>Consequence: No reproducibility, no audit trail, and no trust calibration.</p> <p>Preventive Action: Establish PromptOps practices. Version and log prompts like source code.</p>"},{"location":"plays/fundamentals-play/#3-bypassing-human-in-the-loop-checks","title":"3. Bypassing Human-in-the-Loop Checks","text":"<p>The Mistake: Generated code, test cases, or documentation are automatically merged or published without review.</p> <p>Why It Happens: Pressure for speed or belief that \u201cAI knows best.\u201d</p> <p>Consequence: Introduces hallucinated logic, security vulnerabilities, or compliance violations into the system.</p> <p>Preventive Action: Require human checkpoints or policy-as-code gates before AI outputs influence production.</p>"},{"location":"plays/fundamentals-play/#4-ignoring-model-update-drift","title":"4. Ignoring Model Update Drift","text":"<p>The Mistake: Teams don\u2019t track when a model updates in the background (especially true for SaaS models).</p> <p>Why It Happens: Lack of visibility into vendor-side operations.</p> <p>Consequence: Regression bugs, inconsistent results, broken compliance artifacts.</p> <p>Preventive Action: Lock versions in production. Test new versions in staging. Create an AI model bill of materials (MLBOM).</p>"},{"location":"plays/fundamentals-play/#5-over-reliance-on-tooling-without-calibrated-trust","title":"5. Over-Reliance on Tooling Without Calibrated Trust","text":"<p>The Mistake: Teams accept all AI suggestions as truth or assume AI always improves quality.</p> <p>Why It Happens: Trust by default rather than trust by design.</p> <p>Consequence: Increased risk of low-quality or misleading results, especially in decision support or automation contexts.</p> <p>Preventive Action: Evaluate confidence and risk\u2014not just performance.</p>"},{"location":"plays/fundamentals-play/#strategic-insight","title":"\ud83d\udccc Strategic Insight","text":"<p>As we start moving beyond what's possible with GenAI, solid opportunities are emerging to help solve a number of perennial issues plaguing cybersecurity, particularly the skills shortage and unsecure human behavior.</p> <p>- Deepti Gopal, Director Analyst at Gartner, speaking at Gartner Security &amp; Risk Management Summit 2024</p>"},{"location":"plays/fundamentals-play/#9-recommendations-next-best-play","title":"9. Recommendations &amp; Next Best Play","text":"<p>Choosing the right AI hosting and usage model is foundational\u2014it\u2019s not a one-time configuration but an evolving architectural and cybersecurity commitment. As AI tools become more deeply integrated into the SDLC, organizations must treat them as first-class citizens in the software ecosystem, subject to the same rigor as any mission-critical capability.</p> <p>Below are actionable recommendations for DoD technical teams, leadership, and acquisition stakeholders.</p>"},{"location":"plays/fundamentals-play/#recommendations-for-today","title":"Recommendations for Today","text":"Action Why It Matters Establish an internal AI Hosting Tier Policy Define approved hosting patterns (e.g., IL5 SaaS, enclave-only, hybrid), aligned to data classification and mission sensitivity. Use a decision support framework with each major AI integration decision Enables consistent evaluation of value, risk, effort, and trust\u2014helps justify model/tool selection to stakeholders. Adopt PromptOps practices Treat prompts and context injections like source code: version them, test them, audit them. Create an AI Model Bill of Materials (MLBOM) Track what models were used, when, where, and how\u2014critical for reproducibility and post-incident forensics. Align with Zero Trust and DoD AI Risk Frameworks Incorporate AI usage into existing DevSecOps, Zero Trust, and Supply Chain Risk Management strategies."},{"location":"plays/fundamentals-play/#next-best-play-code-generation-and-completion","title":"Next Best Play: Code Generation and Completion","text":"<p>This play focused on where GenAI capabilities live\u2014exploring hosting models, usage patterns, and architectural integration. The next play turns to how GenAI is reshaping one of the most immediate and visible SDLC tasks: writing code.</p> <p>Code generation and completion tools like Copilot, TabNine, and open-source agentic assistants are rapidly entering developer workflows. But adoption is often ahead of understanding.</p> <p>The next play will explore:</p> <ul> <li>Leading practices for integrating GenAI into coding workflows</li> <li>Human-in-the-loop patterns for safe and productive use</li> <li>Guardrails for quality, maintainability, and team alignment</li> <li>Prompting strategies, telemetry, and testing approaches</li> <li>Real-world lessons from DoD and commercial teams</li> </ul> <p>Whether your team is experimenting with AI-assisted code suggestions or looking to streamline scaffolding and unit test generation, this next play will help you answer: \u201cHow do we use GenAI for code responsibly, repeatably, and at scale?\u201d</p> <p>End of Play</p>"},{"location":"plays/human-machine-patterns/","title":"Human-Machine Interaction Patterns (HMT)","text":"<p>This reference defines common interaction models between humans and Generative AI tools across the software development lifecycle. These patterns shape team roles, trust boundaries, traceability, and governance posture.</p>"},{"location":"plays/human-machine-patterns/#1-introduction","title":"1. Introduction","text":"<p>Adopting GenAI is not just about choosing a model\u2014it's about defining how humans and machines work together. These patterns influence developer workflows, DevSecOps alignment, compliance posture, and trust calibration.</p>"},{"location":"plays/human-machine-patterns/#2-interaction-patterns-in-practice","title":"2. Interaction Patterns in Practice","text":"Pattern Description Benefits Challenges Standalone Web Interfaces Browser-based, disconnected from toolchains Easy to access No traceability, encourages out-of-band use IDE Plugins and Adapters Inline assistance in VSCode, JetBrains, etc. Familiar UX No prompt versioning, hard to share AI-First IDEs / Workspaces Purpose-built GenAI environments (e.g., WindSurf, OpenHands) Integrated agents Changes team dynamics Custom API Integrations Embedded model calls in codebases or pipelines High control Requires governance Agentic Platforms Autonomous agents handling multi-step logic Automates workflows Emergent behavior, trust risk"},{"location":"plays/human-machine-patterns/#3-key-design-insight","title":"3. Key Design Insight","text":"<p>\"Essentially, the human-in-the-loop approach reframes an automation problem as a Human-Computer Interaction (HCI) design problem...\" \u2014 Ge Wang, Stanford University</p>"},{"location":"plays/human-machine-patterns/#4-architectural-implications","title":"4. Architectural Implications","text":"<p>Each pattern affects: - Data flow boundaries - Prompt versioning and auditability - DevSecOps alignment - Calibrated trust across teams</p>"},{"location":"plays/human-machine-patterns/#5-reference","title":"5. Reference","text":"<p>This guidance was introduced in Play Fundamentals for Designing an AI-Augmented Tool Chain</p>"},{"location":"plays/ide-security/","title":"Integrated Development Environment (IDE) Security","text":"<p> In the DoD context where auditability and traceability are compliance requirements, we must consider AI-augmented tools running inside the IDE (like Copilot, Tabnine, or CodeWhisperer). In this situation, traceability becomes more difficult because the interaction is often ephemeral and undocumented unless explicit controls are in place. </p> <p> Here's how to address this gap architecturally and procedurally: </p>"},{"location":"plays/ide-security/#architectural-and-governance-strategies-for-prompt-traceability-in-ide-integrated-tools","title":"Architectural and Governance Strategies for Prompt Traceability in IDE-Integrated Tools","text":""},{"location":"plays/ide-security/#1-ide-plugin-instrumentation-client-side-logging","title":"1. IDE Plugin Instrumentation (Client-Side Logging)","text":"<ul> <li> <p>Require IDE plugins to log prompts and completions locally with metadata:</p> </li> <li> <p>Prompt text</p> </li> <li>File name and commit hash</li> <li>Timestamp</li> <li>User identity (e.g., Common Access Card-based auth)</li> <li>These logs should be persisted to a secure location or telemetry pipeline for centralized auditing.</li> <li>Ensure logs include linkage to relevant tickets (e.g., via Git branch naming conventions like <code>feature/JIRA-123</code>).</li> </ul>"},{"location":"plays/ide-security/#2-prompt-logging-middleware-or-agent","title":"2. Prompt Logging Middleware or Agent","text":"<ul> <li>Insert a middleware layer (e.g., using HTTP proxies or local agents) that captures interactions between IDE and LLM services.</li> <li>Middleware can enrich requests with SDLC context tags (e.g., project ID, ATO boundary, classification level) before forwarding.</li> </ul>"},{"location":"plays/ide-security/#3-model-driven-trace-embedding","title":"3. Model-Driven Trace Embedding","text":"<ul> <li>Encourage prompt templates that include traceability anchors directly in the prompt:</li> </ul> <p><code># Prompt for JIRA-4321 | Branch: feature/encrypt-endpoint   Generate Python code for encrypting form data using FIPS 140-2 compliant libraries.</code></p>"},{"location":"plays/ide-security/#4-integration-with-cicd-audit-pipelines","title":"4. Integration with CI/CD Audit Pipelines","text":"<ul> <li> <p>Ensure prompts (or artifacts derived from prompts) are linked in:</p> </li> <li> <p>Commit messages (<code>Generated with Prompt ID 78a2-LLM</code>)</p> </li> <li>PR descriptions or JIRA tickets</li> <li>Build metadata (<code>.promptlog</code> files)</li> <li>Treat prompts like you would test cases or security scan reports: curated, reviewed, and versioned.</li> </ul>"},{"location":"plays/ide-security/#5-mandate-use-of-observability-tools","title":"5. Mandate Use of Observability Tools","text":"<ul> <li> <p>Recommend or require use of prompt observability frameworks like:</p> </li> <li> <p>LangSmith</p> </li> <li>PromptLayer</li> <li>WandB</li> <li>These tools can track prompt lineage, performance, and outcomes across environments (including IDE).</li> </ul>"},{"location":"plays/ide-security/#dod-specific-recommendations","title":"DoD-Specific Recommendations","text":"Principle Guidance Zero Trust Treat IDE plugins as boundary components. Log and monitor accordingly. Traceability (DoD Ethics) Require attribution for any AI-assisted contribution. SP 800-53 Audit Control Implement <code>AU-2</code>, <code>AU-6</code>, <code>AU-12</code> for logging and reviewing prompt use. DoDI 5000.82 Treat prompt templates as acquisition artifacts or reviewable assets."},{"location":"plays/prompt-engineering-and-security-policy/","title":"Mapping Prompt Engineering to DoD Cyber and AI Governance","text":"<p>To reflect these key frameworks\u2014Zero Trust, DoD AI Ethics, NIST RMF, and DoDI 5000.82\u2014this guidance maps principles to prompting behaviors. Rather than just listing them, it operationalizes these frameworks for prompt authors, reviewers, and acquisition leads in the DoD.</p>"},{"location":"plays/prompt-engineering-and-security-policy/#zero-trust-architecture-dod-zt-strategy","title":"Zero Trust Architecture (DoD ZT Strategy)","text":"<p>Core Principle: Never trust, always verify. Application to Prompt Engineering:</p> Zero Trust Tenet Prompt Engineering Implication Assume breach Prompts must not leak mission, user, or credential information. Treat every prompt as potentially exposed. Least privilege Restrict model access by role. Ensure prompts don\u2019t exceed user\u2019s authorized data boundaries. Explicit verification Prompt results should not be trusted blindly. Require human review or automated validation against policy. Continuous monitoring All prompt usage and LLM interactions must be logged, auditable, and monitored for anomalies. <p>\u2705 Play Recommendation: Integrate prompt review gates in CI/CD pipelines and AI platform workflows that enforce access controls, classify inputs/outputs, and detect unsafe behavior.</p>"},{"location":"plays/prompt-engineering-and-security-policy/#dod-ai-ethical-principles","title":"DoD AI Ethical Principles","text":"<p>Core Principle: Responsible, equitable, and governable AI.</p> Ethical Pillar Prompt Engineering Implication Responsible Ensure prompts avoid hallucinations, overreliance, or opaque results. Outputs should be attributable and verifiable. Equitable Avoid prompts that reinforce bias (e.g., role stereotypes in generated content). Include fairness checks in review. Traceable Prompt inputs and model configurations must be versioned and reproducible. Reliable Design prompts to encourage safe defaults, testability, and structured outputs that reduce downstream fragility. Governable Include prompts in artifact traceability. Audit prompt history, usage context, and contributor identity. <p>\u2705 Play Recommendation: Use structured metadata around prompts and outputs (e.g., who wrote, when/why, what model was used) to enhance traceability and ethics oversight.</p>"},{"location":"plays/prompt-engineering-and-security-policy/#nist-risk-management-framework-sp-800-37-800-53","title":"NIST Risk Management Framework (SP 800-37, 800-53)","text":"<p>Core Principle: Security controls and lifecycle risk mitigation.</p> RMF Phase Prompt Engineering Mapping Prepare Include prompt engineering in system-level risk assessment and threat modeling. Categorize Prompts must be evaluated for potential impact (e.g., if they generate IaC or policy, categorize as moderate/high risk). Implement Apply security controls like logging, constrained output formats, and red-teaming during prompt implementation. Assess Periodically review prompt libraries and usage patterns for security and compliance gaps. Authorize Include prompt artifacts and model behavior evidence in ATO packages. Monitor Continuously track prompt-related risks: drift, policy violations, adversarial behavior. <p>\u2705 Play Recommendation: Treat prompts and prompt libraries as software configuration artifacts subject to security control baselines and risk assessments.</p>"},{"location":"plays/prompt-engineering-and-security-policy/#dodi-500082-acquisition-of-digital-capabilities","title":"DoDI 5000.82: Acquisition of Digital Capabilities","text":"<p>Core Principle: Software is continuously acquired, delivered, and evolved.</p> Acquisition Phase Prompt Engineering Tie-In Needs Definition Use structured prompts to analyze requirements, summarize JCIDS artifacts, and translate operational intent. Design &amp; Development Prompt engineering used to co-generate artifacts like user stories, architecture descriptions, test cases, IaC. Testing &amp; Evaluation Prompts guide automated test generation, requirements validation, and cyber red-teaming. Fielding Ensure prompts used to generate fieldable code or documentation are part of digital review/audit packages. Sustainment Prompt libraries should be versioned and sustainment teams trained in their secure use and evolution. <p>\u2705 Play Recommendation: Prompt engineering should be included as a discrete practice in software acquisition lifecycle documentation and vendor expectations.</p>"},{"location":"plays/prompt-engineering/","title":"Play: Prompt Engineering Play","text":""},{"location":"plays/prompt-engineering/#i-purpose-scope","title":"I. Purpose &amp; Scope","text":"<p>Objective: To establish foundational prompt engineering practices for DoD software teams, enabling secure, consistent, and auditable use of large language models (LLMs) in software development workflows.</p> <p>Primary Audience: Software developers, security analysts, and program managers who are integrating or evaluating LLM tools within their development processes.</p> <p>Secondary Audience: DevSecOps engineers, test leads, operations teams, and acquisition professionals using AI-assisted tools for documentation, compliance, or technical analysis.</p> <p>What This Playbook Covers: - Core principles and anatomy of effective prompts - Security and compliance considerations for DoD environments - Advanced prompting techniques and when to apply them - Role-specific examples across the software development lifecycle - Risk mitigation strategies and common pitfalls</p> <p>What This Playbook Does NOT Cover: - AI model deployment, fine-tuning, or infrastructure management - Organization-wide AI governance frameworks or policy development - Procurement guidance and vendor evaluation for AI platforms - Advanced topics like model training, embeddings, or vector databases</p> <p>Alignment: This guidance supports the Department's strategic objectives including Zero Trust Architecture, DoD AI Ethical Principles, and secure software development practices outlined in DoDI 5000.82.</p> <p>Why This Matters Now: DoD teams are rapidly adopting LLMs for code review, documentation generation, compliance assessment, and technical analysis. Without structured prompting practices, teams risk inconsistent outputs, security vulnerabilities, and compliance gaps. Early adoption of disciplined prompt engineering prevents these issues and establishes secure-by-design AI integration.</p> <p>Expected Outcomes: After applying this guidance, teams will be able to: - Write structured, repeatable prompts that produce consistent, mission-aligned outputs - Apply appropriate security controls and risk mitigation strategies - Integrate prompt engineering into existing SDLC processes and tooling - Avoid common prompting pitfalls that lead to hallucinations or compliance issues</p> <p>This playbook provides the foundational knowledge needed to use LLMs effectively and securely within DoD software development constraints. For advanced topics like model deployment or enterprise AI governance, refer to specialized guidance documents.</p> <p>\ud83d\udccc Note: This playbook covers prompt engineering across the entire software development lifecycle\u2014not just code generation. Effective prompting is equally important for documentation, security assessment, infrastructure configuration, and compliance activities.</p>"},{"location":"plays/prompt-engineering/#ii-introduction-to-prompt-engineering","title":"II. Introduction to Prompt Engineering","text":""},{"location":"plays/prompt-engineering/#what-is-prompt-engineering","title":"What Is Prompt Engineering?","text":"<p>In the context of Generative AI (GenAI), a prompt is text input in natural language provided to a large language model (LLM) to guide its behavior and generate a relevant, useful output. It is both the instruction and the context. This is the critical mechanism through which we shape model responses to support technical, analytical, and operational tasks.</p> <p>Prompt engineering is the practice of crafting those inputs with precision and intentionality. A well-structured prompt provides the LLM with clear instructions, sufficient context, defined constraints, and an expected format\u2014allowing it to produce more accurate, secure, and mission-aligned outputs.</p> <p>\u201cWithout clear and precise instructions, these models can produce irrelevant, biased, or overly general outputs.\u201d \u2014 IBM Prompt Engineering Guide <sup>1</sup></p> <p>In modern software value streams, prompt engineering is no longer an experimental skill\u2014it is an operational discipline. From writing IaC policies to summarizing acquisition artifacts, prompts are increasingly how we interact with AI systems to support the SDLC.</p> <p>There are three common structural approaches to prompting<sup>2</sup>:</p> <ul> <li>Direct Instructions \u2013 Specific, task-oriented commands that tell the model what to do (e.g., \u201cSummarize this technical report in under 150 words.\u201d). These are concise and do not assume domain context.</li> <li>Task-Specific Instructions \u2013 Tailored prompts aligned to domain processes or roles (e.g., \u201cConvert this JCIDS Capability Document into MoSCoW-prioritized user stories.\u201d). These assume deeper context and are ideal for repeatable mission or enterprise tasks.</li> <li>Open-Ended Instructions \u2013 Used for ideation or exploratory analysis (e.g., \u201cList risks associated with deploying LLMs in IL5 environments.\u201d). These prompts leave room for the model to explore possible directions.</li> </ul> <p>\ud83d\udccc Clarification: Task-specific prompts differ from direct instructions in that they assume and embed domain context. A direct instruction tells the model what to do in a general sense; a task-specific instruction frames the request in terms of an existing process, role, or structured artifact (e.g., acquisition workflow, STIG review, mission thread).</p> <p>As with any engineering discipline, effectiveness improves with structure, governance, and iteration. This playbook provides the foundational techniques and patterns to apply prompt engineering with confidence across DoD software value streams.</p>"},{"location":"plays/prompt-engineering/#why-it-matters-in-the-sdlc","title":"Why It Matters in the SDLC","text":"<p>Prompt engineering is crucial for the software development lifecycle (SDLC).  It directly impacts the quality, reliability, and efficiency of Generative AI functionality.  Well defined prompts enable AI models to understand user intent and produce the desired outcomes. This becomes particularly important as AI-augmentation is integrated into more stages of the SDLC, from requirements through deployment. </p> <p>Benefits to the AI-Augmented SDLC:</p> <ul> <li>Reduces ambiguity - Effective prompt engineering eliminates confusion between the human intent and the AI interpretation. This minimizes rework.</li> <li>Accelerates documentation - Concise prompt engineering streamlines the generation of documentation saving time and improving consistency. They can be used to quickly summarize technical documents as well.</li> <li>Codifies expertise - Prompt engineering captures domain knowledge and leading practices in a reusable format. This can help less experience team members to learn from an leverage collective knowledge from experienced team members. It also builds a reusable library of prompts for future projects.</li> <li>Improves human/machine teaming - The process of prompting and the assets created act as a shared language between the humans and the language models (the machine.) The iterative nature of prompt refinement and continuous feedback can make the workflow between humans and machines more intuitive.  </li> </ul>"},{"location":"plays/prompt-engineering/#security-policy-considerations","title":"Security &amp; Policy Considerations","text":"<p>Prompt engineering, like all use of AI-Augmentation,  must be approached as a security-sensitive activity. In the DoD software environment, careless prompting could lead to unauthorized disclosures, model misuse, or generation of insecure artifacts. This section outlines key considerations to protect mission integrity, enforce compliance, and align with DoD cybersecurity strategy.</p>"},{"location":"plays/prompt-engineering/#responsible-use-of-commercial-models","title":"Responsible Use of Commercial Models","text":"<p>A primary concern is preventing data leakage through commercial AI models that may train on government inputs. DoD organizations must establish contractual guarantees with vendors that government data remains isolated from training processes, commercial analysis.  In some instances, it may require implementation of  air-gapped solutions for classified environments.</p> <p>It is an important step to carefully reviewing End User License Agreements (EULAs) and establish contractual controls that restrict commercial AI models from retaining government data and from training models based on inputs.  You should also understand any retention and secondary use.</p>"},{"location":"plays/prompt-engineering/#leverage-approved-platforms","title":"Leverage Approved Platforms","text":"<p>Use government-authorized LLM deployments that meet IL-level hosting and accreditation requirements. Prompts should be tested and deployed on platforms that align with DoD security and compliance standards, especially when used in mission, weapon, or enterprise environments. </p> <p>==Do we have a list of approved platforms and LLMs? for DoD --&gt; Check with Navy and HPC==</p>"},{"location":"plays/prompt-engineering/#data-protection-and-access-controls","title":"Data Protection and Access Controls","text":"<p>To prevent leakage of sensitive or classified information,  prompts must never include Controlled Unclassified Information (CUI), Personally Identifiable Information (PII), or mission-sensitive details unless processed within appropriately cleared and accredited environments.  It is also imperative to respect classification boundaries, that is, do not rely on model outputs to infer or verify classification status.  Always confirm user access controls are enforced.  </p> <p>Individuals also have a responsibility to honor  access controls and classification of data and documents</p>"},{"location":"plays/prompt-engineering/#auditability-and-traceability","title":"Auditability and Traceability","text":"<p>Audit logs play an important role when leveraging AU-augmentation. It is important to log both inputs and outputs and include data such as userID and model version.  Remember that a prompt is an electronic asset.  Prompts used across the SDLC should support traceability and be linkable to tickets, commits, documents, or delivery artifacts.  </p> <p>In some instances, you may be using and integrated development environment (IDE) which has AI-augmentation configured, installed or available withing the IDE. The IDE is the workbench tool where software engineers and developers create/generate software code, automated tests, and documentation. When operating within a local IDE, consider the following:</p> <ul> <li>Structuring code comments that include prompt references</li> <li>Configure tools to automatically include prompt metadata in commit messages / notes </li> <li>Establish centralized prompt repositories</li> </ul> <p>Additional IDE related guidance available for applying security principles when using in-IDE AI augmentation here: IDE Security.</p>"},{"location":"plays/prompt-engineering/#adversarial-prompting-and-red-teaming","title":"Adversarial prompting and red-teaming","text":"<p>Adversarial prompting refers to deliberately crafted inputs designed to bypass safeguards, extract sensitive information, or manipulate outputs in unintended ways or even induce unsafe behavior from AI systems.</p> <p>In the DoD context, the technique called Red-teaming applies systematic testing methodologies to identify these vulnerabilities and simulate misuse as well as test the robustness of prompt guards that may be in place.  Security teams security teams attempt to exploit AI systems, and in the case of the SDLC, exploit Ai-augmented tooling through carefully constructed prompts that circumvent built-in protections. Regular red-teaming helps validate that prompt engineering practices and AI tools withstand manipulation and remain compliant with security policies.</p> <p>In addition to defending against attacks, proactive security measures in prompt design are equally important:</p> <ul> <li>Incorporate prompt red-teaming. Test prompts for potential leakage, injection, or manipulation by adversaries.</li> <li>Embed security-focused prefixes. According to Kim et al. [^3], adding a security-focused prompt prefix reduced generated vulnerabilities by up to 56% in benchmark tasks.</li> <li>Avoid anti-patterns. Prompts that encourage shortcutting authentication, bypassing validation, or using deprecated cryptographic libraries must be flagged and rejected.</li> </ul> <p>Detailed guidance on designing prompts is covered under Section III. How to Write an Effective Prompt below.</p> <p>Prompt engineering is foundational to secure and privacy-preserving AI integration in the SDLC. By embedding security and privacy considerations directly into prompt design and workflow, organizations can reduce ambiguity, accelerate documentation, codify expertise, and improve human-machine teaming\u2014all while maintaining strict compliance with DoD standards and safeguarding sensitive information</p> <p>These key frameworks\u2014Zero Trust, DoD AI Ethics, NIST RMF, and DoDI 5000.82\u2014must also be considered during prompt engineering. See Mapping Prompt Engineering to DoD Cyber and AI Governance</p> <p>Advanced techniques can include incorporating encryption into prompt design and designing prompting workflows that are secure by default.Prompt engineering is a secure-by-design behavior. Embedding secure-by-design behavior into the fabric of software value streams verifies that the benefits of LLMs are realized without introducing new operational or cybersecurity risks.</p> <p>[^3] H. Kim, L. Wang, T. Kim, and S. Lee, \u201cBenchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models,\u201d in Proceedings of the 2nd International Conference on Foundations of Responsible Generative AI (FORGE 2025), 2025. [Online]. Available: https://conf.researchr.org/details/forge-2025/forge-2025-papers/11/Benchmarking-Prompt-Engineering-Techniques-for-Secure-Code-Generation-with-GPT-Models. [Accessed: 20-Jun-2025]</p> <p>\ud83d\udccc Prompts shape model behavior. Better prompts lead to better outcomes\u2014securely, repeatably, and transparently.</p>"},{"location":"plays/prompt-engineering/#iii-how-to-write-an-effective-prompt","title":"III. How to Write an Effective Prompt","text":"<p>Now that we understand the importance of prompting and the risks involved, let\u2019s get specific. This section outlines how to author high-quality prompts that drive mission alignment, increase explainability, and ensure reuse.</p> <p>Prompt engineering is the practice of crafting precise, structured inputs that guide large language models (LLMs) to generate reliable, relevant, and secure outputs. In DoD environments, prompts must be treated as governed artifacts\u2014subject to review, refinement, and reuse. A well-written prompt improves traceability, reduces risk, and ensures that generated artifacts support mission assurance.</p>"},{"location":"plays/prompt-engineering/#prompt-engineering-principles-for-the-mission-focused-sdlc","title":"Prompt Engineering Principles for the Mission-Focused SDLC","text":"<p>Principles represent conceptual guidelines and create a framework for thinking about a topic like prompt engineering.  Principles enable practitioners to adapt their approach based on context and objectives while maintaining consistency with underlying quality factors. Stated differently, principles are the rules of engagements.  Let's start with a core set of prompt engineering principles:</p> <ul> <li>Be Specific and Complete \u2013 Provide precise instructions and sufficient context to reduce ambiguity and improve outcome quality.</li> <li>Define Role and Intent \u2013 Establish the AI\u2019s role, expertise, and purpose to align responses with mission needs.</li> <li>Clear Objectives and Boundaries \u2013 Define the desired outcome and set explicit constraints for format, tone, and scope. When the goal is exploration or ideation, use open-ended prompts to broaden insight. When precision and compliance are critical, use tightly scoped directives.</li> <li>Tailor to the Audience \u2013 Adjust technical depth, language, and tone based on whether the output is for developers, analysts, operators, or decision-makers.</li> <li>Use Structured Context and Output \u2013 Organize inputs and expected outputs using lists, delimiters, or structured formats like JSON/YAML to guide response shape.</li> <li>Refine Through Iteration and Versioning \u2013 Continuously improve prompt effectiveness with testing and maintain version control for traceability and reproducibility.</li> <li>Treat Prompts as Artifacts \u2013 Manage prompts and output as part of the SDLC: link to issues, commits, and documentation to support Zero Trust and compliance.</li> </ul>"},{"location":"plays/prompt-engineering/#anatomy-of-a-high-quality-prompt","title":"Anatomy of a High-Quality Prompt","text":"<p>A well-engineered prompt is more than a question\u2014it\u2019s a structured input that acts like a design artifact. In the DoD context, where prompts may influence everything from Infrastructure-as-Code (IaC) to CONOPS summaries or cyber policy generation, clarity, control, and context are essential. The components below can be combined to build consistently effective prompts for mission-focused outcomes.</p> <p>\ud83d\udccc Tip: Think of prompts like test cases\u2014they must be understandable, reproducible, and reviewable. They are design artifacts, not one-off conversations.</p>"},{"location":"plays/prompt-engineering/#1-role-definition","title":"1. Role Definition","text":"<p>What to do: Assign the AI a role to establish its point of view and domain expertise.</p> <p>Why it matters: This frames the model\u2019s tone, terminology, and assumptions. In mission systems, role identity helps align AI reasoning with the expectations of security engineers, test leads, PMs, or policy authors.</p> <p>Prompt Pattern: <code>You are a [specific role] with expertise in [domain]. Your primary responsibility is [function].</code></p> <p>Example: <code>You are a cybersecurity analyst trained on NIST 800-53 and DoD STIGs.</code></p>"},{"location":"plays/prompt-engineering/#2-define-the-goal","title":"2. Define the Goal","text":"<p>What to do: State the specific task using action verbs. Describe the intended outcome and, when helpful, include context or dependencies.</p> <p>Why it matters: Action-oriented prompts reduce ambiguity and focus the model on a mission-aligned outcome.</p> <p>Prompt Pattern: <code>Your task is to [action] that supports [objective].</code></p> <p>Example: <code>Evaluate this infrastructure-as-code file for security misconfigurations within a cloud-native container platform.</code></p>"},{"location":"plays/prompt-engineering/#3-specify-the-audience","title":"3. Specify the Audience","text":"<p>What to do: Tailor the language, technical depth, and tone based on the expected consumer of the output.</p> <p>Why it matters: Models adapt output style and complexity better when the audience is clear\u2014this avoids wasted cycles or confusing jargon.</p> <p>Prompt Pattern: <code>This output is intended for [audience].</code></p> <p>Example: <code>Prepare this response for a cross-functional engineering team familiar with DevSecOps practices.</code></p>"},{"location":"plays/prompt-engineering/#4-provide-context-and-input","title":"4. Provide Context and Input","text":"<p>What to do: Describe the scenario, system, or constraints and define the input structure.</p> <p>Why it matters: Background enables better relevance and accuracy. Precise input formats prevent model confusion and increase reliability.</p> <p>Prompt Pattern: <code>Given [situation/environment], process the following input [format]...</code></p> <p>Example: <code>This operates within an IL5 Kubernetes environment. The following input is a YAML deployment manifest.</code></p>"},{"location":"plays/prompt-engineering/#5-set-constraints-style-and-compliance","title":"5. Set Constraints, Style, and Compliance","text":"<p>What to do: Define policies, formatting rules, or prohibited content.</p> <p>Why it matters: Explicit constraints help reduce hallucination, keep outputs mission-safe, and enforce internal quality standards.</p> <p>Prompt Pattern: <code>Do not [restrictions]. Limit to [format/style/length].</code></p> <p>Example: <code>Do not reference public GitHub repos. Limit response to 300 words. Use only DoD-approved terminology.</code></p>"},{"location":"plays/prompt-engineering/#6-format-the-result","title":"6. Format the Result","text":"<p>What to do: Request specific formats such as JSON, checklist, or markdown.</p> <p>Why it matters: Structured output enables immediate use, especially when integrated into tooling or compliance workflows. Output formatting should reflect the audience\u2019s needs\u2014for example, structured formats for engineers, or narrative summaries for policy leads.</p> <p>Prompt Pattern: <code>Return the results as [format] with [fields].</code></p> <p>Example: <code>Return the results as a markdown checklist with inline annotations.</code></p>"},{"location":"plays/prompt-engineering/#7-optional-reinforce-with-examples-and-delimiters","title":"7. (Optional) Reinforce with Examples and Delimiters","text":"<p>What to do: Add illustrative examples or delimiter scaffolds.</p> <p>Why it matters: Examples improve clarity and help models generalize the pattern. Delimiters improve parseability and prompt reuse.</p> <p>Prompt Pattern: </p> <pre><code>### INPUT ###\n[structured example input]\n\n### OUTPUT ###\n[desired structured output]\n</code></pre> <p>Example:</p> <pre><code>### INPUT ###\n[Insert Terraform configuration]\n\n### OUTPUT ###\n- Detected STIG violations\n- Remediation steps\n</code></pre>"},{"location":"plays/prompt-engineering/#assembly-pattern","title":"Assembly Pattern","text":"<p>You don\u2019t need every element every time\u2014but following this pattern improves reliability, security, and reproducibility.</p> <pre><code>[Role Definition]  \nYou are a [specific role] with expertise in [domain]. Your primary responsibility is [function].\n\n[Define the Goal]  \nYour task is to [action] that supports [objective].\n\n[Specify the Audience]  \nThis output is intended for [audience].\n\n[Provide Context and Input]  \nGiven [situation/environment], process the following input [format]...\n\n[Set Constraints, Style, and Compliance]  \nDo not [restrictions]. Limit to [format/style/length].\n\n[Format the Result]  \nReturn the results as [format] with [fields].\n\n[Optional: Examples and Delimiters]  \n### INPUT ###  \n[structured example input]  \n### OUTPUT ###  \n[desired structured output]\n</code></pre> <p>This pattern enables repeatable, compliant prompting across use cases\u2014whether you're working on acquisition strategies, system modeling, or defensive cybersecurity tools.</p> <p>For readers applying prompt engineering to generate or complete code, refer to the dedicated Play: Leading Practices for Code Completion and Generation. It contains additional role-specific guidance, architectural guardrails, and usage patterns for secure software delivery in DoD environments.</p>"},{"location":"plays/prompt-engineering/#iv-advanced-prompting-techniques","title":"IV. Advanced Prompting Techniques","text":"<p>Advanced prompting techniques enhance the 7-component anatomy for specific cognitive tasks and complex scenarios. These patterns can be combined and modified to address sophisticated requirements in DoD environments.</p>"},{"location":"plays/prompt-engineering/#core-cognitive-patterns","title":"\u2699\ufe0f Core Cognitive Patterns","text":""},{"location":"plays/prompt-engineering/#1-chain-of-thought-cot-prompting","title":"1. Chain-of-Thought (CoT) Prompting","text":"<p>A reasoning-based pattern that encourages the AI to \"think out loud\" through step-by-step logic. This increases transparency in complex problem-solving and mitigates hallucinations by making the reasoning process visible.</p> <ul> <li>Use case: Risk analysis, defect triage, threat modeling</li> <li>Example: <code>Walk through the logic of how a buffer overflow could occur in this C code.</code></li> </ul>"},{"location":"plays/prompt-engineering/#2-tree-of-thought-tot-prompting","title":"2. Tree-of-Thought (ToT) Prompting","text":"<p>An expansion of CoT that branches into multiple reasoning paths, enabling the AI to evaluate tradeoffs or alternatives. Particularly useful for decision analysis where multiple criteria must be balanced.</p> <ul> <li>Use case: Architectural decision analysis, requirements negotiation</li> <li>Example: <code>Generate 3 architectural options for IL5 Kubernetes deployment, then evaluate based on cost, security, and maintainability.</code></li> </ul>"},{"location":"plays/prompt-engineering/#3-self-critique-self-refinement-prompting","title":"3. Self-Critique / Self-Refinement Prompting","text":"<p>This technique asks the AI to review and improve its own outputs, enabling higher-quality generation, especially in technical or compliance-sensitive contexts.</p> <ul> <li>Use case: ATO packages, quality assurance</li> <li>Example: <code>Review the following Bash script for STIG compliance and suggest improvements based on DoD IA controls.</code></li> </ul>"},{"location":"plays/prompt-engineering/#structured-context-patterns","title":"\ud83c\udfd7\ufe0f Structured Context Patterns","text":""},{"location":"plays/prompt-engineering/#4-chop-pattern-context-history-objective-prompt","title":"4. CHOP Pattern (Context, History, Objective, Prompt)","text":"<p>A structured prompting technique that explicitly defines the task context, relevant history, the user's objective, and the core prompt. This structure improves clarity and reduces ambiguity in outputs, especially for complex or multi-stage requests.</p> <ul> <li>Use case: Mission thread generation, architectural analysis</li> <li>Example:   <code>CONTEXT: A Flask-based DoD logistics API   HISTORY: Uses JWT authentication, FIPS 140-3 encryption required   OBJECTIVE: Add a new `/request/submit` endpoint that logs all access attempts   PROMPT: Generate a secure Flask route that uses role-based access control and logs each submission in JSON.</code></li> </ul>"},{"location":"plays/prompt-engineering/#5-context-windowing-embedded-context-prompting","title":"5. Context Windowing / Embedded Context Prompting","text":"<p>Embedding supporting documents or code within the prompt, usually framed by delimiters. This keeps context tightly scoped and prevents the model from straying from the intended source material.</p> <ul> <li>Use case: Policy Q&amp;A, source code interpretation</li> <li>Example:   <code>### DoD Policy Extract ###   [Policy content here]   ###   Based on this, what documentation is needed for RMF Step 4?</code></li> </ul>"},{"location":"plays/prompt-engineering/#6-rag-enhanced-prompting-retrieval-augmented-generation","title":"6. RAG-Enhanced Prompting (Retrieval-Augmented Generation)","text":"<p>A pattern that combines LLM reasoning with external knowledge retrieved from a vector database or document index. Ideal for high-assurance, mission-critical use where answers must be grounded in validated sources.</p> <ul> <li>Use case: Classified references, MBSE model interaction</li> <li>Example: <code>Using the mission thread stored in this vector index, summarize constraints affecting EW operations.</code></li> </ul>"},{"location":"plays/prompt-engineering/#workflow-and-format-patterns","title":"\ud83d\udd17 Workflow and Format Patterns","text":""},{"location":"plays/prompt-engineering/#7-prompt-chaining-pipeline-prompting","title":"7. Prompt Chaining / Pipeline Prompting","text":"<p>A technique where multiple linked prompts handle sequential tasks (e.g., requirements \u2192 user stories \u2192 tests). Chaining improves traceability, modularity, and reuse across SDLC phases.</p> <ul> <li>Use case: Requirements-to-code-to-test transformation</li> <li>Example:</li> <li>Extract features from transcript</li> <li>Generate user stories</li> <li>Generate unit tests for each story</li> </ul>"},{"location":"plays/prompt-engineering/#8-zero-shot-few-shot-prompting","title":"8. Zero-shot / Few-shot Prompting","text":"<p>These techniques involve prompting the model with no (zero-shot) or minimal (few-shot) examples. Few-shot methods are helpful when the task format needs to be inferred through pattern recognition.</p> <ul> <li>Use case: Test case generation, bug triage, regex generation</li> <li>Example (Few-shot):   <code>Input: '123-45-6789' \u2192 Output: SSN   Input: 'john.doe@af.mil' \u2192 Output: Email   Input: '10.10.10.1' \u2192 Output: ?</code></li> </ul>"},{"location":"plays/prompt-engineering/#9-schemaformat-constrained-prompting","title":"9. Schema/Format-Constrained Prompting","text":"<p>A pattern that explicitly requests outputs in a structured format, such as tables, JSON, or YAML. Useful for downstream parsing or integrations with dashboards and pipelines.</p> <ul> <li>Use case: Integrations, dashboards, output parsing</li> <li>Example: <code>Output a table in Markdown with columns: Risk ID, Severity, Mitigation.</code></li> </ul>"},{"location":"plays/prompt-engineering/#cross-cutting-enhancement-techniques","title":"\ud83d\udd00 Cross-Cutting Enhancement Techniques","text":""},{"location":"plays/prompt-engineering/#10-role-based-prompt-pattern-rbpp-a-cross-cutting-modifier","title":"**10. Role-Based Prompt Pattern (RBPP): A Cross-Cutting Modifier **","text":"<p>RBPP is not a standalone prompt pattern. It\u2019s a powerful modifier that enhances other techniques. By assigning the model a specific mission-relevant role, it shapes tone, domain language, compliance expectations, and the scope of acceptable reasoning.</p> <ul> <li>Use case: Adding domain expertise to any prompting pattern</li> <li>Enhancement example: </li> <li>Basic CoT: <code>Walk through how this vulnerability occurs</code></li> <li>Role-Enhanced CoT: <code>Act as a DoD cybersecurity analyst and walk through how this vulnerability could be exploited in an IL5 environment</code></li> </ul> <p>Role-based enhancement works with all patterns. It can be combined with CoT for expert reasoning, CHOP for domain-specific context, or RAG for authoritative role-based retrieval. Think of it as a multiplier that adds domain credibility and context to any cognitive pattern.</p>"},{"location":"plays/prompt-engineering/#pattern-selection-guidance","title":"Pattern Selection Guidance","text":"When you need... Use this pattern Consider enhancing with Step-by-step reasoning Chain-of-Thought (CoT) Role-based enhancement Multiple solution evaluation Tree-of-Thought (ToT) Schema formatting Complex background context CHOP or Context Windowing Role-based enhancement Authoritative source grounding RAG-Enhanced Format constraints Multi-stage workflows Prompt Chaining Self-critique validation Consistent output format Schema/Format-Constrained Role-based enhancement Quality improvement Self-Critique Any base pattern"},{"location":"plays/prompt-engineering/#v-common-pitfalls-and-anti-patterns","title":"V. Common Pitfalls and Anti-Patterns","text":"<p>Understanding what not to do is as important as following best practices. These anti-patterns frequently undermine prompt effectiveness in DoD environments, leading to inconsistent outputs, security risks, or compliance failures.</p>"},{"location":"plays/prompt-engineering/#1-vague-role-definition","title":"1. Vague Role Definition","text":"<ul> <li>Problem: Using generic expertise without domain specificity  </li> <li>Anti-pattern: \"Act as an expert and review this code\"  </li> <li>Why it fails: The AI has no context for what type of expertise or standards to apply  </li> <li>Better approach: <code>\"Act as a DoD cybersecurity analyst certified in NIST 800-53 and familiar with DISA STIGs\"</code></li> </ul>"},{"location":"plays/prompt-engineering/#2-missing-audience-context","title":"2. Missing Audience Context","text":"<ul> <li>Problem: Producing outputs without considering the consumer  </li> <li>Anti-pattern: Technical implementation details for executive briefings  </li> <li>Why it fails: Misaligned complexity and terminology waste stakeholder time  </li> <li>Better approach: <code>\"Format this for senior leadership (GS-15/SES) requiring executive summary with risk implications\"</code></li> </ul>"},{"location":"plays/prompt-engineering/#3-unrestricted-output-scope","title":"3. Unrestricted Output Scope","text":"<ul> <li>Problem: Open-ended prompts that encourage hallucination  </li> <li>Anti-pattern: \"Tell me about security vulnerabilities\"  </li> <li>Why it fails: Leads to generic, potentially inaccurate information without actionable specifics  </li> <li>Better approach: <code>\"Identify OWASP Top 10 vulnerabilities in this Node.js application with specific remediation steps\"</code></li> </ul>"},{"location":"plays/prompt-engineering/#4-ignoring-output-format-requirements","title":"4. Ignoring Output Format Requirements","text":"<ul> <li>Problem: Requesting analysis without specifying structure  </li> <li>Anti-pattern: \"Analyze this system for compliance issues\"  </li> <li>Why it fails: Produces narrative text that's difficult to integrate into reports or track  </li> <li>Better approach: <code>\"Return compliance findings as a table with Control ID, Status, Risk Level, and Remediation Steps\"</code></li> </ul>"},{"location":"plays/prompt-engineering/#5-mixing-classification-levels","title":"5. Mixing Classification Levels","text":"<ul> <li>Problem: Including sensitive context in prompts for commercial models  </li> <li>Anti-pattern: Referencing specific mission names, locations, or system details  </li> <li>Why it fails: Potential unauthorized disclosure and policy violations  </li> <li>Better approach: <code>Use sanitized examples or process within appropriate IL-level environments</code></li> </ul>"},{"location":"plays/prompt-engineering/#6-no-constraints-or-guardrails","title":"6. No Constraints or Guardrails","text":"<ul> <li>Problem: Failing to set boundaries on AI behavior  </li> <li>Anti-pattern: Allowing suggestions of unapproved tools or non-compliant practices  </li> <li>Why it fails: Outputs may recommend solutions outside DoD policy or procurement constraints  </li> <li>Better approach: <code>\"Use only DISA-approved tools and reference FedRAMP-authorized solutions\"</code></li> </ul>"},{"location":"plays/prompt-engineering/#7-single-shot-complex-tasks","title":"7. Single-Shot Complex Tasks","text":"<ul> <li>Problem: Asking for multiple complex deliverables in one prompt  </li> <li>Anti-pattern: \"Generate requirements, design the architecture, and create test plans\"  </li> <li>Why it fails: Reduces quality of each deliverable and makes validation difficult  </li> <li>Better approach: <code>Use prompt chaining for sequential tasks with validation points between steps</code></li> </ul>"},{"location":"plays/prompt-engineering/#8-assuming-model-knowledge-currency","title":"8. Assuming Model Knowledge Currency","text":"<ul> <li>Problem: Expecting AI to know the latest policies or technical updates  </li> <li>Anti-pattern: \"Apply the latest CMMC requirements\" without providing current guidance  </li> <li>Why it fails: Models may reference outdated information or hallucinate recent changes  </li> <li>Better approach: <code>Provide current policy extracts or explicit version references</code></li> </ul>"},{"location":"plays/prompt-engineering/#risk-mitigation-checklist","title":"Risk Mitigation Checklist","text":"<p>Before deploying prompts in production workflows, verify: - [ ] Role and domain expertise clearly defined - [ ] Audience and output format specified - [ ] Appropriate constraints and compliance boundaries set - [ ] No sensitive information included in prompt text - [ ] Expected output scope is bounded and testable - [ ] Authoritative sources referenced when needed - [ ] Prompt is versioned and traceable to requirements</p>"},{"location":"plays/prompt-engineering/#vi-applying-the-framework-by-role","title":"VI. Applying the Framework by Role","text":"<p>This introduces role-specific guidance for designing effective prompts across the software development lifecycle (SDLC). Each role brings unique mission objectives, technical touchpoints, and trust boundaries. Each example follows this structure:</p> <ul> <li>Role \u2013 The SDLC stakeholder (e.g., Developer, Security Analyst) whose responsibilities shape the prompt\u2019s tone, detail, and purpose.  </li> <li> <p>Use Case \u2013 A real task or mission scenario where prompting improves speed, quality, or compliance.  </p> </li> <li> <p>Leading Practice \u2013 A proven prompting technique that enables structured, secure, and reusable outputs.</p> </li> <li> <p>Risk Mitigation \u2013 Guardrails to reduce hallucination, data leakage, noncompliance, or deviation from standards.</p> </li> </ul> <p>This structure moves prompting from ad hoc experimentation toward structured, repeatable design..</p>"},{"location":"plays/prompt-engineering/#examples-at-a-glance","title":"Examples \"At-a-Glance\"","text":"<p>This is a quick summary of the five role-based examples that will be detailed subsequently.</p> Role Use Case Key Emphasis Areas A - Capability Sponsor / Requirements Owner Drafting capability statements aligned with JP 3-0 and IL5 hosting constraints Doctrine, Goal Clarity, Compliance Language, Audience B - Developer IaC security review with Terraform Role, Task, Format, Compliance C - Security Analyst Log anomaly detection for IL5 environments Constraints, Format, Examples D - Project Manager Executive-ready AI-generated SITREP or progress update Audience, Goal, Output Control E - Operations Engineer Diagnosing runtime issues in containerized workloads Context, Role, Task, Structured Input/Output <p>\ud83d\udccc Note: Each example will include all 7 prompt components of the prompt framework.  All components are not always necessary.  These are intended to provide holistic examples.</p>"},{"location":"plays/prompt-engineering/#a-capability-sponsor-requirements-owner-sample-prompt","title":"A. Capability Sponsor / Requirements Owner Sample Prompt","text":"<ul> <li>Role \u2013 Capability Sponsor / Requirements Owner  </li> <li>Use Case \u2013 Operational Capability Statement Generation \u2013 Drafting requirements aligned to doctrinal sources like JP 3-0, incorporating mission-specific constraints such as IL5 compliance  </li> <li>Leading Practice \u2013 Use doctrinal vocabulary and outcome-focused tasking to frame capabilities in mission-aligned language  </li> <li>Risk Mitigation \u2013 Ground outputs in authoritative sources to reduce doctrinal drift and clarify dependencies or environmental assumptions  </li> </ul>"},{"location":"plays/prompt-engineering/#1-role-definition_1","title":"1. Role Definition","text":"<pre>\nYou are a capability sponsor responsible for articulating mission-aligned requirements and translating operational needs into formal capability statements. You are fluent in joint doctrine, including JP 3-0 and CJCSI 3170.01I, and understand the technical and hosting constraints of DoD IL5 cloud environments.\n</pre>"},{"location":"plays/prompt-engineering/#2-define-the-goal_1","title":"2. Define the Goal","text":"<pre>\nDraft a capability requirement aligned with joint operational doctrine that describes the mission objective and includes relevant constraints such as IL5 hosting. Incorporate considerations for availability, resilience, and integration with existing mission systems.\n</pre>"},{"location":"plays/prompt-engineering/#3-specify-the-audience_1","title":"3. Specify the Audience","text":"<pre>\nThis requirement will be reviewed by both acquisition strategy leads and system architects responsible for downstream technical decomposition and risk evaluation.\n</pre>"},{"location":"plays/prompt-engineering/#4-provide-context-and-input_1","title":"4. Provide Context and Input","text":"<pre>\nThe sponsoring command has identified a capability gap in rapid, theater-wide coordination of logistics assets across Joint and Coalition partners. The goal is to define a cloud-native decision support capability that operates in disconnected, intermittent, and limited (DIL) connectivity conditions and complies with IL5 hosting and data handling constraints.\n\n### INPUT ###\n[Insert operational context or mission scenario]\n### OUTPUT ###\n</pre>"},{"location":"plays/prompt-engineering/#5-set-constraints-and-compliance","title":"5. Set Constraints and Compliance","text":"<pre>\n- Reference only authoritative sources such as JP 3-0 and CJCSI 5123.01\n- Use plain language with embedded doctrinal terms where appropriate\n- Do not suggest acquisition solutions or specific vendors\n- Ensure compliance language aligns with DoD IL5 security requirements\n</pre>"},{"location":"plays/prompt-engineering/#6-format-the-result_1","title":"6. Format the Result","text":"<pre>\nReturn the requirement as a structured narrative with:\n- Capability Title\n- Operational Need Statement\n- Capability Description\n- Mission Threads or Use Scenarios\n- Known Constraints and Assumptions\n</pre>"},{"location":"plays/prompt-engineering/#7-reinforce-with-examples-and-delimiters","title":"7. Reinforce with Examples and Delimiters","text":"<pre>\n### EXAMPLE OUTPUT ###\n**Capability Title:** Joint Logistics Situational Awareness (JLSA)  \n**Operational Need:** Enable synchronized, real-time tracking and planning of Joint logistics operations across domains in contested environments  \n**Capability Description:** A cloud-native platform that aggregates multi-domain logistics data and enables decision-makers to plan, reallocate, and prioritize sustainment under IL5 constraints  \n**Constraints:** Must operate in DIL environments, support IL5-compliant hosting, and integrate with GCCS-J and Global Combat Support System (GCSS) APIs\n</pre>"},{"location":"plays/prompt-engineering/#b-developer-sample-prompt-infrastructure-as-code-security-review","title":"B. Developer Sample Prompt: Infrastructure-as-Code Security Review","text":"<ul> <li>Role \u2013 Developer  </li> <li>Use Case \u2013 Code Security Review \u2013 Evaluating Terraform configuration for compliance with DoD cloud security standards  </li> <li>Leading Practice \u2013 Multi-stage prompting with specific domain expertise  </li> <li>Risk Mitigation \u2013 Prevent hallucination of security standards and ensure alignment with compliance requirements  </li> </ul>"},{"location":"plays/prompt-engineering/#1-role-definition_2","title":"1. Role Definition","text":"<pre>\nYou are a senior DevSecOps engineer with expertise in NIST 800-53 controls, DoD STIGs, and cloud security best practices. Your primary responsibility is identifying security misconfigurations in infrastructure-as-code that could create compliance violations or operational vulnerabilities.\n</pre>"},{"location":"plays/prompt-engineering/#2-define-the-goal_2","title":"2. Define the Goal","text":"<pre>\nAnalyze the provided Terraform configuration for security misconfigurations, policy violations, and STIG compliance issues. Identify specific violations and provide actionable remediation steps.\n</pre>"},{"location":"plays/prompt-engineering/#3-specify-the-audience_2","title":"3. Specify the Audience","text":"<pre>\nThis analysis is intended for a development team with intermediate Terraform experience working on IL4/IL5 systems requiring FedRAMP compliance.\n</pre>"},{"location":"plays/prompt-engineering/#4-provide-context-and-input_2","title":"4. Provide Context and Input","text":"<pre>\nThis Terraform configuration deploys a microservices architecture to AWS GovCloud for a mission-critical application processing CUI data. The environment must comply with NIST 800-53 moderate baseline and relevant DoD STIGs.\n\n### INPUT ###\n[Insert Terraform configuration file]\n### OUTPUT ###\n</pre>"},{"location":"plays/prompt-engineering/#5-set-constraints-and-compliance_1","title":"5. Set Constraints and Compliance","text":"<pre>\n- Reference only authoritative sources (NIST 800-53, DoD STIGs, FedRAMP baselines)\n- Do not suggest commercial tools without explicit approval\n- Limit response to 500 words\n- Use DoD-approved terminology for security controls\n- Flag any findings that could impact Authority to Operate (ATO)\n</pre>"},{"location":"plays/prompt-engineering/#6-format-the-result_2","title":"6. Format the Result","text":"<pre>\nReturn results as a structured markdown report with:\n- Executive summary of critical findings\n- Detailed findings table with STIG/NIST control references\n- Remediation code snippets\n- Risk severity ratings (Critical/High/Medium/Low)\n</pre>"},{"location":"plays/prompt-engineering/#7-reinforce-with-examples-and-delimiters_1","title":"7. Reinforce with Examples and Delimiters","text":"<pre>\n### FINDING EXAMPLE ###\n**Control:** NIST 800-53 SC-7 (Boundary Protection)  \n**Issue:** Security group allows unrestricted inbound access (0.0.0.0/0) on port 22  \n**Risk:** Critical  \n**Remediation:** Restrict SSH access to specific IP ranges or VPN endpoints\n</pre>"},{"location":"plays/prompt-engineering/#c-security-analyst-sample-prompt-stigfisma-compliance-checklist-generation","title":"C. Security Analyst Sample Prompt: STIG/FISMA Compliance Checklist Generation","text":"<ul> <li>Role \u2013 Security Analyst</li> <li>Use Case \u2013 STIG/FISMA Checklist Generation for ATO Support</li> <li>Leading Practice \u2013 Use structured prompts to generate repeatable, auditable checklists based on authoritative controls</li> <li>Risk Mitigation \u2013 Reduce hallucination by anchoring to approved STIG families and DoD guidance; limit output scope and enforce checklist formats</li> </ul>"},{"location":"plays/prompt-engineering/#1-role-definition_3","title":"1. Role Definition","text":"<pre>\nYou are a cybersecurity analyst supporting DoD system ATO preparation. You specialize in interpreting STIG guidance, FISMA requirements, and DISA controls to assess compliance posture and generate audit-ready artifacts.\n</pre>"},{"location":"plays/prompt-engineering/#2-define-the-goal_3","title":"2. Define the Goal","text":"<pre>\nGenerate a detailed compliance checklist based on the latest DISA STIGs for a RHEL 8 server, aligned with FISMA moderate controls. Identify unmet requirements and include mitigation notes for open items.\n</pre>"},{"location":"plays/prompt-engineering/#3-specify-the-audience_3","title":"3. Specify the Audience","text":"<pre>\nThis output is intended for an ISSO preparing documentation for an RMF package and presenting findings to the security controls assessor (SCA).\n</pre>"},{"location":"plays/prompt-engineering/#4-provide-context-and-input_3","title":"4. Provide Context and Input","text":"<pre>\nThe system is hosted in an IL5 cloud environment and supports processing of CUI data. The checklist must reflect Red Hat Enterprise Linux 8 STIG v3r2 requirements.\n\n### INPUT ###\n[Insert scanned configuration or assessment output]\n### OUTPUT ###\n</pre>"},{"location":"plays/prompt-engineering/#5-set-constraints-and-compliance_2","title":"5. Set Constraints and Compliance","text":"<pre>\n- Reference only official DISA STIG guidance (https://public.cyber.mil/stigs/)\n- Do not suggest unapproved tools or remediation outside the DISA ecosystem\n- Output should use checklist format with Control ID, Description, Status, and Comments\n- Limit total length to 1,000 words\n</pre>"},{"location":"plays/prompt-engineering/#6-format-the-result_3","title":"6. Format the Result","text":"<pre>\nReturn results as a markdown-formatted checklist including:\n- STIG ID (e.g., RHEL-08-010010)\n- Control description\n- Compliance status (Compliant, Not Compliant, Not Applicable)\n- Comments and mitigation strategy (if not compliant)\n</pre>"},{"location":"plays/prompt-engineering/#7-reinforce-with-examples-and-delimiters_2","title":"7. Reinforce with Examples and Delimiters","text":"<pre>\n### CHECKLIST ENTRY EXAMPLE ###\n**STIG ID:** RHEL-08-010010  \n**Description:** The operating system must enable FIPS mode.  \n**Status:** Not Compliant  \n**Comments:** System FIPS mode is disabled. Remediation scheduled in Q3 maintenance window.\n</pre>"},{"location":"plays/prompt-engineering/#d-program-manager-sample-prompt-cmmc-status-report-generation","title":"D. Program Manager Sample Prompt: CMMC Status Report Generation","text":"<ul> <li>Role \u2013 Program Manager</li> <li>Use Case \u2013 Compliance Reporting \u2013 Drafting weekly updates on CMMC progress and blocker resolutions</li> <li>Leading Practice \u2013 Time-box prompts and define desired output formats (bullet summary, table, narrative)</li> <li>Risk Mitigation \u2013 Reduce verbosity, hallucinated milestones, or misrepresented compliance posture through scoped, auditable responses</li> </ul>"},{"location":"plays/prompt-engineering/#1-role-definition_4","title":"1. Role Definition","text":"<pre>\nYou are a Program Manager responsible for overseeing technical delivery, risk management, and compliance reporting across multiple workstreams in a DoD Agile acquisition program. You are familiar with CMMC Level 2 requirements, RMF workflows, and stakeholder communications for IL5 systems.\n</pre>"},{"location":"plays/prompt-engineering/#2-define-the-goal_4","title":"2. Define the Goal","text":"<pre>\nGenerate a weekly status update highlighting progress toward CMMC Level 2 compliance, including resolved blockers, active risks, and any new artifacts generated this sprint. Summarize this in a format suitable for executive review.\n</pre>"},{"location":"plays/prompt-engineering/#3-specify-the-audience_4","title":"3. Specify the Audience","text":"<pre>\nThis output is intended for senior leadership (GS-15/SES) overseeing cybersecurity compliance, as well as contracting officers and government leads tracking milestone alignment and authorization readiness.\n</pre>"},{"location":"plays/prompt-engineering/#4-provide-context-and-input_4","title":"4. Provide Context and Input","text":"<pre>\nThe system is undergoing phased CMMC readiness and has completed 6 of 17 domains. Recent sprint activities included control implementation reviews, POA&amp;M updates, and team onboarding to a new security automation platform.\n\n### INPUT ###\n[List of sprint accomplishments, unresolved risks, and artifacts generated]\n### OUTPUT ###\n</pre>"},{"location":"plays/prompt-engineering/#5-set-constraints-and-compliance_3","title":"5. Set Constraints and Compliance","text":"<pre>\n- Limit the output to 400 words\n- Organize information by domain (Access Control, Configuration Management, etc.)\n- Avoid speculative claims or unstated assumptions about control maturity\n- Use CMMC-defined terminology and reference relevant control identifiers\n- Include only verified artifacts or actions completed during the reporting period\n</pre>"},{"location":"plays/prompt-engineering/#6-format-the-result_4","title":"6. Format the Result","text":"<pre>\nReturn results as an executive summary containing:\n- Overall compliance status (numeric and narrative)\n- Sectioned highlights by CMMC domain\n- Tabular risk summary (risk, impact, mitigation status)\n- Bullet list of artifacts submitted this week\n</pre>"},{"location":"plays/prompt-engineering/#7-reinforce-with-examples-and-delimiters_3","title":"7. Reinforce with Examples and Delimiters","text":"<pre>\n### CMMC WEEKLY STATUS EXAMPLE ###\n**Compliance Summary:**  \n- Completed: 6 of 17 domains  \n- In Progress: 4 domains  \n- Risks: 2 open, 1 mitigated\n\n**Access Control:**  \n- Completed AC.1.001 and AC.1.002  \n- Risk: Delayed MFA integration for contractor systems  \n- Artifact: Access Policy v1.3 (uploaded to eMASS)\n\n**Risk Summary Table:**  \n| Risk ID | Domain | Description                        | Impact | Mitigation Status |\n|---------|--------|------------------------------------|--------|-------------------|\n| R-019   | AC     | MFA delay on external systems      | High   | In Progress       |\n| R-020   | CM     | Incomplete change tracking policy  | Medium | Mitigated         |\n</pre>"},{"location":"plays/prompt-engineering/#e-operations-engineer-sample-prompt-diagnosing-runtime-issues-in-containerized-workloads","title":"E. Operations Engineer Sample Prompt: Diagnosing Runtime Issues in Containerized Workloads","text":"<ul> <li>Role \u2013 Operations Engineer</li> <li>Use Case \u2013 Diagnosing runtime issues in containerized workloads across IL environments</li> <li>Leading Practice \u2013 Use role-defined context with structured logs and environment metadata to focus diagnostic reasoning</li> <li>Risk Mitigation \u2013 Limit scope to in-scope logs, constrain external references, and enforce consistent output structure to avoid drift or unsupported assumptions</li> </ul>"},{"location":"plays/prompt-engineering/#1-role-definition_5","title":"1. Role Definition","text":"<pre>\nYou are a site reliability engineer supporting containerized workloads running in IL5 Kubernetes clusters. Your primary responsibility is diagnosing service disruptions, latency spikes, and resource contention across application pods.\n</pre>"},{"location":"plays/prompt-engineering/#2-define-the-goal_5","title":"2. Define the Goal","text":"<pre>\nAnalyze runtime logs and environment metadata to identify root cause of service degradation impacting container startup and response times. Recommend next troubleshooting steps and possible mitigation actions.\n</pre>"},{"location":"plays/prompt-engineering/#3-specify-the-audience_5","title":"3. Specify the Audience","text":"<pre>\nThis output is intended for a cross-functional incident response team including developers, operations engineers, and security monitors during post-incident review.\n</pre>"},{"location":"plays/prompt-engineering/#4-provide-context-and-input_5","title":"4. Provide Context and Input","text":"<pre>\nThe environment includes a Kubernetes cluster hosted in AWS GovCloud with IL5 controls enabled. The affected service is a mission-support API used during operational readiness assessments.\n\n### INPUT ###\n- Container logs (last 5 minutes)\n- Pod metadata (CPU/mem limits, restart count)\n- Deployment YAML\n- Service mesh (Envoy) metrics\n\n### OUTPUT ###\n</pre>"},{"location":"plays/prompt-engineering/#5-set-constraints-and-compliance_4","title":"5. Set Constraints and Compliance","text":"<pre>\n- Do not reference commercial SaaS observability tools unless pre-cleared\n- Limit response to 750 words\n- Flag any issues that might trigger a continuity of operations (COOP) response\n- Provide plain language summary for non-engineers\n</pre>"},{"location":"plays/prompt-engineering/#6-format-the-result_5","title":"6. Format the Result","text":"<pre>\nReturn a structured report including:\n- Root cause hypothesis\n- Timeline of relevant log events\n- System components impacted\n- Suggested next steps (e.g., isolate pod, restart container, modify autoscaler thresholds)\n</pre>"},{"location":"plays/prompt-engineering/#7-reinforce-with-examples-and-delimiters_4","title":"7. Reinforce with Examples and Delimiters","text":"<pre>\n### DIAGNOSTIC SUMMARY ###\n**Root Cause:** Resource limits exceeded due to unthrottled initialization routines  \n**Log Evidence:** Init container terminated with OOMKill at 10:42:17 UTC  \n**Impacted Components:** api-service pod (namespace: ops-readiness)  \n**Recommended Action:** Adjust memory limits, instrument retry backoff for init tasks\n</pre>"},{"location":"plays/prompt-engineering/#vii-practices-tools-patterns-and-resources","title":"VII. Practices, Tools, Patterns, and Resources","text":""},{"location":"plays/prompt-engineering/#prompt-review-and-reuse","title":"Prompt Review and Reuse","text":"<p>Prompts are not disposable. They should be versioned, shared, and curated over time to improve effectiveness and reduce redundant experimentation.</p> <p>Consider using tools like:</p> <ul> <li>LangSmith or PromptLayer for prompt observability and versioning</li> <li>Git repositories to track prompt evolution</li> <li>Prompt libraries for team-wide reuse and learning</li> </ul>"},{"location":"plays/prompt-engineering/#measuring-prompt-engineering-effectiveness","title":"Measuring Prompt Engineering Effectiveness","text":"<p>Effective prompt engineering should be measurable and continuously improved. Track these key indicators to validate your team's prompt engineering effectiveness:</p>"},{"location":"plays/prompt-engineering/#prompt-quality-metrics","title":"Prompt Quality Metrics","text":"<ul> <li>Prompt clarity score: How often prompts require clarification or additional context from users</li> <li>Instruction completeness: Percentage of prompts that include all 7 framework components (role, goal, audience, context, constraints, format, examples)</li> <li>Prompt reusability rate: How often prompts can be adapted across similar use cases without major revision</li> <li>Component effectiveness: Which prompt elements (role definition, constraints, examples) most improve output quality</li> </ul>"},{"location":"plays/prompt-engineering/#prompt-iteration-metrics","title":"Prompt Iteration Metrics","text":"<ul> <li>Revision cycles per prompt: Average number of refinements needed before a prompt produces consistent, quality outputs</li> <li>First-attempt success rate: Percentage of new prompts that produce usable output on initial deployment</li> <li>Prompt refinement velocity: Time from initial prompt draft to production-ready version</li> <li>Template adoption rate: How quickly teams adopt and customize proven prompt patterns</li> </ul>"},{"location":"plays/prompt-engineering/#prompt-governance-metrics","title":"Prompt Governance Metrics","text":"<ul> <li>Security compliance rate: Percentage of prompts that follow DoD security guidelines (no CUI, proper constraints, approved platforms)</li> <li>Version control adoption: How consistently teams version and document prompt changes</li> <li>Anti-pattern detection: Frequency of prompts exhibiting known anti-patterns (vague roles, missing constraints, etc.)</li> <li>Prompt review coverage: Percentage of prompts that undergo peer review before production use</li> </ul>"},{"location":"plays/prompt-engineering/#prompt-library-maturity","title":"Prompt Library Maturity","text":"<ul> <li>Cross-team reuse: How often prompts developed by one team are successfully adopted by others</li> <li>Pattern standardization: Consistency in applying the 7-component framework across different roles and use cases</li> <li>Knowledge capture: How effectively prompts codify domain expertise for reuse</li> <li>Template coverage: Percentage of common SDLC tasks with standardized, tested prompt templates</li> </ul>"},{"location":"plays/prompt-engineering/#implementation-guidance","title":"Implementation Guidance","text":"<ul> <li> <p>Start Simple: Begin by tracking prompt revision cycles and reuse rates before expanding to comprehensive measurement.</p> </li> <li> <p>Baseline Establishment: Document current prompting practices (if any) and time spent on prompt creation vs. refinement.</p> </li> <li> <p>Regular Review: Conduct monthly prompt retrospectives focusing on which patterns work best for different roles and use cases.</p> </li> <li> <p>Version Control Integration: Track prompt evolution alongside code changes to understand what prompt modifications correlate with better outcomes.</p> </li> </ul>"},{"location":"plays/prompt-engineering/#viii-resources-and-references","title":"VIII. Resources and References","text":"<p>The following resources offer practical and evolving guidance for writing effective AI prompts. While tailored for different use cases, many principles (e.g., specificity, context-setting, declarative language) are applicable across DoD environments.</p> Source Guide Title Notes Microsoft Prompt engineering 101 Practical patterns, risk considerations, and use case structure for GitHub Copilot Google DeepMind What is Prompt Engineering? (Google Cloud) Emphasizes examples, constraints, and prompt testing in code generation LangChain / LangSmith Prompt Engineering Quick Start (LangSmith UI) Versioning, testing, and observability in structured pipelines AcqBot (GovPrompt) Prompt Engineering Crash Course for Government Role-based prompt walkthroughs tailored to government use cases; emphasizes structure and risk mitigation Anthropic Anthropic Prompt Library Library of categorized prompts for Claude models with secure design examples OpenAI OpenAI Prompt Examples Code-focused examples showing real-world prompting for common use cases West Point GenAI Prompting Library Educational resource with prompt structure examples aligned to military education <p>These guides reinforce that prompting is not just an art.  It's an engineering discipline, especially in regulated or mission-critical environments.</p> <ol> <li> <p>IBM, \u201cThe Prompt Engineering Guide,\u201d IBM Think Blog. [Online]. Available: https://www.ibm.com/think/topics/prompt-engineering-guide. [Accessed: 20-Jun-2025].\u00a0\u21a9</p> </li> <li> <p>IBM, \u201cPrompt Engineering Techniques,\u201d IBM Think Blog. [Online]. Available: https://www.ibm.com/think/topics/prompt-engineering-techniques. [Accessed: 15-Jun-2025].\u00a0\u21a9</p> </li> </ol>"},{"location":"resources/ArchitecturalDecisionRecord/","title":"Architectural Decision Record (ADR)","text":"<p>Title: AI Model Hosting and Usage Decision ADR ID: ADR-00X Status: Proposed / Approved / Superseded Date: [YYYY-MM-DD] Author(s): [Name(s), Org, Contact Info] Reviewer(s): [Architecture board, cyber lead, PM, etc.]</p>"},{"location":"resources/ArchitecturalDecisionRecord/#1-context","title":"1. Context","text":"<p>Briefly describe the context in which the AI model will be used:</p> <ul> <li>Project / Mission Area:  </li> <li>Classification Level: (e.g., Unclassified, CUI, IL4, IL5, IL6)  </li> <li>Stage in SDLC: (e.g., Code Generation, Testing, Planning Support)  </li> <li>Team / Software Factory:  </li> <li>Model Use Purpose: (e.g., LLM-generated tests, agentic task coordination, documentation support)</li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#2-decision","title":"2. Decision","text":"<p>We have selected the following model hosting and usage pattern:</p> <ul> <li>Hosting Model:</li> <li>\u2610 Public SaaS (e.g., OpenAI.com, Bard)</li> <li>\u2610 Gov SaaS / Controlled Cloud (e.g., Azure OpenAI IL5)</li> <li>\u2610 Self-Hosted / Open Source (e.g., LLaMA 2, Mistral)</li> <li> <p>\u2610 Hybrid / RAG with Controlled External Model Access</p> </li> <li> <p>Model(s) Chosen:  </p> </li> <li>Access Method: (e.g., API call, RAG via broker, IDE plugin)  </li> <li>Environment of Use: (e.g., Platform One, air-gapped enclave, GovCloud pipeline)  </li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#3-rationale","title":"3. Rationale","text":"<p>Provide justification for the selected model and hosting choice:</p> Factor Consideration Notes Mission Risk &amp; Classification Required Trust Level Pipeline Integration Capability Model Transparency / Versioning Needs Sustainment Capacity Vendor Compliance (FedRAMP, IL4/5) Security &amp; Privacy Constraints Performance Requirements Prompt / Output Governance Maturity"},{"location":"resources/ArchitecturalDecisionRecord/#4-controls-and-safeguards","title":"4. Controls and Safeguards","text":"<p>Document the cybersecurity and governance guardrails in place:</p> <ul> <li>[ ] Model version control (locked or tracked)</li> <li>[ ] Prompt and output logging enabled</li> <li>[ ] Prompt templates versioned and reviewed</li> <li>[ ] Human-in-the-loop validation (where applicable)</li> <li>[ ] Egress controls for hybrid or external access</li> <li>[ ] Automated or manual rollback plan in place</li> <li>[ ] Integration with existing DevSecOps logging and policy</li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#5-consequences","title":"5. Consequences","text":"<p>Describe potential consequences or tradeoffs of this decision:</p> <ul> <li>Known Risks (e.g., latency, vendor lock-in, insufficient observability):  </li> <li>Fallback Plan or Alternative:  </li> <li>Periodic Review Timeline (e.g., every 6 months or upon model update):  </li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#6-related-records-or-references","title":"6. Related Records or References","text":"<ul> <li>[Link to system architecture doc]  </li> <li>[Trust calibration checklist or framework alignment]  </li> <li>[Supply Chain Risk Management record]  </li> <li>[DoD AI Strategy / AI RMF mapping]  </li> <li>[Testing and evaluation results, if any]</li> </ul>"},{"location":"resources/template/","title":"\ud83e\uddf1 AI4SDLC Play: [Title of Play]","text":"<p>Working title \u2014 be explicit, not clever. Plays are meant to be teachable, repeatable, and usable. Titles should describe what this play teaches (e.g., \u201cAI-Augmented Testing,\u201d \u201cNavigating the AI Autonomy Continuum\u201d).</p>"},{"location":"resources/template/#executive-summary-the-play-in-brief","title":"Executive Summary (The Play in Brief)","text":"<p>Purpose: Provide a concise, 150\u2013250-word overview so the reader immediately understands the mission impact and key takeaway. Why: Most DoW, DoD, and industry readers will skim before diving in. This section is the \u201celevator pitch\u201d for the play. Healthy Divergence: Style and tone can vary \u2014 use story, analogy, or key quote if it clarifies the play\u2019s essence.</p> <p>Example:</p> <p>Generative AI is reshaping [phase of SDLC]. This play outlines secure, auditable practices to integrate AI augmentation while preserving mission assurance and traceability. TL;DR: AI augments, not replaces. Focus on calibrated trust, governance, and measurable improvement.</p>"},{"location":"resources/template/#1-why-it-matters-and-how-to-use-this-play","title":"1. Why It Matters and How to Use This Play","text":"<p>Purpose: Establish relevance and scope. Describe the problem being solved and how this play connects to others. Why: Anchors the play in mission context and helps readers self-select. Healthy Divergence: Plays dealing with higher autonomy (e.g., Autonomy Continuum) may expand this into a short narrative or conceptual framing rather than a procedural \u201cwhy.\u201d</p> <p>Include:</p> <ul> <li>Operational or mission driver (e.g., \u201cAI adoption is accelerating testing risk\u201d).</li> <li>Audience (roles, stakeholders).</li> <li>When this play is relevant (phase or situation).</li> <li>Relationship to other plays (Fundamentals, Requirements, etc.).</li> </ul>"},{"location":"resources/template/#2-prerequisites-and-foundations-optional-but-recommended","title":"2. Prerequisites and Foundations (optional but recommended)","text":"<p>Purpose: Define readiness conditions (maturity, toolchain, data, or workforce). Why: AI amplification fails without disciplined foundations. Clarifying prerequisites protects credibility and security. Healthy Divergence: Optional for conceptual plays like Autonomy or Fundamentals; mandatory for tooling or execution-focused plays (Code, Testing, Operate).</p> <p>Include:</p> <ul> <li>Expected DevSecOps maturity (e.g., CI/CD automation, IaC baseline).</li> <li>Data hygiene and model readiness.</li> <li>Policy alignment (e.g., Zero Trust, SSDF, DoD AI RMF).</li> <li>Workforce skills or minimum fluency required.</li> </ul>"},{"location":"resources/template/#3-guardrails-security-compliance-and-trust","title":"3. Guardrails: Security, Compliance, and Trust","text":"<p>Purpose: Define non-negotiable safety and security boundaries for AI use. Why: Aligns with DoD Zero Trust, RMF, and AI containment principles; establishes accountability. Healthy Divergence: The depth of this section may vary \u2014 detailed for high-risk plays (Testing, Secure, Deploy), summarized for conceptual ones (Autonomy, Prompt Engineering).</p> <p>Include:</p> Risk Area Guardrail Data Handling No classified or export-controlled data in AI prompts. Model Provenance Record model name, version, and hosting environment. Human Oversight All AI-generated artifacts require validation. Containment Execute AI actions within approved boundaries. <p>\ud83d\udccc Guardrails protect not just data, but decision integrity.</p>"},{"location":"resources/template/#4-patterns-and-practices","title":"4. Patterns and Practices","text":"<p>Purpose: Present the core architecture or workflow patterns for AI use in this phase. Why: This is the instructional heart of the play \u2014 how teams implement the capability safely and repeatably. Healthy Divergence: Encourage creativity here. The \u201cpattern\u201d could be prompt templates, autonomy levels, process maps, or architecture patterns \u2014 depending on topic.</p> <p>Structure:</p> <ul> <li>Short intro paragraph (\u201cThis play introduces five trusted patterns\u2026\u201d)</li> <li> <p>Subsections for each pattern, e.g.:</p> </li> <li> <p>4.1 Pattern: [Name]</p> </li> <li>Description</li> <li>Example / Prompt</li> <li>Benefits / Risks</li> <li>Alignment to Autonomy Pattern Level</li> </ul> <p>\ud83d\udccc Plays-as-code concept: each pattern should be independently reusable.</p>"},{"location":"resources/template/#5-decision-framework-when-and-how-to-apply","title":"5. Decision Framework: When and How to Apply","text":"<p>Purpose: Help teams decide if and when to use AI in this context. Why: Supports mission-aligned decision-making and calibrated autonomy \u2014 \u201cjust because you can, doesn\u2019t mean you should.\u201d Healthy Divergence: May take the form of a matrix, flowchart, or checklist depending on the play\u2019s complexity.</p> <p>Include:</p> <ul> <li>Use-case table: AI benefits, risks, recommended autonomy pattern.</li> <li>Maturity or risk thresholds (e.g., \u201cRequires IL5 model,\u201d \u201cNot suitable for safety-critical systems\u201d).</li> <li>Criteria for scaling from pilot to production.</li> </ul>"},{"location":"resources/template/#6-examples-and-quick-start-patterns","title":"6. Examples and Quick-Start Patterns","text":"<p>Purpose: Make the play actionable immediately \u2014 practical examples, prompts, or scenarios. Why: Converts abstract guidance into operational practice; improves adoption. Healthy Divergence: Some plays (Prompt Engineering) may feature detailed prompt fragments; others (Autonomy) may use case narratives or architecture snippets.</p> <p>Include:</p> <ul> <li>2\u20133 short examples in code, pseudo-code, or structured prompt format.</li> <li>Each example should demonstrate role, context, input, expected outcome.</li> </ul> <p>\ud83d\udccc Keep examples aligned with the DoW\u2019s operational classification levels \u2014 no public API assumptions unless explicitly allowed.</p>"},{"location":"resources/template/#7-next-steps","title":"7. Next Steps","text":"<p>Purpose: Give readers a clear, minimal roadmap to pilot, measure, and iterate. Why: Encourages safe experimentation and feedback loops. Healthy Divergence: Adjust tone \u2014 procedural for technical plays; reflective for conceptual ones.</p> <p>Include:</p> <ul> <li>\u201cPilot, Measure, Iterate\u201d guidance.</li> <li>Suggested success metrics (software health, defect rate, trust calibration).</li> <li>Mechanism to share lessons learned (e.g., submit to AI4SDLC working group).</li> </ul>"},{"location":"resources/template/#8-key-takeaways","title":"8. Key Takeaways","text":"<p>Purpose: Reinforce learning and close with conviction. Why: Repetition builds retention. This ensures every play ends with clarity and momentum. Healthy Divergence: Style can vary \u2014 bullet summary, narrative reflection, or quotation.</p> <p>Example:</p> <ul> <li>AI augments human testers; it does not replace them.</li> <li>Requirements\u2014not code\u2014drive trustworthy tests.</li> <li>Evidence-as-code supports accreditation and auditability.</li> <li>Calibrated trust is earned, not automated.</li> </ul>"},{"location":"resources/template/#9-companion-plays-and-references","title":"9. Companion Plays and References","text":"<p>Purpose: Connect this play within the larger AI4SDLC ecosystem. Why: Reinforces the idea that no play stands alone; they form a living framework. Healthy Divergence: For high-level plays (e.g., Fundamentals), this may become an extended reading section with external standards.</p> <p>Include:</p> <ul> <li>Links to related plays (Fundamentals, Prompt Engineering, Autonomy, etc.)</li> <li>Citations (NIST SSDF, DoD AI RMF, DevSecOps Reference Design, MITRE AI4SDLC corpus).</li> <li>Optional \u201cSuggested Next Read\u201d for practitioners expanding their scope.</li> </ul>"},{"location":"resources/template/#summary-for-authors","title":"\ud83d\udcd8 Summary for Authors","text":"Section Must Include Can Vary Why It Exists Executive Summary 1-paragraph overview, TL;DR Tone, story style Ensures immediate clarity Why It Matters Mission context, audience, scope Narrative depth Anchors relevance Prerequisites Maturity, baseline Optional for conceptual plays Defines readiness Guardrails Security, compliance, human review Depth of detail Protects integrity Patterns &amp; Practices Core framework Content type (code, prompt, process) Drives application Decision Framework Use-case guidance Format (table, checklist) Informs autonomy choice Examples 2\u20133 quick-starts Format and complexity Enables experimentation Next Steps Pilot &amp; feedback path Level of specificity Ensures iterative improvement Key Takeaways Summary bullets Style Reinforces learning References Linked plays, standards Scope Encourages traceability"},{"location":"resources/template/#when-divergence-is-healthy","title":"\u2733\ufe0f When Divergence Is Healthy","text":"Divergence Type Justification Example Narrative or Conceptual Expansion For plays like Autonomy or Fundamentals that define principles rather than execution. Replace \u201cPatterns\u201d with \u201cFrameworks and Principles.\u201d Depth of Guardrails When the play\u2019s domain carries higher classification or compliance risk. Testing, Secure, Deploy. Pattern Presentation Plays can choose code, prompt, or process diagrams depending on topic. Code Gen vs. Requirements. Absence of Prerequisites Acceptable for plays that establish foundational concepts. Prompt Engineering. Additional Section (\u201cRoles &amp; Personas\u201d) Valuable for cross-functional areas (Requirements, Testing). Add between Sections 2 and 3."},{"location":"resources/template/#usage-notes-for-gitlabmkdocs-integration","title":"\ud83e\udde9 Usage Notes for GitLab/MkDocs Integration","text":"<ul> <li>Each section corresponds to a Markdown include or template partial.</li> <li>Front matter should include:</li> </ul> <p><code>yaml   # automatic badge generation   lifecycle: draft | beta | stable   last_updated: \"YYYY-MM-DD\"</code> * Keep headings (<code>##</code>) consistent for cross-play analytics and automatic index generation. * Use fenced code blocks for prompt examples to support syntax highlighting and safe copy-paste. * Limit each play to ~5,000 words max for readability; deeper guidance can live in \u201cCompanion Guides.\u201d</p>"}]}