{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the United States Department of Defense's #AI4SDLC Site","text":""},{"location":"#the-future-of-software-development-in-the-dod-starts-here","title":"The Future of Software Development in the DoD Starts Here","text":"<p>Artificial Intelligence (AI) is reshaping how software is designed, built, tested, and deployed. In the DoD, where security, compliance, and mission impact are paramount, AI must be harnessed responsibly, strategically, and effectively.</p> <p>#AI4SDLC is your playbook for integrating AI into the Software Development Lifecycle (SDLC)\u2014providing clear, actionable guidance tailored to DoD software value streams. Whether you\u2019re a developer, security engineer, acquisition professional, or program manager, this is where you\u2019ll find the insights and strategies to make AI work for mission success.  </p> <p>Our goal is to provide practical guidance while also linking to relevant and helpful content from across the DoD and industry, helping teams can access the latest policies, frameworks, and best practices.  </p>"},{"location":"#_1","title":"Overview","text":""},{"location":"#ai4sdlc","title":"#AI4SDLC?","text":"<p>AI is already transforming software engineering, but the DoD can\u2019t afford to adopt AI blindly. To be effective, AI integration must align with:</p> <ul> <li>Mission Readiness \u2013 AI must enhance, not disrupt, DoD software pipelines.  </li> <li>Security &amp; Compliance \u2013 AI tools must meet strict DoD security standards and Zero Trust principles.  </li> <li>Human-AI Teaming \u2013 AI should augment and empower software teams, not replace them. </li> <li>Operational Resilience \u2013 AI models must be governable, explainable, and adaptable to DoD requirements.  </li> </ul> <p>That\u2019s where #AI4SDLC comes in. We deliver practical, tested plays to guide AI adoption at every stage of the SDLC. What's more important is that we are collaborating with programs across the DoD making sure our hands on guidance aligns with their on-the-ground experiences.</p>"},{"location":"#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Generative AI (GenAI) refers to artificial intelligence systems\u2014typically large language models (LLMs)\u2014that can generate new content in response to natural language prompts. Unlike traditional AI that classifies, detects, or predicts within defined rules, GenAI can produce human-readable code, text, infrastructure scripts, documentation, diagrams, and even test cases.</p> <p>In the context of the SDLC, GenAI acts as an amplifier for human intent. Engineers and analysts describe what they want, and the GenAI tool attempts to generate a working solution. When properly integrated and governed, this can reduce toil, accelerate scaffolding, and improve documentation quality.</p> <p>However, GenAI is not magic\u2014it introduces new forms of error, creates verification challenges, and shifts the role of developers and engineers from creators to reviewers. It requires intentional design and oversight to ensure alignment with mission needs, security standards, and operational constraints.</p>"},{"location":"#genais-impact-across-the-software-value-stream","title":"GenAI's Impact Across the Software Value Stream","text":"<p>Generative AI isn't just a tool for writing code\u2014it is reshaping the entire software development value stream, from initial concept to deployed operations. Its potential and pitfalls extend well beyond individual developer productivity.</p> <p>When thoughtfully integrated, GenAI supports:</p> <ul> <li> <p>Requirements Engineering - Accelerates the transformation of stakeholder intent into structured, testable user stories or models. Prompts can be used to generate acceptance criteria, identify edge cases, or translate between technical and non-technical language.</p> </li> <li> <p>Architecture and Design  - Assists with documenting system context, suggesting interface definitions, creating UML sketches, and even proposing architectural tradeoffs based on past designs or known patterns.</p> </li> <li> <p>Development and Testing - Generates boilerplate code, test scaffolding, and documentation. Code suggestions can speed up delivery, but they also introduce new risks\u2014requiring human review, security scanning, and traceability.</p> </li> <li> <p>Deployment and Sustainment - Produces configuration files (e.g., Dockerfiles, CI/CD YAML), infrastructure-as-code templates, and observability scripts. GenAI can also assist in identifying performance tuning issues or log pattern anomalies.</p> </li> <li> <p>Validation and Compliance - Helps draft documentation, security policies, SBOM entries, and regulatory artifacts. Can be used to cross-check outputs against known standards\u2014though human validation remains essential.</p> </li> </ul> <p>The result is not just faster delivery\u2014but new interaction models, trust boundaries, and workforce behaviors across the SDLC. This site helps teams navigate these shifts securely and responsibly.</p>"},{"location":"#playbook-orientation","title":"Playbook Orientation","text":""},{"location":"#what-youll-find-here","title":"What You\u2019ll Find Here","text":"<p>Each Play in #AI4SDLC provides structured, no-nonsense guidance on adopting AI in secure, scalable, and mission-driven ways. Our plays cover:</p> <ul> <li>Hosting AI \u2013 Should you run AI on-prem, in the cloud, or hybrid? We break it down.  </li> <li>AI for Development \u2013 AI copilots, IDE plugins, and agentic workflows\u2014what works for DoD teams? </li> <li>Security &amp; Trust \u2013 How to govern AI tools, mitigate risk, and enforce compliance without slowing innovation.  </li> <li>AI for DevSecOps \u2013 Automating testing, security, and compliance while maintaining human oversight.  </li> <li>Operational Impact \u2013 AI isn\u2019t just a tool\u2014it\u2019s part of your mission execution strategy.</li> </ul> <p>No fluff. No hype. Just clear, actionable guidance.</p> <p>We also provide foundational knowledge to support teams across roles\u2014whether you\u2019re AI-curious or building for mission-critical environments. Topics include prompting, human-machine teaming, DevSecOps integration, and trust calibration.</p>"},{"location":"#available-and-upcoming-plays","title":"Available and Upcoming Plays","text":"<p>This guidance is structured as a series of modular \u201cplays,\u201d each focused on a specific aspect of AI-augmented software engineering:</p>"},{"location":"#available-plays","title":"Available Plays","text":"<ul> <li>Home / Basics \u2013 Terminology, responsible AI use, and the case for action  </li> <li>Fundamentals for Designing an AI-Augmented Tool Chain \u2013 Foundational interaction and integration patterns  </li> <li>Code Generation &amp; Completion \u2013 Leading practices, risks, and DevSecOps alignment for GenAI-driven authoring  </li> </ul>"},{"location":"#upcoming-plays","title":"Upcoming Plays","text":"<ul> <li>Governance for Responsible GenAI Adoption </li> <li>Building an AI-Augmented Workforce </li> <li>Risk Reference Companion </li> <li>Futures Watch: Agentics and Emerging Practices</li> </ul> <p>Each play is designed to stand alone while reinforcing shared principles\u2014so you can start where it matters most to your team or mission.</p>"},{"location":"#human-machine-teaming-themes","title":"Human-Machine Teaming Themes","text":"<p>AI augmentation isn\u2019t just a tooling upgrade\u2014it\u2019s a shift in how humans and machines collaborate across the software lifecycle. As organizations adopt GenAI, they must address more than integration and governance\u2014they must rethink roles, decision-making, and trust.</p> <p>The following themes appear throughout the AI4SDLC plays and reflect critical human-centric issues teams must face when responsibly using GenAI in secure, mission-aligned environments:</p>"},{"location":"#shifting-roles-from-creators-to-reviewers","title":"Shifting Roles: From Creators to Reviewers","text":"<p>GenAI shifts human work from hand-coding to reviewing, prompting, and validating machine-suggested outputs. This changes team dynamics, responsibilities, and required skills. [Read more \u2192 TBD]</p>"},{"location":"#cognitive-overload-and-prompting-fatigue","title":"Cognitive Overload and Prompting Fatigue","text":"<p>As teams adopt GenAI, developers and engineers are experiencing decision fatigue from unclear outputs, inconsistent prompting results, and lack of context. Effective prompting strategies and human-in-the-loop workflows are essential. [Read more \u2192 TBD]</p>"},{"location":"#calibrated-trust-and-human-oversight","title":"Calibrated Trust and Human Oversight","text":"<p>Trusting AI-generated outputs is not binary. Teams must design workflows that ensure the right level of human oversight based on risk, criticality, and system phase. [Read more \u2192 TBD]</p>"},{"location":"#human-machine-interaction-patterns","title":"Human-Machine Interaction Patterns","text":"<p>As teams integrate GenAI into their workflows, they aren't just choosing tools\u2014they're defining how humans and machines collaborate. From standalone web interfaces to agentic platforms, each interaction pattern affects traceability, trust, and governance. Whether you're prompting in an IDE, using an AI-first workspace, or delegating tasks to autonomous agents, it's critical to architect the right level of human oversight. [Read more \u2192 TBD]</p>"},{"location":"#who-should-use-this-playbook","title":"Who Should Use This Playbook?","text":"<p>This isn\u2019t just for software developers. If you\u2019re part of any software value stream in the DoD, AI is already impacting your world.</p> <ul> <li>Developers &amp; Engineers \u2013 AI-powered tools to accelerate coding &amp; debugging.  </li> <li>Security &amp; Compliance Teams \u2013 AI governance &amp; risk mitigation strategies.  </li> <li>Program &amp; Acquisition Managers \u2013 Links to Guidance on AI adoption and procurement within DoD policies.  </li> <li>AI Researchers &amp; Policy Leaders \u2013 Insights into AI\u2019s evolving role in DoD software.  </li> </ul> <p>If AI touches your software mission, #AI4SDLC is for you.</p>"},{"location":"#get-started-with-ai4sdlc","title":"Get Started with #AI4SDLC","text":"<ol> <li>Explore Plays \u2013 Browse AI guidance tailored for DoD SDLC.  </li> <li>New to AI in DoD? Start Here \u2013 Foundational insights to build your knowledge.  </li> <li>Latest AI Trends \u2013 Emerging AI use cases shaping DoD software development.  </li> </ol> <p>AI is not the future. It\u2019s now. Equip yourself with the right strategies to make it work for DoD missions.</p> <p>Mission-driven. Security-first. AI-powered. Welcome to #AI4SDLC.</p>"},{"location":"glossary/","title":"Glossary of Terms and Acronyms**","text":"Term / Acronym Definition ADR Architectural Decision Record AI Artificial Intelligence API Application Programming Interface ATO Authority to Operate AWS Amazon Web Services BOM Bill of Materials CD Continuous Delivery CFR Change Failure Rate CI Continuous Integration CUI Controlled Unclassified Information DB Database FLAN Fine-tuned Language Net (e.g., FLAN-T5) GPT Generative Pre-trained Transformer IDE Integrated Development Environment IL4 Impact Level 4 (DoD classification tier) ISR Intelligence, Surveillance, and Reconnaissance JWCC Joint Warfighting Cloud Capability LLM Large Language Model ML Machine Learning MLBOM Machine Learning Bill of Materials NIST National Institute of Standards and Technology RAG Retrieval-Augmented Generation RMF Risk Management Framework SBOM Software Bill of Materials SCIF Sensitive Compartmented Information Facility SCRM Supply Chain Risk Management SDLC Software Development Lifecycle ZTA Zero Trust Architecture"},{"location":"plays/fundamentals-play/","title":"Play: Fundamentals for Designing an AI-Augmented Tool Chain","text":""},{"location":"plays/fundamentals-play/#executive-summary-the-play-in-brief","title":"Executive Summary (The Play in Brief)","text":"<p>Generative   AI (GenAI) is flooding into software teams\u2014often through developer tools, Integrated Development Environment (IDE) plugins, chatbots, and open Application Programming Interface (API) integrations. As a form of Machine Learning (ML), GenAI models are capable of generating new content\u2014like code, documentation, or test cases\u2014rather than simply classifying or predicting. Across the DoD, there's a clear mandate to accelerate adoption, with recent strategies urging agencies to scale AI capabilities in support of mission objectives. But without foundational knowledge of how these tools are architected\u2014and how they intersect with mission workflows, trust boundaries, and cyber risk\u2014organizations risk making decisions that are fast, but fragile. This play is designed to help DoD teams and software factories design their AI-augmented toolchains with intention. It starts with tools\u2014not because tools come first, but because tooling is where AI shows up first. By unpacking hosting models, usage patterns, and human interaction modalities, this play enables teams to make mission-aligned, architecture-led decisions that support lasting change.</p> <p>It\u2019s not about picking a model. It\u2019s about understanding how tools fit into the broader SDLC\u2014who uses them, why, and with what risk.</p> <ul> <li> <p>TL;DR: Start with your mission use case and workflow\u2014who needs the AI-augmented capability, why, and where it fits in the SDLC.  Then architect your hosting and integration strategy with an eye toward trust boundaries, usage models, interaction patterns, and cyber risk posture. This play unpacks the landscape of  options\u2014so teams can make deliberate, mission-aligned decisions about where GenAI belongs and how to adopt it responsibly.</p> </li> <li> <p>Intended audience: CIOs, Chief Engineers, DevSecOps Leads, Program Managers, Technical Leads, Software Engineers, and Software Factory Architects.</p> </li> <li> <p>\ud83d\udccc Key takeaway: The most important architectural decision isn\u2019t which model\u2014it\u2019s how and why you\u2019re using it. Define your use case first, then select a hosting and integration model that aligns with your mission, workforce, and risk posture.</p> </li> </ul>"},{"location":"plays/fundamentals-play/#1-why-this-play-matters","title":"1. Why This Play Matters","text":"<p>The moment a team or organization decides to experiment with or integrate GenAI into the software development lifecycle, they face a foundational architectural decision: Where will the model live? And just as importantly, who controls it, who can see the inputs, and what risks come with those choices?</p> <p>Choosing a hosting and usage model isn't a technical formality\u2014it\u2019s a strategic inflection point with operational, security, and trust implications. The wrong choice can introduce mission-impacting risk, stall authorizations, or limit scale. The right choice aligns with the mission's sensitivity, embraces the reality of cybersecurity-by-design, and supports long-term sustainability and trust.</p> <p>This is not just about hosting infrastructure. It\u2019s about:</p> <ul> <li>Defining your trust boundaries</li> <li>Controlling data ingress and egress</li> <li>Quantifying exposure to adversarial AI threats, model drift, and data leakage</li> <li>Ensuring compliance with DoD data classification, Zero Trust mandates, and supply chain integrity</li> </ul> <p>In short, before choosing a model or a vendor, teams must step back and ask: What is the mission, what is the risk tolerance, and what\u2019s the operational boundary that will keep us in control?</p>"},{"location":"plays/fundamentals-play/#2-architecting-the-ai-augmented-toolchain","title":"2. Architecting the AI-Augmented Toolchain","text":"<p>GenAI is not a bolt-on capability. As it becomes embedded across the software development lifecycle\u2014from planning and design to testing, deployment, and sustainment\u2014it demands deliberate architectural thinking. Choosing the right tooling isn\u2019t enough. We must design the AI-augmented toolchain to ensure it\u2019s observable, controllable, and secure by design.</p>"},{"location":"plays/fundamentals-play/#what-is-an-ai-augmented-toolchain","title":"What Is an AI-Augmented Toolchain?","text":"<p>An AI-augmented toolchain refers to a pipeline or suite of tools where one or more components (code generators, test writers, documentation agents, deployment optimizers, etc.) are infused with AI\u2014often via LLMs or agentic systems. These tools can be passive (suggesting code) or active (taking autonomous actions).</p> <p>In DoD and other high-assurance environments, augmenting the toolchain introduces new architectural surfaces:</p> <ul> <li>Trust boundaries between human, model, and action</li> <li>Prompt design and versioning as code artifacts</li> <li>Feedback loops that must be observable and auditable</li> <li>Emergent behavior that must be bounded or governed</li> </ul>"},{"location":"plays/fundamentals-play/#architecting-for-security-and-trustworthiness","title":"Architecting for Security and Trustworthiness","text":"<p>Just as we embrace *DevSecOps as a mindset, we must extend it to AI-Augmented DevSecOps. This includes:</p> Area Architectural Guidance Identity &amp; Access Enforce least-privilege access for AI tools. Ensure LLMs or agents cannot access sensitive scopes unless explicitly permitted. Prompt Provenance Treat prompts and context injections as code\u2014version them, audit them, and protect them. Model Boundaries Clarify where models live (local, cloud, hybrid) and enforce strict egress controls. Observability &amp; Logging Introduce telemetry at every interaction point: prompts, model outputs, agent actions. Testability Validate AI-generated outputs with both traditional and AI-aware test harnesses. Rollback &amp; Recovery Enable rollback of both model versions and AI-generated artifacts (e.g., code, configs). Policy Enforcement Integrate policies into DevSecOps to block unauthorized model use, drift, or dependency pulls."},{"location":"plays/fundamentals-play/#reference-architecture-concepts","title":"Reference Architecture Concepts","text":"<p>To build mission-ready, AI-augmented pipelines, organizations must rethink how data, models, and logic interact across the software development lifecycle. The following architectural patterns provide building blocks for trustworthy, scalable, and observable GenAI integrations:</p>"},{"location":"plays/fundamentals-play/#a-promptops-layer","title":"A. PromptOps Layer","text":"<p>Establish a dedicated layer in your pipeline to manage prompts as code\u2014including reusable templates, version control, parameter injection, and governance. This improves traceability and reproducibility of AI-generated outputs.  Think of it like CI/CD for prompts.</p>"},{"location":"plays/fundamentals-play/#b-retrieval-augmented-generation-rag-broker","title":"B. Retrieval-Augmented Generation (RAG) Broker","text":"<p>When external models (like LLMs) are used, a RAG broker retrieves internal, curated context (e.g., knowledge bases, architecture docs, ticket history) to send along with the prompt. This reduces hallucination risk and increases model relevance\u2014without retraining the model itself.</p>"},{"location":"plays/fundamentals-play/#c-policy-as-code-for-ai","title":"C. Policy-as-Code for AI","text":"<p>Use policy engines (e.g., Open Policy Agent, Conftest) to inspect and enforce rules around where and how AI is used. For example: block certain model types in production, restrict prompt content, or require logging before execution.   AI needs to be governed just like infrastructure.</p>"},{"location":"plays/fundamentals-play/#d-agent-execution-guardrails","title":"D. Agent Execution Guardrails","text":"<p>For agent-based systems (e.g., AutoGPT, Crew.AI, OpenHands), introduce runtime controls that constrain behavior. Examples include: - Memory limits - Execution timeouts - Tool usage boundaries These guardrails reduce the risk of emergent or uncontrolled AI behaviors.</p>"},{"location":"plays/fundamentals-play/#e-software-bill-of-materials-sbom-model-bom-mlbom","title":"E. Software Bill of Materials (SBOM) + Model BOM (MLBOM)","text":"<p>Track not only your traditional open-source dependencies, but also: - The models you're using - The datasets they were trained or fine-tuned on - The versioning and update dates  </p> <p>This transparency supports reproducibility, compliance, and secure software supply chain practices.</p>"},{"location":"plays/fundamentals-play/#recommendation","title":"\ud83d\udccc Recommendation","text":"<p>These components are not one-size-fits-all. Start small\u2014map one toolchain, identify one use case\u2014and apply these patterns incrementally as trust and complexity grow.</p>"},{"location":"plays/fundamentals-play/#example-from-code-suggestion-to-secure-toolchain","title":"Example: From Code Suggestion to Secure Toolchain","text":"<p>Let\u2019s say your team adopts GitHub Copilot or integrates an internal LLM for code generation.</p> <p>Without architectural planning:</p> <ul> <li>Prompts aren\u2019t versioned.</li> <li>No record of generated output.</li> <li>Outputs go straight into pipeline with no review.</li> <li>Developers rely on outputs without understanding.</li> </ul> <p>With AI-Augmented Toolchain Architecture:</p> <ul> <li>Prompts and outputs are logged and versioned.</li> <li>Every AI suggestion is validated against policy.</li> <li>Code gen is gated by a test scaffold validator.</li> <li>Generated code can be traced back to a prompt and model version.</li> </ul>"},{"location":"plays/fundamentals-play/#takeaway","title":"\ud83d\udccc Takeaway","text":"<p>It's kind of like a driver assistance system. It doesn't prevent all accidents that can happen, but it makes traffic a little bit more secure.</p> <p>\u2014 Thomas Dohmke, CEO of GitHub, June 2023 </p>"},{"location":"plays/fundamentals-play/#3-define-the-hosting-and-usage-models","title":"3. Define the Hosting and Usage Models","text":"<p>Before selecting a tool, service, model, or vendor, it\u2019s essential to understand the distinct hosting and usage patterns available for GenAI across the SDLC. Each model comes with architectural, security, and operational implications\u2014especially in the context of federal classification levels, cATO pipelines, and Zero Trust mandates.</p> <p>The primary categories are:</p>"},{"location":"plays/fundamentals-play/#public-saas-model","title":"Public SaaS Model","text":"<p>Examples: ChatGPT via OpenAI.com, Claude, Bard, Gemini (unclassified public interfaces)</p> <ul> <li>\u2705 Pros: Immediate access, broad community knowledge, rapid iteration</li> <li>\u26a0\ufe0f Risks: Model weights are opaque, vendor-managed updates may introduce regression or drift; user inputs may be logged or retained; no guarantee of U.S. jurisdiction; limited ability to enforce data governance</li> <li>Security Context: High external trust boundary; not suitable for mission-critical, export-controlled, or CUI workloads</li> <li>Use Case Fit: Low-risk experimentation, internal education, code snippets for generic use\u2014not for production or sensitive use</li> </ul>"},{"location":"plays/fundamentals-play/#government-saas-controlled-cloud","title":"Government SaaS / Controlled Cloud","text":"<p>Examples: Azure OpenAI in IL4/5, AWS Bedrock in GovCloud, Google Gov AI</p> <ul> <li>\u2705 Pros: FedRAMP Moderate/High or IL4/5/6 compliance; access to proprietary model strengths with more boundary clarity; improved telemetry and logging</li> <li>\u26a0\ufe0f Risks: Reliant on third-party updates; difficult to guarantee reproducibility or model immutability; access control is only as strong as your cloud configuration</li> <li>Integration Fit: Good for DevSecOps teams using Platform One, Cloud One, or internal AI services aligned to JWCC constructs</li> <li>Security Context: Moderate to High trust boundary; acceptable for CUI and some mission workloads depending on use case</li> </ul>"},{"location":"plays/fundamentals-play/#self-hosted-air-gapped-open-source-model","title":"Self-Hosted / Air-Gapped / Open Source Model","text":"<p>Examples: LLaMA 2, Mistral, Falcon, Dolly, fine-tuned FLAN-T5, Mixtral, custom RAG solutions</p> <ul> <li>\u2705 Pros: Maximum control, full auditability, offline operation; essential for classified, air-gapped, or multi-national environments</li> <li>\u26a0\ufe0f Risks: Requires internal expertise to fine-tune, maintain, secure, and serve models; significant MLOps burden; potential risk of underperforming models without tuning</li> <li>Security Context: Highest trust boundary; architected for enclave deployment, SCIF integration, and red/black separation</li> <li>Use Case Fit: Mission-critical decision support, secure coding, embedded agents in ISR/command software, disconnected ops</li> </ul>"},{"location":"plays/fundamentals-play/#hybrid-models","title":"Hybrid Models","text":"<p>Examples: RAG architecture with local vector DB + external LLM callout, or mixed trust layering</p> <ul> <li>\u2705 Pros: Retain local context and control; reduce data exposure by decoupling model from sensitive knowledge; opportunity to build fine-tuned pipelines with prompt injection defense</li> <li>\u26a0\ufe0f Risks: Complex architecture introduces new attack surfaces; demands rigorous prompt sanitization, inference auditing, and failover strategies</li> <li>Trust Impact: Can increase calibrated trust if traceability and observability are engineered properly</li> </ul>"},{"location":"plays/fundamentals-play/#ai-hosting-and-usage-models-comparison-matrix","title":"AI Hosting and Usage Models: Comparison Matrix","text":"Criteria Public SaaS(e.g., OpenAI, Bard) Gov SaaS / Controlled Cloud(e.g., Azure OpenAI IL5) Self-Hosted / Air-Gapped(e.g., LLaMA2, Mistral) Hybrid (RAG / Mixed Trust) Security Posture \ud83d\udd34 Low (external trust boundary) \ud83d\udfe1 Moderate  (cloud-config dependent) \ud83d\udfe2 High (max control, enclave ready) \ud83d\udfe1 Variable (depends on architecture) Data Control \ud83d\udd34 None (inputs may be retained/logged) \ud83d\udfe1 Partial(depends on configuration) \ud83d\udfe2 Full (data stays within domain) \ud83d\udfe1 Conditional (requires strict design) Model Transparency \ud83d\udd34 Opaque (vendor-managed weights) \ud83d\udd34 Mostly opaque (versioning limited) \ud83d\udfe2 Transparent (open weights and config) \ud83d\udfe1 Partial (depends on external callouts) Performance \ud83d\udfe2 High (vendor-optimized infra) \ud83d\udfe2 High (cloud-accelerated) \ud83d\udfe1 Moderate (depends on in-house tuning) \ud83d\udfe1 Variable (depends on pipeline design) Operational Cost \ud83d\udfe1 Low up-front (can scale fast) \ud83d\udfe1 Subscription/ licensing (cost per token or instance) \ud83d\udd34 High setup\ud83d\udfe1 Lower long-term TCO \ud83d\udfe1 Moderate (RAG infra + model callout costs) Model Reproducibility \ud83d\udd34 None (model updates at vendor discretion) \ud83d\udd34 Limited(some vendor change control) \ud83d\udfe2 High(versions locked, reproducible) \ud83d\udfe1 Mixed (depends on callout dependencies) Mission Fit (Classified/CUI) \ud83d\udd34 Poor(not suitable) \ud83d\udfe1 Moderate (IL4/5 compliant workloads) \ud83d\udfe2 Excellent (supports SCIF, disconnected ops) \ud83d\udfe1 Moderate (requires strict partitioning) Integration Complexity \ud83d\udfe2 Minimal (API-based access) \ud83d\udfe2 Moderate  (P1/C1 aligned^) \ud83d\udd34 High (requires MLOps, infra buildout) \ud83d\udd34 High (requires orchestrated pipeline) <p>^ P1 = Platform One, C1 = Cloud One</p>"},{"location":"plays/fundamentals-play/#legend","title":"Legend:","text":"<ul> <li>\ud83d\udfe2 = Preferred / Strong Alignment</li> <li>\ud83d\udfe1 = Acceptable with Caution / Design Needed</li> <li>\ud83d\udd34 = High Risk / Higher Complexity </li> </ul>"},{"location":"plays/fundamentals-play/#4-decision-framework-choosing-the-right-ai-hosting-and-usage-model","title":"4. Decision Framework: Choosing the Right AI Hosting and Usage Model","text":"<p>When introducing GenAI into the software development lifecycle, it\u2019s tempting to reach for the most powerful model or the easiest plug-in. But success in a high-assurance, mission-driven environment like the DoD demands a deliberate decision-making framework\u2014one that accounts for classification level, trust posture, sustainment, and mission criticality.</p> <p>This framework helps guide teams\u2014software factories, PMOs, mission leads, and cyber operators\u2014through a set of architectural questions to select the right AI hosting and usage model, not just the most available one.</p>"},{"location":"plays/fundamentals-play/#step-1-determine-mission-sensitivity-and-context","title":"Step 1: Determine Mission Sensitivity and Context","text":"Question Why It Matters What is the classification level of the data or workload? Determines model placement (e.g., public SaaS is disallowed for CUI or classified) Is this mission-critical, safety-critical, or time-sensitive? High-stakes decisions demand higher trust, auditability, and model control Will AI outputs directly influence code, policy, deployment, or operations? Direct impact requires tighter control, testing, and provenance"},{"location":"plays/fundamentals-play/#step-2-assess-technical-and-operational-constraints","title":"Step 2: Assess Technical and Operational Constraints","text":"Question Why It Matters Are you able to sustain a self-hosted or hybrid model (e.g., infrastructure, MLOps)? Not every environment has the staff or resources to support open-source models securely Does your pipeline support prompt logging, model versioning, and traceability? Without these, you can\u2019t build calibrated trust or meet ATO expectations What latency, scale, or throughput requirements do you have? Some models work best locally; others need elastic cloud scale or acceleration"},{"location":"plays/fundamentals-play/#step-3-evaluate-cyber-and-compliance-risk","title":"Step 3: Evaluate Cyber and Compliance Risk","text":"Question Why It Matters Is this model FedRAMP authorized or operating inside IL4/5/6? Ensures alignment with DoD risk management frameworks Can you audit all AI interactions (input, model, output)? Required for cyber resilience and post-incident forensics Are there constraints around vendor ownership, model sourcing, or training data provenance? Critical for understanding and mitigating geopolitical risk or model bias exposure"},{"location":"plays/fundamentals-play/#step-4-match-to-a-hosting-model","title":"Step 4: Match to a Hosting Model","text":"Hosting Model Best Fit For\u2026 Public SaaS Low-risk prototyping and internal training only. Not mission workloads. Gov SaaS / Controlled Cloud Moderate-trust workloads in IL4/5/6 with vendor-supported models. Self-Hosted / Open Source High-assurance, enclave or air-gapped missions. Full model control and traceability. Hybrid / RAG Context-specific augmentation of existing SDLC with controlled external inference."},{"location":"plays/fundamentals-play/#output-architectural-decision-record-adr","title":"Output: Architectural Decision Record (ADR)","text":"<p>For each decision, teams should record:</p> <ul> <li>Mission description and data classification</li> <li>Selected hosting model and justification</li> <li>Expected model usage (e.g., generate tests, support code review)</li> <li>Controls in place (prompt logging, access control, rollback procedures)</li> <li>Risk exceptions or mitigations</li> </ul> <p>A sample architectural decision record template is available[here].(https://ArchitecturalDecisionRecord.md) </p>"},{"location":"plays/fundamentals-play/#5-how-humans-interact-with-ai-augmented-tools","title":"5. How Humans Interact with AI-Augmented Tools","text":"<p>As organizations adopt GenAI, they're not just choosing models or hosting platforms\u2014they're defining how humans and machines will collaborate. These interaction patterns vary widely across environments, each bringing different levels of traceability, auditability, and risk.</p> <p>These patterns shape:</p> <ul> <li>Developer workflows and mental models  </li> <li>Security and compliance boundaries  </li> <li>Trust calibration and explainability  </li> <li>The software factory\u2019s ability to scale and govern</li> </ul>"},{"location":"plays/fundamentals-play/#ai-interaction-patterns-todays-landscape","title":"AI Interaction Patterns: Today\u2019s Landscape","text":"Pattern Description Benefits Challenges Standalone Web Interfaces (e.g., ChatGPT, Claude) Accessed via browser or mobile interface, disconnected from enterprise tools or pipelines. \u2705 Easy to access  \u2705 Fast iteration for prototyping and learning \ud83d\udd34 No integration with enterprise workflows  \ud83d\udd34 No traceability or auditability  \ud83d\udd34 Encourages \u201cout-of-band\u201d use IDE Plugins and Adapters (e.g., GitHub Copilot, Continue.Dev) Embedded directly into local development environments, offering in-line code suggestions. \u2705 Accelerates code scaffolding  \u2705 Familiar developer UX \u26a0\ufe0f No architectural context  \u26a0\ufe0f Little prompt/version control  \u26a0\ufe0f Difficult to share prompts across teams AI-First IDEs / Workspaces (e.g., WindSurf, OpenHands) Full-stack environments built around natural language workflows and agentic collaboration. \u2705 Abstracts complexity  \u2705 Integrated agents and tool orchestration \u26a0\ufe0f Redefines team roles  \u26a0\ufe0f Harder to trace decisions  \u26a0\ufe0f Challenges compliance and DevSecOps gates Custom API Integrations (e.g., OpenAI API, Bedrock, internal-hosted LLMs) Embedded into backend or infrastructure via programmatic callouts. \u2705 High control  \u2705 Supports observability and prompt templating \u26a0\ufe0f Requires prompt governance  \u26a0\ufe0f Must be securely integrated into pipelines Agentic Platforms (e.g., AutoGPT, DevAgent prototypes) Orchestrate multi-step tasks using AI agents with memory, planning, and autonomy. \u2705 Delegation of complex tasks  \u2705 Can span across SDLC phases (e.g., testing, deployment) \u26a0\ufe0f Changes developer role to \"AI supervisor\"  \u26a0\ufe0f Emergent behavior risk  \u26a0\ufe0f Needs new trust models and calibration layers"},{"location":"plays/fundamentals-play/#key-insight","title":"\ud83d\udccc Key Insight","text":"<p>\"Essentially, the human-in-the-loop approach reframes an automation problem as a Human-Computer Interaction (HCI) design problem. In turn, we've broadened the question of 'how do we build a smarter system?' to 'how do we incorporate useful, meaningful human interaction into the system?'\"</p> <p>\u2014 Ge Wang, Professor at Stanford University's Human-centered AI initiative</p>"},{"location":"plays/fundamentals-play/#architectural-implication","title":"Architectural Implication","text":"<p>Each interaction pattern affects:</p> <ul> <li>Data flow boundaries</li> <li>Prompt versioning and auditability</li> <li>Alignment with DevSecOps pipelines</li> <li>Calibrated trust for decision-making</li> </ul> <p>These choices must be made intentionally and architected accordingly\u2014especially in regulated or mission-critical environments.</p>"},{"location":"plays/fundamentals-play/#6-designing-for-trust-and-cybersecurity","title":"6. Designing for Trust and Cybersecurity","text":"<p>Incorporating GenAI into the SDLC isn\u2019t just about innovation\u2014it\u2019s a redefinition of trust boundaries. Every prompt, every model call, and every AI-generated output introduces a new surface area for cyber risk, architectural drift, and decision-making opacity.</p> <p>To build mission-ready AI systems, we must design trust in from the start, not inspect it in after the fact.</p>"},{"location":"plays/fundamentals-play/#trust-is-a-system-propertynot-a-feature","title":"Trust Is a System Property\u2014Not a Feature","text":"<p>In traditional systems, trust is built through validation, test coverage, logging, and code reviews. But AI changes the game:</p> <ul> <li>The same prompt may yield different results across time or models.</li> <li>Model updates may occur silently, breaking reproducibility.</li> <li>Human developers may unknowingly accept AI-generated errors or biased outputs.</li> </ul> <p>Trust in an AI-augmented system must be designed, measured,  and recalibrated\u2014just like we do with human teammates.</p>"},{"location":"plays/fundamentals-play/#use-the-calibrated-trust-lens","title":"Use the Calibrated Trust Lens","text":"<p>To support responsible and mission-aligned integration of AI, apply the Calibrated Trust framework (Belief, Understanding, Intent, and Reliance):</p> Dimension Design Questions Belief Is this AI tool appropriate for this task? Is it performing as advertised? Understanding Can users and reviewers explain how the model was used and what data it touched? Intent Are we confident the model\u2019s goals (training data, fine-tuning) align with mission objectives? Reliance Under what conditions should this output be trusted, used, or overridden? <p>This framework helps system owners place the right trust in the right AI at the right time.</p>"},{"location":"plays/fundamentals-play/#design-principles-for-secure-ai-augmented-systems","title":"Design Principles for Secure AI-Augmented Systems","text":"Principle Description Minimize Model Attack Surface Restrict model exposure to only those workflows where it adds verifiable value. Apply egress filtering. Enforce Prompt Governance Treat prompts like code\u2014require reviews, change control, and versioning. Audit All Interactions Log prompts, outputs, model version, and decision trails. Tie to existing DevSecOps audit logs. Enable Model Locking Freeze model versions in production systems. Defer updates until retested in staging. Bound Emergent Behavior Use runtime policies to define agent action scope, memory limits, and timeouts. Secure the Data Plane Encrypt context payloads, anonymize sensitive data, and validate all external callouts. Continuous Evaluation Monitor AI outputs against KPIs and known vulnerabilities. Watch for hallucination, bias, drift."},{"location":"plays/fundamentals-play/#enabling-controls-from-dod-strategy","title":"Enabling Controls from DoD Strategy","text":"<ul> <li>Align to Zero Trust Architecture (ZTA) principles\u2014identity, device, data, network, and workload segmentation</li> <li>Apply Supply Chain Risk Management (SCRM) to model sourcing, prompt datasets, and third-party APIs</li> <li>Adopt AI RMF (NIST SP 1270) and emerging DoD AI Governance practices for mission assurance</li> </ul>"},{"location":"plays/fundamentals-play/#key-insight_1","title":"\ud83d\udccc Key Insight","text":"<p>Calibrated trust is the 1:1 ratio between human trust and trustworthiness of the automation.</p> <p>\u2014 Patricia L. McDermott</p>"},{"location":"plays/fundamentals-play/#7-measures-and-success-indicators-measuring-what-matters","title":"7: Measures and Success Indicators \u2013 Measuring What Matters","text":"<p>Choosing the right AI hosting and usage model is only the first step. To ensure long-term success, teams must define and track key indicators that reflect not just adoption\u2014but effectiveness, governance, and alignment with mission outcomes.</p> <p>This section outlines how to design meaningful measurements for GenAI use across the SDLC, recognizing that metrics will vary by task, tool, and maturity level. As organizations experiment and scale, early metric volatility is expected and should be proactively communicated to leadership to prevent misinterpretation as failure.</p>"},{"location":"plays/fundamentals-play/#key-considerations","title":"Key Considerations","text":""},{"location":"plays/fundamentals-play/#start-with-the-mission-goal","title":"Start with the Mission Goal","text":"<p>Metrics should be anchored to your use case. Is the goal to improve code quality? Accelerate delivery? Enhance documentation? Metrics must fit the outcome, not the novelty of the tool.</p>"},{"location":"plays/fundamentals-play/#track-a-balanced-set-of-indicators","title":"Track a Balanced Set of Indicators","text":"<p>Avoid over-optimizing on a single metric. Instead, track multiple dimensions of performance\u2014DevSecOps health, developer experience, code quality, trust posture, and mission impact.</p>"},{"location":"plays/fundamentals-play/#expect-experimentation-and-learning","title":"Expect Experimentation and Learning","text":"<p>GenAI adoption introduces learning curves. New metrics will fluctuate as teams adjust, and even traditional ones may temporarily dip. This is normal\u2014and part of evaluating transformation.</p>"},{"location":"plays/fundamentals-play/#maintain-a-comprehensive-sdlc-view","title":"Maintain a Comprehensive SDLC View","text":"<p>Adoption effects ripple across the lifecycle\u2014from planning and testing to operations and compliance. Look beyond the IDE. Watch how workflows shift.</p>"},{"location":"plays/fundamentals-play/#on-metric-targets-and-minimums","title":"On Metric Targets and Minimums","text":"<p>While some metrics may have industry benchmarks or compliance thresholds, blindly aiming for generic targets can lead to shallow improvements\u2014or worse, gaming the system.</p> <p>Instead:</p> <ul> <li> <p>If the metric is new to your organization (e.g., % of AI-generated code accepted), the priority is to establish a baseline\u2014not chase a number.</p> </li> <li> <p>Use that baseline to understand behavior, not to assign judgment.</p> </li> <li> <p>Focus on trendlines, deltas, and alignment with mission goals\u2014especially for metrics related to DevEx, prompt governance, or ML model trust.</p> </li> <li> <p>Qualitative context matters\u2014metrics like Code Coverage or Churn require understanding of how and why those numbers move.</p> </li> </ul>"},{"location":"plays/fundamentals-play/#metric-categories-and-indicators","title":"Metric Categories and Indicators","text":"Measure Area Sample Metric / Indicator Why It Matters Security Posture % of AI interactions logged and reviewed  # of model version rollbacks initiated Ensures traceability, detects misuse, and supports post-incident forensics Hosting Alignment % of AI tools hosted in IL4/5+ environments  # of tools outside approved environments Highlights shadow IT, supports ATOs, and enforces classification policy Prompt Governance % of prompts versioned and stored  Time to detect/respond to unsafe prompt behavior Promotes secure-by-design usage and trust calibration Operational Effectiveness Time to integrate GenAI into CI/CD  % of outputs accepted without modification Tracks tooling maturity, usefulness, and risk of over-reliance Mission Impact Estimated hours saved by automation  % of teams actively using AI in at least one SDLC phase Supports ROI discussions and shows adoption maturity Developer Experience (DevEx) Sentiment survey scores  Time saved on routine coding tasks Signals morale, tool fit, and efficiency in daily work"},{"location":"plays/fundamentals-play/#traditional-metrics-still-matter","title":"Traditional Metrics Still Matter","text":"<p>Keep tracking foundational metrics\u2014they often reveal subtle shifts in quality or risk:</p> <ul> <li>Change Failure Rate (CFR): % of code changes causing production issues. GenAI should reduce CFR with better tests and cleaner code.</li> <li>Code Churn: Measures how often code is changed. GenAI might reduce churn or, conversely, increase it during prompt tuning.</li> <li>Code Coverage: GenAI-generated tests can improve this\u2014but quality, not just quantity, must be monitored.</li> </ul>"},{"location":"plays/fundamentals-play/#how-to-use-these-metrics","title":"How to Use These Metrics","text":"<ol> <li> <p>Pre-Adoption Baseline    Capture a snapshot of SDLC practices and outcomes before GenAI integration.</p> </li> <li> <p>Post-Adoption Comparison    Reassess metrics after each AI-Augmented tool rollout to evaluate measurable impact.</p> </li> <li> <p>Scorecard Reporting    Aggregate metrics into an AI Integration Score for quarterly leadership briefings, ATO artifacts, or cyber posture reviews.</p> </li> </ol>"},{"location":"plays/fundamentals-play/#chasing-the-right-metrics","title":"Chasing the Right Metrics","text":"<p>Don\u2019t chase metrics\u2014calibrate them. When introducing new metrics to track AI-augmented work, your first goal isn\u2019t to hit a target\u2014it\u2019s to understand your starting point.</p> <ul> <li>If it\u2019s a new metric: Establish a baseline. Don\u2019t assign judgment yet.</li> <li>If it\u2019s a legacy metric: Expect movement. Track trends and context, not just the number.</li> </ul> <p>Examples:</p> <ul> <li>A Code Coverage rate of 10% means developers are checking a box\u2014not delivering testable systems.</li> <li>A spike in Code Churn after AI adoption may indicate misaligned prompts or low-quality suggestions.</li> <li>A drop in CFR might reflect better testing\u2014or developers overriding useful feedback to \u201cimprove\u201d the number.</li> </ul> <p>\ud83d\udccc Metrics don\u2019t create value\u2014insight does. Use metrics to tell a story about your transformation, not to perform for one.</p>"},{"location":"plays/fundamentals-play/#expect-metric-volatilityand-plan-for-it","title":"Expect Metric Volatility\u2014And Plan for It","text":"<p>One of the most important truths about measuring AI-augmented SDLC performance is this: metrics will waver.</p> <ul> <li>If you\u2019re tracking existing metrics (like Change Failure Rate, deployment frequency, or code review coverage), they may dip or spike as teams adopt new tooling, rewire their workflows, and recalibrate what \u201cgood\u201d looks like.</li> <li>If you introduce net-new metrics (like prompt reuse rates or AI-generated output acceptance), expect early variability as teams build familiarity and establish baseline behaviors.</li> </ul> <p>This is normal\u2014and it does not mean the adoption is failing.</p> <p>Instead, use this \u201cmetrics turbulence\u201d as a signal:</p> <ul> <li>Is the team adjusting well to the AI-augmented workflow?</li> <li>Are quality or trust issues driving dips?</li> <li>Do we need to shift training, modify prompts, or adjust where the tool is used?</li> </ul> <p>Teams should anticipate a learning curve and pair metrics with context: surveys, interviews, human-in-the-loop feedback, and AI-SWEC evaluations. This helps distinguish between signal and noise, and supports a calibrated trust journey rather than a binary success/failure view.</p> <p>\ud83d\udccc A dip in metrics doesn\u2019t mean you made the wrong architectural choice. It means you\u2019re watching transformation in real time\u2014and transformation takes iteration.</p> <p>Early metric volatility is expected during AI adoption and should be proactively communicated to leadership to prevent misinterpretation as failure rather than part of the transformation curve.</p>"},{"location":"plays/fundamentals-play/#8-five-common-missteps","title":"8. Five Common Missteps","text":"<p>Even well-intentioned teams can run into trouble when integrating GenAI into the SDLC. Without an architecture-first, trust-aware approach, the AI can accelerate risk just as fast as it accelerates productivity.</p> <p>Here are the most common missteps observed in early adopters across government and industry\u2014and how to avoid them:</p>"},{"location":"plays/fundamentals-play/#1-using-public-saas-models-for-sensitive-workloads","title":"1. Using Public SaaS Models for Sensitive Workloads","text":"<p>The Mistake: Teams use OpenAI.com or other public endpoints for code review, test generation, or design suggestions on mission or export-controlled projects.</p> <p>Why It Happens: Accessibility and ease of use. It \u201cjust works.\u201d</p> <p>Consequence: Potential data exfiltration, policy violations, and untrackable model influence.</p> <p>Preventive Action: Enforce hosting tiers with strict boundary rules. Train teams on model provenance and data classification constraints.</p>"},{"location":"plays/fundamentals-play/#2-treating-prompts-like-ephemeral-artifacts","title":"2. Treating Prompts Like Ephemeral Artifacts","text":"<p>The Mistake: Prompts are created ad hoc, modified on the fly, and never versioned or reviewed.</p> <p>Why It Happens: Developers see prompts as \u201cinputs\u201d rather than \u201ccode.\u201d</p> <p>Consequence: No reproducibility, no audit trail, and no trust calibration.</p> <p>Preventive Action: Establish PromptOps practices. Version and log prompts like source code.</p>"},{"location":"plays/fundamentals-play/#3-bypassing-human-in-the-loop-checks","title":"3. Bypassing Human-in-the-Loop Checks","text":"<p>The Mistake: Generated code, test cases, or documentation are automatically merged or published without review.</p> <p>Why It Happens: Pressure for speed or belief that \u201cAI knows best.\u201d</p> <p>Consequence: Introduces hallucinated logic, security vulnerabilities, or compliance violations into the system.</p> <p>Preventive Action: Require human checkpoints or policy-as-code gates before AI outputs influence production.</p>"},{"location":"plays/fundamentals-play/#4-ignoring-model-update-drift","title":"4. Ignoring Model Update Drift","text":"<p>The Mistake: Teams don\u2019t track when a model updates in the background (especially true for SaaS models).</p> <p>Why It Happens: Lack of visibility into vendor-side operations.</p> <p>Consequence: Regression bugs, inconsistent results, broken compliance artifacts.</p> <p>Preventive Action: Lock versions in production. Test new versions in staging. Create an AI model bill of materials (MLBOM).</p>"},{"location":"plays/fundamentals-play/#5-over-reliance-on-tooling-without-calibrated-trust","title":"5. Over-Reliance on Tooling Without Calibrated Trust","text":"<p>The Mistake: Teams accept all AI suggestions as truth or assume AI always improves quality.</p> <p>Why It Happens: Trust by default rather than trust by design.</p> <p>Consequence: Increased risk of low-quality or misleading results, especially in decision support or automation contexts.</p> <p>Preventive Action: Evaluate confidence and risk\u2014not just performance.</p>"},{"location":"plays/fundamentals-play/#strategic-insight","title":"\ud83d\udccc Strategic Insight","text":"<p>As we start moving beyond what's possible with GenAI, solid opportunities are emerging to help solve a number of perennial issues plaguing cybersecurity, particularly the skills shortage and unsecure human behavior.</p> <p>- Deepti Gopal, Director Analyst at Gartner, speaking at Gartner Security &amp; Risk Management Summit 2024</p>"},{"location":"plays/fundamentals-play/#9-recommendations-next-best-play","title":"9. Recommendations &amp; Next Best Play","text":"<p>Choosing the right AI hosting and usage model is foundational\u2014it\u2019s not a one-time configuration but an evolving architectural and cybersecurity commitment. As AI tools become more deeply integrated into the SDLC, organizations must treat them as first-class citizens in the software ecosystem, subject to the same rigor as any mission-critical capability.</p> <p>Below are actionable recommendations for DoD technical teams, leadership, and acquisition stakeholders.</p>"},{"location":"plays/fundamentals-play/#recommendations-for-today","title":"Recommendations for Today","text":"Action Why It Matters Establish an internal AI Hosting Tier Policy Define approved hosting patterns (e.g., IL5 SaaS, enclave-only, hybrid), aligned to data classification and mission sensitivity. Use a decision support framework with each major AI integration decision Enables consistent evaluation of value, risk, effort, and trust\u2014helps justify model/tool selection to stakeholders. Adopt PromptOps practices Treat prompts and context injections like source code: version them, test them, audit them. Create an AI Model Bill of Materials (MLBOM) Track what models were used, when, where, and how\u2014critical for reproducibility and post-incident forensics. Align with Zero Trust and DoD AI Risk Frameworks Incorporate AI usage into existing DevSecOps, Zero Trust, and Supply Chain Risk Management strategies."},{"location":"plays/fundamentals-play/#next-best-play-code-generation-and-completion","title":"Next Best Play: Code Generation and Completion","text":"<p>This play focused on where GenAI capabilities live\u2014exploring hosting models, usage patterns, and architectural integration. The next play turns to how GenAI is reshaping one of the most immediate and visible SDLC tasks: writing code.</p> <p>Code generation and completion tools like Copilot, TabNine, and open-source agentic assistants are rapidly entering developer workflows. But adoption is often ahead of understanding.</p> <p>The next play will explore:</p> <ul> <li>Leading practices for integrating GenAI into coding workflows</li> <li>Human-in-the-loop patterns for safe and productive use</li> <li>Guardrails for quality, maintainability, and team alignment</li> <li>Prompting strategies, telemetry, and testing approaches</li> <li>Real-world lessons from DoD and commercial teams</li> </ul> <p>Whether your team is experimenting with AI-assisted code suggestions or looking to streamline scaffolding and unit test generation, this next play will help you answer: \u201cHow do we use GenAI for code responsibly, repeatably, and at scale?\u201d</p> <p>End of Play</p>"},{"location":"resources/ArchitecturalDecisionRecord/","title":"Architectural Decision Record (ADR)","text":"<p>Title: AI Model Hosting and Usage Decision ADR ID: ADR-00X Status: Proposed / Approved / Superseded Date: [YYYY-MM-DD] Author(s): [Name(s), Org, Contact Info] Reviewer(s): [Architecture board, cyber lead, PM, etc.]</p>"},{"location":"resources/ArchitecturalDecisionRecord/#1-context","title":"1. Context","text":"<p>Briefly describe the context in which the AI model will be used:</p> <ul> <li>Project / Mission Area:  </li> <li>Classification Level: (e.g., Unclassified, CUI, IL4, IL5, IL6)  </li> <li>Stage in SDLC: (e.g., Code Generation, Testing, Planning Support)  </li> <li>Team / Software Factory:  </li> <li>Model Use Purpose: (e.g., LLM-generated tests, agentic task coordination, documentation support)</li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#2-decision","title":"2. Decision","text":"<p>We have selected the following model hosting and usage pattern:</p> <ul> <li>Hosting Model:</li> <li>\u2610 Public SaaS (e.g., OpenAI.com, Bard)</li> <li>\u2610 Gov SaaS / Controlled Cloud (e.g., Azure OpenAI IL5)</li> <li>\u2610 Self-Hosted / Open Source (e.g., LLaMA 2, Mistral)</li> <li> <p>\u2610 Hybrid / RAG with Controlled External Model Access</p> </li> <li> <p>Model(s) Chosen:  </p> </li> <li>Access Method: (e.g., API call, RAG via broker, IDE plugin)  </li> <li>Environment of Use: (e.g., Platform One, air-gapped enclave, GovCloud pipeline)  </li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#3-rationale","title":"3. Rationale","text":"<p>Provide justification for the selected model and hosting choice:</p> Factor Consideration Notes Mission Risk &amp; Classification Required Trust Level Pipeline Integration Capability Model Transparency / Versioning Needs Sustainment Capacity Vendor Compliance (FedRAMP, IL4/5) Security &amp; Privacy Constraints Performance Requirements Prompt / Output Governance Maturity"},{"location":"resources/ArchitecturalDecisionRecord/#4-controls-and-safeguards","title":"4. Controls and Safeguards","text":"<p>Document the cybersecurity and governance guardrails in place:</p> <ul> <li>[ ] Model version control (locked or tracked)</li> <li>[ ] Prompt and output logging enabled</li> <li>[ ] Prompt templates versioned and reviewed</li> <li>[ ] Human-in-the-loop validation (where applicable)</li> <li>[ ] Egress controls for hybrid or external access</li> <li>[ ] Automated or manual rollback plan in place</li> <li>[ ] Integration with existing DevSecOps logging and policy</li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#5-consequences","title":"5. Consequences","text":"<p>Describe potential consequences or tradeoffs of this decision:</p> <ul> <li>Known Risks (e.g., latency, vendor lock-in, insufficient observability):  </li> <li>Fallback Plan or Alternative:  </li> <li>Periodic Review Timeline (e.g., every 6 months or upon model update):  </li> </ul>"},{"location":"resources/ArchitecturalDecisionRecord/#6-related-records-or-references","title":"6. Related Records or References","text":"<ul> <li>[Link to system architecture doc]  </li> <li>[Trust calibration checklist or framework alignment]  </li> <li>[Supply Chain Risk Management record]  </li> <li>[DoD AI Strategy / AI RMF mapping]  </li> <li>[Testing and evaluation results, if any]</li> </ul>"},{"location":"resources/template/","title":"[Play Title]","text":"<p>(Example: Foundations \u2013 Hosting Options &amp; Usage Models)</p>"},{"location":"resources/template/#0-executive-summary","title":"**0. Executive Summary **","text":"<p>What is this play all about. </p>"},{"location":"resources/template/#1-introduction","title":"1. Introduction","text":"<p>Why this play matters. - How this AI capability or decision impacts the DoD software value stream. - Key challenges or opportunities it addresses. - Where it fits in the SDLC (e.g., Plan, Develop, Test, Deploy, Secure, Monitor).  </p>"},{"location":"resources/template/#2-key-considerations-trade-offs","title":"2. Key Considerations &amp; Trade-offs","text":"<p>What DoD teams need to evaluate before implementation. - Security &amp; Compliance: Zero Trust, IL5+, CMMC, mission risk. - Cost &amp; Scalability: Infrastructure investment vs. cloud flexibility. - Performance &amp; Latency: Real-time AI response needs. - Integration &amp; Interoperability: Compatibility with existing DevSecOps pipelines.  </p> <p>\ud83d\udee0 Decision Matrix Example: (If applicable, a comparison of options based on these trade-offs.)</p> Factor Option A Option B Option C Security \u2705 High \u26a0\ufe0f Medium \u274c Low Cost \u274c High \u2705 Moderate \u2705 Low Scalability \u26a0\ufe0f Moderate \u2705 High \u2705 High"},{"location":"resources/template/#3-implementation-guidance","title":"3. Implementation Guidance","text":"<p>How to apply this play in DoD environments. - Step-by-step process or best practices for deploying this AI capability. - Tools &amp; platforms recommended for DoD adoption. - Potential challenges and mitigation strategies. </p>"},{"location":"resources/template/#4-best-use-cases-mission-alignment","title":"4. Best Use Cases &amp; Mission Alignment","text":"<p>Where this AI capability provides the most value in DoD software systems. - Examples of real-world scenarios where this play is useful. - Which mission types or software environments benefit most. - Which teams (developers, security, acquisition) should be involved.  </p> <p>\ud83d\udca1 Example: \ud83d\udee0 Mission-Critical AI \u2192 Use on-prem LLMs for secure DevSecOps pipelines. \ud83d\udee0 Software Experimentation \u2192 Use cloud-based AI copilots for low-risk R&amp;D.  </p>"},{"location":"resources/template/#5-recommendations-next-steps","title":"5. Recommendations &amp; Next Steps","text":"<p>How DoD teams should move forward. \u2705 Key policy or technical actions to take after reading this play. \u2705 Questions teams should ask when making decisions. \u2705 Resources, references, and links to supporting materials.  </p> <p>\ud83d\udccd Final takeaway: Clear, concise summary of when and how to use this play. </p>"},{"location":"resources/template/#template-benefits","title":"Template Benefits:","text":"<ul> <li>Ensures all plays are structured consistently for DoD teams.  </li> <li>Provides clear trade-offs and decision-making frameworks.  </li> <li>Aligns AI integration with security, mission, and operational needs.  </li> </ul>"}]}